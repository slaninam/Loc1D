{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "data_exploration.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slaninam/Loc1D/blob/master/data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnEnBclAgSXj",
        "colab_type": "text"
      },
      "source": [
        "# Environment setup and data import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wheBrhFcw0HU",
        "colab_type": "text"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "This is a view on the github-stored notebook. clone it first and navigate to a subfolder in orde to access the data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRQaDrNgR5M",
        "colab_type": "code",
        "outputId": "564bd5a5-fcf0-403b-c487-63bc16f55a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "! git clone https://github.com/slaninam/Loc1D.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Loc1D'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 96 (delta 3), reused 0 (delta 0), pack-reused 84\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BLNLNWmwHAG",
        "colab_type": "code",
        "outputId": "713fda3e-5e0f-411a-d593-7f7524d2ea5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd Loc1D"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Loc1D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr0ma13Aw-pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbhKYd7HffcY",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "All input files are read into a single pandas DataFrame.\n",
        "The naming convention is maintained, one additional column 'dist' is added to record the actual distance at which the measurement was taken.\n",
        "\n",
        "The complete preprocessed data is available in the DataFrame **df**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjtWnn-vffcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dists = np.arange(0, 35.5, 0.5)\n",
        "dfs = []\n",
        "for dist in dists:\n",
        "    if dist.is_integer():\n",
        "        fname = 'data/dist.' + '{0:.0f}'.format(dist) + 'm.txt'\n",
        "    else:\n",
        "        fname = 'data/dist.' + '{0:.1f}'.format(dist) + 'm.txt'\n",
        "    \n",
        "    df = pd.read_csv(fname, header = None, names=['time','tag id','anchor id','anchor1 id', 'anchor2 id', 'anchor3 id','a0ch0','a0ch1','a0ch2','a0ch3','a0rssi0','a0rssi1','a0rssi2','a0rssi3','a1ch0','a1ch1','a1ch2','a1ch3','a1rssi0','a1rssi1','a1rssi2','a1rssi3','a2ch0','a2ch1','a2ch2','a2ch3','a2rssi0','a2rssi1','a2rssi2','a2rssi3','a3ch0','a3ch1','a3ch2','a3ch3','a3rssi0','a3rssi1','a3rssi2','a3rssi3'])\n",
        "    df['dist'] = dist\n",
        "    dfs.append(df)\n",
        "\n",
        "df = pd.concat(dfs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsmvQwq8ffci",
        "colab_type": "text"
      },
      "source": [
        "## Train and test set creation\n",
        "The data is randomly divided in the train and test sets and the target values are put in separate vectors. \n",
        "\n",
        "Potentially leaky variables removed:\n",
        "* 'time'\n",
        "\n",
        "Othev variables removed as they contain no value:\n",
        "* 'tag id'\n",
        "* 'anchor id'\n",
        "* 'anchor1 id'\n",
        "* 'anchor2 id'\n",
        "* 'anchor3 id'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-ZACYH_ffcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df.drop(axis=1,labels = ['time'])\n",
        "df = df.drop(axis=1,labels = ['tag id','anchor id','anchor1 id','anchor2 id','anchor3 id'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(axis=1,labels = ['dist']), df['dist'], test_size=0.3, random_state=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nzy4MXbffcs",
        "colab_type": "text"
      },
      "source": [
        "# Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zw89eKvxGWW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## ANN\n",
        "* Multi-Layer Perceptron\n",
        "* Totally random structure (first guess)\n",
        "* Output F2-score 0.91 (=not bad)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkb8CN5Tffct",
        "colab_type": "code",
        "outputId": "9cc8b825-337f-48d9-f3a4-341b9f94611a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "clf = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(40, 30), random_state=0)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9531834869657286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU6BPMNkffc3",
        "colab_type": "text"
      },
      "source": [
        "## Classifier performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9n-rpCffcy",
        "colab_type": "text"
      },
      "source": [
        "Plotting the predicted distances vs real distances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWt1PRQxffc0",
        "colab_type": "code",
        "outputId": "c4d9f325-df6d-4c44-91ab-4e556870d5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = y_test\n",
        "y = clf.predict(X_test)\n",
        "\n",
        "plt.scatter(x,y,marker='x')\n",
        "plt.xlabel('Real distance [m]')\n",
        "plt.ylabel('Predicted distance [m]')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsvXl8HOWV7/071d3aN+8StmXZeJPM\nYhJjI9kEbmwDXgl5Z7KS2P7kDiQkBkzuzASDd2yS+87EEE/IDffOa5kkM4GbCQFvgOwQiC3hhWAW\nS7awjWUZJBkv2rfurvP+UV2lqurq7upNaqmf7+cjTFVXVz3VXf2c5+zEzBAIBAKBIBDSQA9AIBAI\nBImNEBQCgUAgCIoQFAKBQCAIihAUAoFAIAiKEBQCgUAgCIoQFAKBQCAIihAUAoFAIAiKEBQCgUAg\nCIoQFAKBQCAIinOgBxALRo4cyUVFRQM9DIFAIBhUvPvuu5eZeVSo44aEoCgqKsLx48cHehgCgUAw\nqCCiOjvHCdOTQCAQCIIiBIVAIBAIgiIEhUAgEAiCIgSFQCAQCIIiBIVAIEhozD1zRA+d/kcICoFA\nkLBsr6jF5j3VmnBgZmzeU43tFbUDPLLkQggKgUCQkDAzWrvd2Hn4vCYsNu+pxs7D59Ha7RaaRT8y\nJPIoBALB0IOIsH5pCQBg5+Hz2Hn4PABg1dwirF9aAiIawNEFhpkNYzNvD0aERiEQCBIWvbBQSWQh\nMVRNZUJQCASChEWdaPXoJ+JEYiibyoTpSSAQJCT6iVY1N6nbQOJpFoPVVGYHoVEIBIKEhIiQk+Yy\nTLTrl5Zg1dwi5KS5EnLi7U9TWX+GDQuNQiAQJCxrFk41OIPViTgRhQQQ2FQW6zFvr6hFa7dbO696\n3Zw0F9YsnBqz66gIjUIgECQ05gk20YWEair75OnFWDW3yOCziNV1+tsXIjQKgUAgiAGBTGUAYmoq\nGwhfCA1mT7zKrFmzWPSjEAgEiUB/5VEwMyY+vk/b/uTpxWFfh4jeZeZZoY4TpieBQCCIIf1hKuvv\nsGEhKAQCgWAQ0V++ED3CRyEQCASDiP7yhRiuKXwUAoFAMPiIhS9E+CgEAoFgAOivRLj+DBsWgkIg\nEAhihCgKKBAIBIKAiKKAAoFAIAiK4mR2oqQg25AIV1KQjZw0Z8JmlNtBaBQCgUAQAxSNwoPqhjbD\n/uqGNrR2ewa1RiEEhUAgEMQAIsK6JcUoKcg27C8pyMa6JcVCo4gGInIQ0XtEtMe3PZGIjhDRGSJ6\nkYhSBnqMAoFAEApmxpa9NZYaxZa9NUKjiJJHANTotn8GYDszTwZwDcD3BmRUAoFAEAZEhOxUp6VG\nkZ0qfBQRQ0TjACwB8H982wTgywD+4DtkF4CvDMzoBAKBwD7MjLYexUehL61R3dCGtp7ofBT92aTI\nioGOenoGwD8BUEXwCADNzOzxbV8EMHYgBiYQCAThEK/SGtsratHa5cb6ZbomRburkZMenyZFVgyY\noCCipQAuMfO7RHRnBO9/AMADAFBYWBjj0QkEAkH4xLojHzPjrdrPcaK+GQCwflkJNu+uxs7K85g5\nPg+PLpjSLyatgdQo5gJYTkSLAaQByAHwLIA8InL6tIpxAD61ejMzPw/geUCp9dQ/QxYIBILgxLq0\nxszxuThR34ydleexs/K8YX9/MWA+CmZ+nJnHMXMRgG8A+DMzfxvAmwD+znfYCgCvDNAQBQLBECYW\ndv94+w6ICBuWzcDKsgmG/SvLJmDDshn95iBPhKgnM/8M4DEiOgPFZ/HvAzwegUCQYEQ7QceiJlN/\n1nUiUNDteJMQgoKZ/8LMS33/f46ZZzPzZGb+e2buGejxCQSCxCHaCToWNZliXdcpkOBTHdd6kxMA\n7Kw8j82749fRzsxARz0JBAKBbfQTNACsX1pi6PZmpyeDPhpJX5NJH60UilicQ2V7RS1auno1UxIz\nY9Puk8hNT8GjC6Zg30cNAICVpROwYfkMbHr1JMqr6rDvowasX1Zi+zrRkBAahUAgENhBnaDV1p8T\nH9+nCYlwJmj9RK8S7gQfi3MoUU2XUF5Zh027T2pCoryyDm/VXgIzoyA3zXdBGP7V9vcDQlAIBIKY\n0J8Ne2IxQW/eU23YF26/6VicAwBuGT8MAFBeWYeJj+9DeWWdtp+I8PJDc7GydILh9ZWlE/DyQ3OT\n2pktEAgGGf3p2I12gtb7E/QZ1Hp/QyzPEUyAEhFeP9kAl2m+dxLw+skGEBGeOfCx/wAI1vvjhBAU\nAoEgKvqzYU80k7z6GhGh+rNWFOdnaVVd1aqv1Z+12vZRWGVhr5pbZOg9sb2iVjMp6cevClCv14uG\nlh64TcP2MPBZSw88Hg9aOntRXlVneL28sg4tXb395swWgkIgEERFrPwGdq8VeIIOXCZDr/EwM4oL\nslHT2I6v/qrSUPW15Loc25PvmoVTDfenFgVUe08wM1q6elFeWYf7njtsKUCZGcGuJsuyn5BQeSHA\n/nggop4EgxZzhIudiBdBfFAnbDX6Bwjfb2CXcMtkWEVKqXkIJ+pbMPHxfQAii1gyX6et26OFsga6\nzsqyCdp13G530HNOXV8R8DWZAY/HA5fLpV0/Xs+/0CgEg5Kh2sR+sBIrx65dwimToS//rWk8ledR\nnG8sBx5ucyHzMwgADMbM8XmG65jLjhP6/A6SFPkUnO6S4HA4lOv68i3i9fwLQSEYdAzlJvaDkVg4\nh+M9PrX8t56aRuP20h2H8PM3Tgc9lyzL2jmtnsHyyjq/Gkzm6+6sPI/WLuU57e7ujvS20OWWsfHV\njwxJeW/Vfh6Xz1uYngSDjlgmOwmiJ17ltWM5vnVLinHk3BW/SXt6fhb2PXw7lu44pL326IIpkCTJ\nz5Tz9V9XoaWzB/se+RIkScKTi6fj5XfrDc+guSZTIPZ9+JmtZLljP56NW//1aMDXX3inHi+8U69t\nv++rMhtrhEYhGJTEIpZeEDusHLvrl5b0W7+EYARqUQoApxrbMWntflQ3tCEv3YkFxWM0IbHhlQ81\nDUOWZXxwsRmnmjqw+Nm3Icsylvzir2ju9hrO996FayivrMOqsiKc3XpPwDE1tfVClmVs2n0y6NiD\nCQnLe4Xit4g1QlAIBiX9bRMXhCbW5bVjhaLx+LcoNfsomrs8aO7s0YTEC+/U45dvntHMTUUjMwEA\np5o6MGntfpxq6vC71ummdgCKryLUs3j9E6/hjx9di/i++hMhKASDjkS3iasMdPvKeDOQ9xfOtRV/\ngqlFaVmRn48CUEw5Ex/fp5lzvAxseOVDEBFuLcyzPP/0MZk4t20RSgqy0e1WhEp5ZR0mP/l6pLcX\nFfHQKISPQjDoSHSbOOBrX9nt1sanCrectP5rXxlP+vP+zL6Cn79xGm09nqDXNofP5qQ5tbBUQFnx\nA0rzn5cfmouNr3yEXe9cMFyXoJhyfnPkIn5z5GLA8akaRqLQ09OD9PT0mJ5TCArBoCTWLSdjSSwq\nnCYy/Xl/ZoEkyzIO1DRp/garaz9z4GO/aqxv1X4ONbNNXWgUF2TjjqmjlOZAy2eg6txl1F7q1K6d\n4gB6vFajSj6EoBAMWhLZJj6Uo7L66/6sBJKWQe3LiVBfKynIxrolxQCgZUOfqG/Gyw/NxabdJ3Gi\nvgUAsHl3NdYvK0Fbjwc1DW24bdIIyLKMr/zysEFIAINXSLS2tiIvz9pMFinCRyFICIaaPX+oR2XZ\nvb9ovtdgpUH2rJ5nOLa6oQ1b9tYo7zNlQ5dX1uHmcTmKcKnsO8/wDBeyUx2QZRkfftoKAHBIwJmn\n7kaqc/BOjU5n7Nf/QT8NIvrAxt/BmI9KkFQMxSzroR6VZef+rL7XTbtPGr7XUJ+HlUBat6RYEwoq\n5qzrlaXGfIZbCof5hcde7XTjQM0lEBEkn3zzysDkJ19Hj0cOOq5Eprk59rkUoUSPA8DiIK8TgFdj\nNxxBsjEU7fnmqCz9PQGDX7Owc38A/L7X+547jBP1LVhZNkETEKEc4FYCSU2Os7q2yjvnrhi2Xz3x\nKdKcErp1AkAiRRO5/onXIv0oEpLOzs7QB4VJKEHxIDMHLVFIRA/FcDyCQUg0xfmGoj1/MERlRYPd\n+7P6XoE+01CoBUEwgaT6JNQ6TsMzXLja2Vdg71RTO0ZmpuDoE/Ox6Nm3cdoi50EeGsqdHxkZGTE/\nJw0FVXjWrFl8/PjxgR5GUhKrMElm1qprAsAnTy8e9BPqUK9ua+f+zN9rSUG2wQRUUpCNBcVj8Nhd\n0yyvEfj5cmLNwmmQZVnTMEoKsrH7R3MxY8MbBs0h2Tjw36dh8uTJto4loneZeVao42x5bIhoKRG9\nR0RXiaiViNqIqNXWSAQJg5phGmg7XGJVnG+o2vMTNSorVoS6P6vv1ewnqG5oQ1uPJ+B3Hbg0iCJY\nJEnCguIxKM7P0sxI3R4ZKY74ftaVD98S1/NHQ0pKSszPade1/wyAFQBGMHMOM2czc07MRyOIG1//\ndRWW7jikCQd1Jfb1X1dFfM5YNKwZLFnWoRhqUVvRYvW9rigt9DtOb0IKRCiBRESYM2mEYV+vN76f\nf9kv3ovr+aOhpaUl5ue0KyjqAXzEyf70D1JkWUZbtxvVDW2asFDV9bZud1SaRbRhoJF2LEskhmLU\nVrSYv1cAkMh/ulHDWiOdWpgZrV1ulFf2X7e3RKegoCDm57QbcPtPAPYR0VsAetSdzPzzmI9IEHMk\nScKe1fM04aCWGygpyMae1fOiap4SyGwUjrBI5CzrUAzFqK1YsWbhVMiyrN1/TroTxflZqGls144p\nKcg29JiOhH0fNkQ91qFEPNbzdmeIrQA6AaQByNb9CQYJqrDQEyshEQuz0WC158fC/DZUMfepbuly\no6axHTPH52rPSnVDm9ZjOhz02ltTW0+Io5OLnJzYewXsahTXMfMNMb+6oN9QzU16lu44FJWwGOph\noHZR77s/+kUPFpT6Spe00hkbls3Aft/KX5UJWjOhz1qDRk/Jsmx4RrdXnEZrt1IU0OsdpHU2Bhl2\nZ4h9RHRXLC9MRGlEdJSI3ieik0S0ybd/IhEdIaIzRPQiEcXehZ9kmEMI1ZLIep9FpCRyw5r+YqhG\nbYWL+X5njlPqDZVX1mHi4/vQ1NYLAOj1yoZmQiXX5Wjv3V5Ri02vntS2f/7GaSx+9q/YXtHXQOiN\nk02a1ipJkqhDZCIaK0HAc9o87gcAXiOirhiGx/YA+DIz3wxgJoB7iOg2AD8DsJ2ZJwO4BuB7UV4n\n6ZEkCdlpLoNPYs/qeSgpyEZ2mivqB2uwmo1igYjaUtheUYvNu433e6K+GSOzjOu8YRku1Pj8ZDsP\nn9dKf6s5Er8/egHlVXXY9OpJyLKM//32OZxqasfzb5+DLMvYvLta6yOx8/B5XP/Ea0jejIn+w5bp\niZlj7o/wRVCpXi2X748BfBnAt3z7dwHYCOBXsb5+svHig6UGFV4VFvFYfSQT/W1+i0cSX7RJk2oZ\n7xO+fs3rl5UoFVsv+odpXtNlT6vvVZFlGZd8/obyqjqUV/VFMnW55YTq+ZDIRJsfZUVQQUFE+czc\nGO0xQd7rAPAugMkAfgngLIBmZlZbNF0EMDaScwv8MQsFISRiQ39FbcWjWVCsorZmjs/Fifpm7Kw8\nj52V521ff1fVBZyob8GffjhXMTlFdBcCPfGoHhvqjPsAfCEGx1jCzF4AM4koD8DLAKbbfS8RPQDg\nAQAoLPRP5BEI+pN4m9/iFYYbi1pbRIQNy2YADIMWMCLThSsdbr/j01wSPtqwELdu+zOudbrx/sUW\nQ5kPQXTEw9wZSlDcHMIXQQCiLuXBzM1E9CaAUgB5ROT0aRXjAHwa4D3PA3geUGo9RTsGgSCRiWfx\nxFhEbW2vqMWRT64azwt/YZHiALrd8oD1k04G4uEjDGp7YGaHr2RHoL9sZo7INEREo3yaBIgoHcBC\nADUA3gTwd77DVgB4JZLzCwRDjWiz4AOh9onQs2n3SdsrU1mW8R9HL2hOZpXLHW5c6XBjRWkhPnl6\nMVwSoVdEs8adgUy4iwcFAN4kog8AHANQwcx7APwzgMeI6AyAEQD+fQDHKBAkDHbDcMOJYGJm3Pfc\nYZRX1mFl2QR88vRirCybgPLKOtz33GHbk05Hjyfga7JXhsfjgXuo1vVOICQCHA5H7M8b8zPahJk/\nYOZbmPkmZr6BmTf79p9j5tnMPJmZ/56ZRdqlYEgS7oRuJwx3e0WtQRtQtYXgdad8TnjTv4A9TYWI\nMHVMVsDXf3P0Iqase8PWuZKZb32xQCmvH8U5HIS4JCHG3j0uEAhCEm4Ek50wXKts6E27T6K8sg4z\nx+fi0QVTLJ3uLz9Uhs27qw0RS6vKirB+mX1n9udtvSBARC1FwbqlxZBlGbnpTjR3BdbQguFySnHR\nKGwLCiKaB2AKM+8kolEAspj5k5iPSCAY4kQawWQnDPeW8cNwor4F5ZV1hoqqt4wfFnA8RIT1y0oM\nYa12hIQ6Fo/Hg8bWbiEkokQtoOh0RG7oyXA54lKI0m7jog1QfAeP+3a5APw2piMRCJIE8rXvnD4m\n01BIcPqYTGSnBq+kGiwMV53wV5ZNMByzsmyC38SvN3NZObNDZZXrS21IkjRk24r2J6qgcEiRT/IO\nifo/6knHfQCWA+gAAGb+DKJ6rEAQEcyMF4/X45Spj/Oppg68eLw+qqiVZw7U4si5K4Z9R85dwTMH\n+nwUej+GKiRU81SwEiR6v8fvj/WV2vB4IjOTCIy4XIoJ8WtfHIe8dKOxx+7Uf7m9Ny6Z2XYFRa+v\n5AYDABFlxnwkAkGSwMwYnmld63J4ZkrEgkIpmNdo6PcAADWN7XjjZCNkWdb8GOWVdZoWYVXVtbgg\n21DVdXvFaU2DYGYMz1DGX15Vh6nrKyIa72BhRWkhzm69B2nO+Mb+SJIEWZZx8NQlNHd5UFKQrVzX\nJdk266W5HANaFPAlIvo1lGS4fwBwAMD/jvloBIMC0fYzOogIs4usfQazi4YFNR0E++yJCKlOa0dm\nqtOhnVf1VwSq6vrVX1WhpqENJQVKVVdZlvH82+c0DYKIcGtRnv0bHuSsW1IMSZKQmx7f2B+n0+lX\nwJOIkOlSpul0l4Rz2xZhen7gCLM0lzRweRTM/C8A/gDgvwBMA7CemXfEfDSChEe0/VQw/xjN6n6w\nHysR4T+P1vuZEwhQ9gfwJWyvqMWGVz40fPYbX/1I++yJCHdMG23po7hj2mgQKfbrk5+1YFiGy+/a\nalVXtbjf3y5cA6Ak33W5lfsrr1KEywvv1Ae8v6HE8PS+FXpBbrrt9w1Ld6J288KwrqU+Qy8+WIrd\nP5qrlFCXJHz7tiJMHZWuFUY81diOlACBTeOGpQ+cj4KIJgL4KzP/IzP/DwCHiKgo5qMRJDT6aB1V\nWKjROq3d7qTRLMzCcnvFaSzdcQg/f0PpmRBKeLrdbnhl9jMnMACvzHC73X3X2d3XIe73R+vwwjv1\nuPffDmlCYlfVBfz+2AVtLI8umGKpdTy6YAoAZTL68NMW/yquFuNUazDtqrqAOFtdEpbxwzO1ibeh\ntRsAsLJUSUz87pzxfsenOwl56U5c6/KEbZLrM/PVGvqIr1k4FWWTRxmOnZ4foItdnH6Cdr/+/wsY\nyr57ffsESYRo++kvLGVZRkV1E6ob2nCgpknpmRBCeDocDngD/KC9rNiqmRmtXW7srDyP+56rhCzL\ncPve9MGnrdoEDgAjfH4NNct6V9UFQ5b1rqoLWpY1M2vaQTh4hnDTh7/9U2nA1y42d2nfYUFumrKT\nlOfgpXcv+h3f5WFbQjXVSTi79R4U5ysxQU5ftBIz48VjFzQfkhZs4PuuVWovtVudFp+2dA9oCQ8n\nM/eqG77/F53nkpB41RsaLJiF5aS1+7XOgdW6hjzBhGeoH/Lm3UqZDlXnOFHfjOufeA1XO91+P9gU\nByEr1alzYAbPso5HRMxgJy8vD2kBTDmdvuJUSmLiXK28yaS1+9Htsf4eL3f4R4EV5yudJdN9/oYs\nXxj03ofnYWSmCyOyUjRBoZoFVR+Smg8zLN2Js1vvwaqyInQHEPYSDUBRQB2fE9FydYOI7gVwOeaj\nESQ8dusNDWWICOuWFBv27f7RXMO2WZgCxvDSYOx654I2QRSbHJfm6aHXyzh2/poWg//yQ2VYVVaE\nnZU+ja/yPFaVFeHlh8pARJAkCanO5BDqdkh1SvB6vVo2s14TAwCJ+vIStHLqNlDNUzePywUA1DQq\ni4gut4zpY7Jw/20TIEkSiAhLbixAU2sPNu+pVgIdJg73O9+wdCeOP7kADocDTy6ZjjSX9dQt88AW\nBfw+gLVEdIGI6qEk3z0Y89EIEhq79YbsnivYdiKj+iT03Lr1oGH7vucq/WouqX6LcPIOzKGuVjD6\n6vuoSXd69Ml2RARPILtXEuL2yHA4HJgSoFaVfr/VIsksciUAGS4JG5bPABHhzmmjMG10huko4+f/\nWnUT0lySZs7dVXXBT5jfe8tYTWt0OBx44PaJGG4KSBie4cK3ZhcOXHgsM59l5tsAlAAoZuYyZj4T\n89EIEppA9YZWzS0Kq+3nYI6c0vsk1Dj34RkuXO10Y3iGSzMNnKhvNtiZ4+307+5WHK2hND6v1xvQ\nP5IMuCTgzFN3Q62SQZLynf7xB2Uozs8ymHtWlk3Ayw/NNdTiUhdJZ7feg7x0p5/vWAaQ6nJoOSvN\nnb04fanTcMyppg4tr2XTqyfR1NrjZ0rqMZm19M8SM6Otx4urpoCEq51utPV4Bk6jIKJUIvoWgIeh\nlABfT0TrYz4aQcKzZuFUg+1dFRZ2W3HGMnJqILQSSZKwoHiM5pNQfQfDM1z49pxCOBwOrF9Wgpnj\nczEi06VNPDsPn8f0/CzkpDlDrvgmDnMp9uwwTEQul8uWxpfsWdQ1m++Gw+HAt24dBwCQQHA4HNiy\nt8ZPe9uwbIbhOTcvkjTntolrnW4taunY+WvW42hsx6S1+1FeVYcVpYV+4coAMG1MJs5tW6SZH/d8\n0KDVccpKcaCkwFgco6QgO2QJmEixq6O8AuBeAB4oZTzUP0ESEk3bz1hFTg2kVvLYXdOwZ/U8w75j\nT8zHj+/u6+Tb0Nzt1wb0VGM7fn+sPmQZ6E+uuRV7dgBnqRXqBPJ/j9fDQUBWimL/fmLRNKQ6CX84\nruRnJLMzOzdN0nJJNn/lJtw/eyzcMmvPoNkfZDan6hdJRIQLV7ssr+OUSHN4q5qnHnOeCxH5hSsD\nwJyJw5WEuxQHJAJSHEpehdfrxe+OXEB1Q5thQVDd0DawGgWAccz8dWb+n8z8r+pfzEcjSAqijZwK\nRyuJh9bBzNiyt8awTx/3HrRER0bkJTqCsfHVj+DxeNDRo5iWnv/rJ/B4PLh120H0eBidbi+8Xi9S\nUpI3WNHt7XNMP3PgY7icxlV8TWN7yHpXeiaPtvZrmAXDraYsfHMtrvfqrllqFO/Xt0CWZZy73AmZ\ngW7fd7js3w5rWuyTi6dHbAIOB7uCopKIboz51QVJiaUdfbd1ATor7Gol8dA6rMw7M8fnGmzIAHCp\nrdvy/S4HtIS6WPLie5cw+cnXNZt5j4cx+cnXca1TMTWlOJTVdFeX9So4GchOd2r+BjVHxczM8Upp\nEquJV/88MTN63NZmvPprxs9YzWs5t20RSgqyUdPYjpICJVx2ZekEvP9pK651ulHs21fsEzTvf9pq\nMG1e7XTj+ide07SUY0/M16K1wjUBh4tdQTEPwLtEdJqIPiCiD30tTAWCsLCeaPOws/K8IQs51IQe\nSiuJVxY5EaH6s1YU52dpIbLq5PLaR40AgC17avzMTn3vl+By+a8e400kSXZDiRQHaUltQF+OSiDM\nE6/5eSIiy+ZCwzJcuNbpNjzbgJLPQkRYUDwGxflZWFgyBpKkREeNyUnFyKwU7F09D5IkYe/qeZqw\nUDn2xHzD9p7V8/waFMUzl8muoFgEYAqAuwAsA7DU969AEBZmpyAAzByvxJq/V684/uxM6KGie+KV\nRc7M6PF4UdPYrl1fvWZja4+WuxCocNvM8bkDlpzo8Xjw9OsfD8i1BwICtKi0Xi+jq1cx3RARctNT\nLGti5aanGBzY5udpZdkE7XlqbO3xm9ALh2co3QF9z9iXpoxESUE2cnzazJqFUzFn0ghoiZFEeOfx\n+Ti6dr4W5EBEmFNkzKUwh18v3XGoX/1NdsNj65i5DkAXlCBgreS4QBAuZqfghmUzsLJsAk7Ut9ia\n0O3mc8Qti9z35KsRTbtM5RUA4HKbdav3vR82DpigmLq+Av/5buOAXHsgYMAyKg1QamKRKQuCQFpN\nLMDfdKkeo6emoQ2rynzPYFkR3r/YohVTVMJYPahuaENrt0d7bssr6wyLIDURUn3P5j3VKK+q08Jw\nzeHXasRdfwoLu+Gxy4noYwCfAHgLwHkA++M4LsEQRz9ZWmW8BpvQ7eZzxCuL/JbCwG1FVTp6vUgz\nhbcOy3Dh+lGZIaOeBMEJkJQcFH1UmrbQqDQtNCrPG3wQfqZLX19xMzIrk7VqzlIfWystZOfh80rH\nwQDPt/nZdjgcmDw6C8MzXLj/tglwOBzYs3qeEgqb5opLcp0VZOdHQ0TvA/gygAPMfAsR/TcA9zPz\n9+I9QDvMmjWLjx8/PtDDEESIXkNQsWMiUkNCrbbNWoe5L3U0moUsy1iy4xBqGtoM+4vzs7D34dux\ndMchVPteW1FaiI3LbzBc+5Hbx2LmTw9HdG1BH9+9bTw2Lr8Bt249ENAnpKL2d1An1u0VtWjtdmvP\ngfq85KS5DH4J83MJKCaqDctmWAoO9TW9A9zsOF9VVoScdJfB8Wx+lmVZNggBfZkRq9cjhYjeZeZZ\noY6zeyU3M18BIBGRxMxvAgh5coEgFNGUBQmWzxGrLPJA4zULCaAviaq6oQ1OiZDqIO3aTy6ejrx0\nJ9442Yj29tBlOQT+1G5eiOlj+pprqrWSlt1UYHm8GkVkZaqxkzhqZbrUC4L1y0qwqqzI8LpeSASK\nrtpZeR6tXX2mJ6vovC17awzBHGbHdX9pEip2WzY1E1EWgLcB/I6ILkEk3AliQKAJHUDUMeFrFk41\nrNTUc0frI3jvQnPIYzwywwNgyY5D2Pfw7bhhw+vo8jDae7zIy0ue7nCxRJIk7HvkS1j87Nto7vKg\nvLJOq6xqBfuEwp7V87B0xyG6nikLAAAgAElEQVQ/U02oxFHr2k6mY0yu2s17qrWOeFavm9+nN3EB\n8NN8zZrGQGFXUNwLxZG9BsC3AeQC2BSvQQmSi3hN6Oq5gm1Hc85VZUVYt7QYU5/cH7BfwymflqHi\nkRmdnZ3WBwsskQjITHHA6VSmq70P3w5JkjDx8X3aMcN8zYJU8tIcyElP0SZsvdnJDsFMlyClPeqW\nvTUor6zze/3IuSva9XLTXSjOzzKUB1lZaoyuUhdGOw+f1wRGovV4sfvJrWdmmZk9zLyLmX8BpYKs\nQBAT4jGhxwMiwh1TRykhkMtKIMsywg08mf3zY/EZ3BDi/tljtWRGmYH/54vjNCfzlr01uO+5SsPx\n17o8WonwVXOL0NztxYyxuZo5J1xTTSjTpSRJfq+vW1KsmbnUTP2/nP7cr4bUkfNXjZFU8YrOiyF2\nPz2r5q+LYjkQQXIRSWmNgSxNbq75s25pMYiUgnI3jg3QllIQMU8uViKUZo5TzHT7fcmMm3afRHll\nHU7UN2thqfqkNsA6qzoSQvkxzK9LkoQ9q+cZcnfev9gCwOgvqdF1QwSir1TQHwQVFET0AyL6EMB0\nX0a2+vcJAJGZLYiISEprDGQRwGDORiLCl6aOQl56gBZpgoh4at8pAMCJi4o/qMmXzFheWYeRWSmY\nOT5X67OhNmvKSXdpuTmxKmcRStM1b0uS5KcdFPuEg75IoJqZHatKBfEmlEbxH1AysF/x/av+fZGZ\n74/mwkQ0nojeJKJqIjpJRI/49g8nogoi+tj3b+igdcGAEs5KP5LSGvEqx2GHUNf2er345Ztn0dwl\nciMCsaqsyFDDyA6/PfopJj6+DyfqW/xeu9zeq2kRQF+zJnPE0kBgpR3MNhUF3P2juVizcBqA2FUq\niDd28yiuB3CRmXuI6E4ANwF4gZlDh38EPmcBgAJm/hsRZQN4F8BXAKwEcJWZf0pEPwEwjJmD+kNE\nHsXAYSce3UwkeROR5lrYIVg+RqBrTxudif2PfgkejwdT11dEdf2hjEMCTvt6QHzll4fx/sUWzByX\ni9//91mYvvFg6BMAfs5gc05EomDlAL/vucN+wk7VKFRhob5XH1armthU4uXcjnUexX8B8BLRZADP\nAxgPRduIGGZuYOa/+f6/DUANgLFQIqx2+Q7bBUV4CBKQSFf6kTjv4uXws2PSsrr26Usd2PDKhwk3\nWSUaeWkOOBwOEBHunDYaK0sn4OUfzvXLCwiG2Rlc3dCGr/6qasDt9mbM2oFSYVbRNEdmpRhyOiqq\nmwzlN6KpVNAf2H3KZWb2APgqgB3M/I8ArLNcIoCIigDcAuAIgDHM3OB7qRHAmFhdRxBb9JEg4RTe\ni6S0ht33xMMMZnVtAPjNkYuY/OTrAc8vAK50erWSJWsWTtV6SYea9FaUFuLctkUYmWXdP4M5Mavh\n6h3ckiRhYUk+ivOzcLm91+CjWFA8JuAiI16lZ6LBdmY2EX0TwHcB7PHti0mtZF8i338BeJSZW/Wv\nsfLJWH46RPQAER0nouOff/55LIbSbwxk9E6sCXelH0kmtt33hOvwVse+orTQIOhWlBb6mdL0vZIF\n4WGV5BZKUKxbokSVLb3Rej36hcLhlvsTAf29PXbXNOx9+HbD63tWz8Njd00zvw1AdJUK4oldQbEK\nQCmArcz8CRFNBPCbaC9ORC4oQuJ3zPxH3+4mn/9C9WNcsnovMz/PzLOYedaoUaOiHUq/MZDRO3YI\nV4iFu/pR1XN9YTS1cFqgcEY75TgiNYN94/l3cNTU1/jo+Wv4xvPvWF57MAv1gSDFYd1+lUjpDwEA\n0/OzcG7bIq00u1PqK8+Rm5GClaXGcuD6CKdER42Q06PvhmgmXqVnosVWZjYzVwN4WLf9CYCfRXNh\nUu743wHUMPPPdS+9CmAFgJ/6/n0lmuskAqqjKtHT9cN1TAfNXkWICrAWJZ6DESp7m4iQnepESUG2\nIcM1WMN5WZZx9vN2XG7vNeyvaWjDyKwUrfDamoVT4Xa7QUTo7e31O4/AGomAm8flaBnVhtckCV+c\nMAwtXb3Y58u03vfw7Vj8i78iV5dR/eiCKdi0+6ThvQw2lANPVCL9fcSzUkGkBBUURPQSM3/Nl0vh\nJwKZ+aYorj0XwHcAfEhEJ3z71kIREC8R0fcA1AH4WhTXGHDMk++6JcU4cu5KwqXrRyLEAq1+gMB1\nmrTrVJ4HSHedytDCMlgMOzPj7Y8va1VbVaob2pDidGDNwqnW4wmQVq3ff+OG19DrZZzcaJV3mlys\nLJ2A9ctKsPjZv+JUU/DihjIDM67LC/idvvhgqaEKqios/HozWJTJME+eoSLXBoJIfh/69wbb7m9C\naRSP+P5dGusLM/MhIOAycn6A/YMKq8l3y94av8lsoIUEYPQ1hCPEwl39aNdh03XKoheWsmydy2De\nr46XiOByOgD4t7R0OZVIHY/Hg14vo8cjY8bGChz9x7KIxzeYSXNK6PbI2vdj1sICsfuDz7TEOCvM\nDl2zP8PORBtJiHZ/kYjaQSQE9VGo0Ufs63Bn/uufIQ5eAkUFlZgSjwY6okEl0hDUcFc/zxz4WGv2\noiKzjGcORN6mk5kDFubzyH2+lu0Vtdi0+6S23dplPeGp+51OJ05uXIhUB6HHI+Pmpw9FPMbBhkTQ\nOqp1e2QMy3BhZ+V5TFq7H5c7lM9nRWkhzm69Bw7fTJLmJJzdeg+K85VnPNpe3aHKaAxkMqZdEk07\niIRQJTzaiKg10F9/DXIwYzX5Vje0BY1oYF/avp7+eOD7IyyPmfGX001+7UN3VV3AX043hQxn1aPv\nFCdJEuZPH4XhGcZgvOEZLi0UkZnxVu0llFfWYdPuk3C73ehyW1+vx8PweBRN49v/fgzj81LDus+h\nwHdLJ2j1i4rzszBltLEPuNqUyeFw4IuFwzAs3YkHvjQJDocDex+eh+L8bNw4Njdoromd5zzYRBtp\niLZdhlKEYjQENT0xczYAENEWAA1QIp0ISqnxmOVRDGWsJt+Sgmwt/C8nzWlwuDIz7nvuMAClhk1/\nqdLROKbDvc6nzd2Wr33a3B3Qtmw2L3ztf1XizKV23H9bIR67azq8Xi9+e6Qe1zqNnc6udrrR5utX\nrAxA+SdULwMvK9qELMs48snVyG42jpzaOB+zt/0Zrb3xmbiGZbhQ/Vmr9l3MmTQCJ+qNhRgk6hMA\nL32/zNCFTZIk7H04ePZ0rExGqrDQZ87HKhkzUU1a/Y3d8NjlzPwcM7cxcysz/wpKBrUgCIFiotUy\nxLIso7Vbab7e1uPRUvdP1LfgRH2zVhQsUBJYLOmvsDwiQlevv08AALp6PcEd4D7Ny+v14syldlzt\ndOO371yA1+vF0n877CckVNRm9+Hi9XrR1dUV0XvjCUERYpX/9KW4XeNapxvtPR7IsozNu6t9FVtb\nDKW89T2mgfC6sIVjMrKTVBlrTXgwmLT6E7uNizqI6NsAfg9lTfZNiA53IQnljNNXmtQ7dleWKXHj\nOyvPa20U9VpIvFY2/eF4Y2Z0B7Bbd7tlv5o3qtNZzWHQf07DM1y42unG9U+8BgBIc0nodstarwi1\np7E5GsYusizjp2+cifBO44fLoXw3VvkJsaTaV/EUAGaOz8PM8bl9bUCj7EJoN3gi1Ko+XppwpMEd\nQxW7GsW3oISpNvn+/t63TxCCNQunahM8AC1EVp3grXwYOWn+Se/6+jbxXNlE4ngLx44ryzLkAC/L\nDG3y215R61eT38zRtV82bN94XQ6KC7K1XhHrlhajuCAbqU5Jm2RqL9nvVz11fQV+c+Si7eP7C4Ly\nvaSlpcXtGitKCw3bLz9UZugHbXYqR0Ko4Ak7q/p4asKRBncMRewm3J1HEpqaYhGbbbUi2rK3xm9F\npKeiutGvEBoAnKhv1to/RrKy0duQrbbtov8ctlfUorXLrYVAhtJ2iChAF2FFVVXP8VbtJa3q5vpl\nJX7VNAFgxkZj1da/1TfDKwNb9tRg/bISbNlTg5qGNswcn6cFCEQbhZMI9HgVgRoq+a/qkS+g9Nm/\nRXSNY6Zs9c17qv0mzVg4iq1MRupzbTeUOl6acKjxJROi9GUAYlFqI9SKSJZlPx/GyrIJlkLCTLgP\n69yfHsStWw9qkUJerxe3bj2IuT+1V+pZRf+5MDNau5Tkufueq7St7QQatbqfua/q5s7K81rDGkAp\nOf3xlruQ6pTQ45GR6pRw5qm7MSzDBa9PBqjvUc12N49TOtD19PSEda+JjFVknBl9VJgV9xbnKH0i\ntNIZpIXDqsXrzm1bFJdaQ4H8d+brPHPgY7BpacFgv1DqWIeg2h1fsmDXR5FURJKlbIUdO6dZbd6w\nbIYWXaKvY6/+eFWsVjb6LFf9ttfrRVevF1c73bh160Ece2I+bt16EFc73RgO+5qF1eei/oj12o6+\njpP6Pv3/B9MoZFnGtv21AYXlx5fa4XK5cNPYHPztQjN6PLJWwTXNJYFlGT2m+fHYucsAgLV/iGx1\nHW+O/Xg2nn37U/z22Ke2jif4O46tCHXMU1+dCUmScNeMfACNWFiSD4fDgYUlY8CAFlasrupjHdQQ\nKpmOmdHS1eunSZZX1mFl2YS4Zl/bTfZLFoSgsCCWjqxQoXtWavMff1CGzXuqDYKiuqGtz0lr4az7\n+q+r0Nbt1hq6yLKMpTsOITvNhRcfLDUIB9X5OzzDhWNPzLdtfiKyrqdkbiyjr9tkNkWFWuVOWfdG\n0Nc9MtDb24sbxuXhWJ0xXJOY0W1x+uqmTk2IJSKpqan4yd2TbQuKUdkptnwUWVlZQV93uRRf2JqF\n0/DI/Cm6RQZhTtFwg+kwsHiPHDsmo0A1wELVBuuv8SULoRLuHgv211+DHAhi5cgKN3TPXN9G6aGr\ntEZUf6xmZ50sy2jrdqO6oQ1LdxzShER1QxvafCauX/z5LJbcaGztseTGMfjFn8+GdS9tPR6/EiTm\n1b++36/ZFBWLRj9T11dg5+HzSHEYv4suz+A0B7jd7rB8RQW56QAQMuoplFDWP4P6+kqt3W6UV9UZ\nzKXllXX9HjxBRMhJd2FVWZHhmP6sHhtrk9ZgJdSvNtv3NwvAD6B0oBsL4PsAvhDfoQ0ssYjNtmPn\nNJeUAID3LjQrzeOXqs3j52Jl2QTkpqcYnHzqik+fPVuta+JenJ+FPavngYjw55pGvwie3xy5iD/X\nNNq+JzVia9qYTL/XVpQW4pOnF2vRMpqfwHfvTy6eHvOQzl4vayUj0gbx77e1tTWsCegWXb/oYIQS\nylavExFOftqC4RkuQ6bz8AwXTn7a0u8T5aMLplj6KAZD9dihRKhaT5uYeROAcQC+wMw/ZuYfA/gi\ngMJg7x3MxMqRFSp0D4ChpISacPf+xRZDrV7VdxGsefyzB89gzqQRhn1zJo3AswfPwOv1orrRqAWo\nVDe2hVx5qjAzbtz4Bk43+afQ/O5IvaIxkP8jdfLTZiz7t8OQZTnmK9JuD+P6J15D9+BUJgAAGRkZ\nYU3A5gzpQFiV99ZjpcXIsoxzlztw1SLD/dzljrjnbuix0q5XzS1CeWVdUjqUBxK7PooxAPSxeL0Y\nwi1KgzmyzL0NQjnU1iycClnuq7qprsrV2kOBSkqE+gmYHcTBnH6SJCHN6UBHr79ASHM6bJs9ZFmG\nN8CP0yOzlpwlEQy5Eu9fbEWPRzGH/fHB2baulUykp6cjJSUFaQ5Y+lgC8fTrwYsohhIUVhARFt8w\nBi+8U+/32uIbxvSrRiEcyomDXYPxCwCOEtFGItoIpbf1rriNKgGwqlqZk+bUSm0A9kJmt1fUGjpa\nqXkU6ntmFlqbEXo93oDXMSejybKMP71n7Qg9cu4KAODDjXdh2mijyWja6Ex8sGGhX+SUHv22JEmY\nPDIj4L1q72ElMufMU3ejpCAbPR4ZEhSH/PSN4YXjJgOqoJ5+Xa7l6yOzUnBu2yItY//ExRYtZDjN\naf0THpnpCrniDvS6Q7JeOATaH09CVY8V9A+2BAUzb4XSDvWa728VM2+L58ASAbPm0NrtCav2S6g8\nCkBZGeWlG1d+KZLiIFYFjDn34q3azw0O46f2nkJzlwepTuMKqzg/Czk+v8Z9z1Xi9CWjyej0pQ7M\n3nZQlw19Gkt2HMLP3zgNAPj5G6exdMchbK84rd3PRw32MpsZwOQnX9cc34M/zS1+dHQo34vXa/0p\n5fuinDYsm6EJC5WcNGutwSFJWlSTFWq9KL/9RMhOc1hW4c1OcwzIKl44lAeecHTTDACtzLyTiEYR\n0URWWqImBepKxlxvyJwzYPUewDrMlpnxH0cvoLnLWCSvV1YaxVi9BwBmjs/FifpmQy0oAJg0Mgs1\nOl/EnEkjsGHZDDAzLl7rtLyvKx1ubN5djfXLSvC7Ixe0hjSPzJ+MAzVNqG5ow6W2Hjwy378lpSA2\nZGZmwuv14sPPlO8uxUGo3nQXSja8gV6vIpzVXBdzWGggAexlhizLWv0rMyOyUizNprIs40DNJUsf\nxYGaS3h0wdSYRK4JBhe2vnEi2gDgnwE87tvlAvDbeA0qUXnmwMeWvZ6DNdyJNMw2K9Uow/VlDaxW\nlsUF2ahpbLN0+gGA02H9VTskoLyqTmlG4xMSNQ1tuP6J1zRt4HJ7Lyat3e/XQ0IQGxwOByRJ0p6s\nSSMzIEkSJvnMfGptJ33L2E+eXoxVZUXad7biNmPU2eX2Xkx+8vWABRivy00LuLi5omtKpD/nlY5e\nsZpPUuwuDe4DsBy+irHM/BmUsNmkQV+uQs/OyvNo7QocX65GMunZ9OpJbTU3Ns86acocEhiqQN7V\n9l6lIJ6vAOG6JcVKNrevp4Aae28mgLVDw1wcThB7UlJSIEkSzm5bhOljMnGqqQOT1u7HqaYOTB+T\nibPbFsHhcPg5dtctLcbIrBQAAEl9Nnw7fGHCMMO2+mwREYpGZGLamEytCOCGZTMwPT8LRSMyhaBI\nUuwKil5WniQGACLyD6RPAgJlpwbcz0oTIjX66JOnF+Pmcbkor6rDfb88DAC4Y+ooPx9FqoNwpcON\nVWV9K0e1npIsy9o59TS19aCmoU3zhai9uUuuU+oc/fEHpSg2tWAdkenCMPO1Tc7Ro+cj6+WQrJx5\n6m6/oIFgDEvvcxBLkoR9jxh7TOx75Euaqcfs2JUkCUfXzsfKsgkor6zTHNxq7aZgvHehL8RWHxzB\nzCguyMbppg589VeV2rN0qrEdJdfliJDUJMWuoHiJiH4NII+I/gHAAQD/J37DSjyICLnpKX4mH30i\nXIB3+v6r1K7p9S3hLzZ3QZZlVFQ3+fkoery+aCZfX+k+QcQ+f4PSTKc4P8tQ1C3NKWkTxs7D57Uq\nmwCwZa9SSVXPlQ43rpmvbWo8XdPQpkXdfPOL+cE+Irz3z2VJX2Vy8pOv+wUNBKOl22uIXlu6w9iT\nW820VzE/Z5IkYcOyGYZ9NY3tWDW3COe2LdI0DjP6Aoz64Ag9J+pbYt5aVDA4sRv19C8A/gDgvwBM\nA7CemX8Rz4ElIoGyQQPtV7KqyzSNYNLa/ahpaENxfjYut/fi+ide08pfaJ3DfOUK0lwSdlVd0K0S\ns3HH1FFwOBz41uxCTPfVV5q0dj9qGttRnJ+Ff7h9ouH6qqAhIuzS1ZoKhZrtnOorkdHui+ryeoIH\n+d/ys0oR3eTD7nSq5pvoy66oVVvVQpBmYaHHqoKAvsnV2DzF5HjzuBytOrF5gGqJGHOlXj1CSCQ3\ndp3ZP2PmCmb+R2b+H8xcQUQ/i/fgEgnFjFSpZInqTELllXVaiW31OD1EhPXLjM7sOROHG7aLC7K1\nBCK12Y7ZCVnT2IbWLiWHY83CqX7n2P2juWg3JdTtqrqAja9+hJ6eHsMEfnrTgqD3qmY793gZpNt+\n6f3Pg75P0EcgA83ITGNORGaK4siWJAnZaS6UFGRrhR33rJ6n9FP3dUP0u0aACgJqq10AuHPaaKwo\nLcSffjjPEAhxx9TRwYMjTOYrkQmd3NgNj10IJepJzyKLfUMcnzDw/fu3C1eVbX1i3O5q5KT3Ne2R\nZVn70aqUVxlXbDUNbbht4gitaN6rD5Xi3ueq/IrvqRrCV355WCnzoWP2tj/jaqcbq+YWYd2SYm11\nuqvqgl+00rQNB8K8Y0EkuAhw6z7AYelOfHP2eIO5SG+2fPHBUkOpeFVYBApHtZO5bFUBVd+pTjuX\nSQeqaWzHyrIJ2LBsRkxaiwoGN0EFBRH9AMBDAK4nog90L2UDqIznwBINtTif2mlNr55/oVCJILnv\nuUqcqG/GqrIizTGoTtjqBL5kxyE/XwEAeOW+hkJznn7TL44dgN+kX1yQjb2r52nXGJ7hwpOLp2sT\nTKBrhYODAK+QFhHhZviVhm/v9WoTt9WEbRYKoXIWbJXqDpKwpi5uzNF8etYtKcaRc1e0CDpB8hHK\n9PQfAJYBeMX3r/r3RWb+dpzHlnCoP249qv9h4uP7tGJtqsahRR75bMaSJGFh8Ri/6CMA2PdRE5gZ\ny/7tMK52uv1s3OZImuL8bNToKsWWFGTj/tsKtXIQzx78GByDAm5eVrJyz269B5NHxK9H81AjzSnh\n5nE5WotYqz7OsZp0Q2Uum01G5u336pXItpVlEwzBEXs+aPCLoBsI81Oo8QviT1CNgplbALQQ0bMA\nrjJzGwAQUQ4RzWHmI/0xyIHC3PnN4/Fg6/7ThmPMobFqqKKqcagltvUhji1dvX4rfTWpre+8wHdv\nG49N996Ie3f8FR98Zjze/GPRmyhkWcYbJxtxyqLKqxVfv2U0fvq1WVi+4218+JmxRIcEGJodCezR\n7ZHR62FttW+10u8PrHq2b95TjexUJx67axqICHdMHY2bx+VqGs7eh2/Hkl/8VQuWACJr2hXP8Qfq\nyS6ID3ajGX8FQD+DtPv2RQUR/X9EdImIPtLtG05EFUT0se/fYcHOES/MPaY9Hg9mbKzw629tFSGi\nJzvViaf2ndJMUZt3V2NX1QXMHJ9naEpkxe4PGuHxeLTSDgTg4y13oTg/G6eajBP6xlc/MiRNpTrt\nF3BLSUmBLMv42BfWSQDObr0HIzNdQyaK6dTG+VEnD35nzjiceeruoMeceepurCxVHMNXO3uDmoDi\nTbBaYxU1TVok1aMLpoCor8KAJEnY+/DthnMNhJAIVStNaBb9h11BQaz7VphZRmzaqJYDuMe07ycA\nDjLzFAAHfdsxx0qdVfeZe0x7vV7M3vZnrQrq2nuUlcwJX9LSmOzUgIKjoqZJe9ABYN9HDQCAmeNy\nwcz4rLkr4BivdboxZd0buiwKpVWoWs/JQcC5bYvw3dvG44V36jFj/WvaCvaOaaP9IlkC8ZsjF5We\nDr4OcbnpTjAz5CH0Q0xJScG6JcVwRDDXpTqAG6/Lwuav3ARJkqDmJI7MdCm5CplKAT2n5MtrWD4D\nK0sn4Bu3FkY9uUZjdlErHquta9WciDSXhJqGNmzZU6MtXsor6/BW7efa78AcgDEQUU96k51+/CKn\no/+xKyjOEdHDROTy/T0C4Fy0F2fmtwFcNe2+F30lzHcB+Eq01zGzvaLW8OCrGdRqmKvD4cCkkRkg\n9JldVOeyDGDr/tNgZpxqbAUAeGQZsizjxaNKDf90l6SFQNY0tMEpkfagN7X2AFDswl6vF5faev3G\nZxdmRai9+r4ifDrdsqYBPTJ/Mt45ezmi8zZ3eTD5yddxtdMT+uBBgizLcDgcSHOFnxLY4wW6ez1a\nVNpDd07G9DGZuNzhVmpkdbgxPT8LD905WanZRIQNy2dEbRqxek5DlbXXo1Y8NkfPqaHXWhdCnyN7\n5vjcmDXtihWR1koTxBa7v5rvAygD8CmAiwDmAHggTmMaw8wNvv9vRIwbJFmps5t2n8SJ+hacqG/G\n5t3V8Hq9OHe5M2BoaHmlUkRPXYFf6VCESZcvqznVodhSq84oeQce2f9M719sxZR1b0R1LzIUDUOf\n2f3UvlOQZRmztx3EqaYOpT3q+v8W1XWGCkSE782b5NfK1Wkx56S5JJzdeg+G+cptn7ncrZlq1iyc\nitsmjTQcv+/h2/HYXdMM14qGWJhd9DW/9BQXZGPFbUYznBoKK0lS0K6MA2F+irYlsSB6bJmPmPkS\ngG/EeSxW12UisnwiiOgB+IRVYaF923Og0t8ryyaAQIbS3QRjHoEE4Du3FWLXO30hqsMyXLhmCmVt\n7vYanL+pTsmvNEa8MIfuzpk0Al1dgc1byUBmitLFj5nR1uPB6aaOkOHKf/+F6yBJEo6t/TJmbKxA\nj0fG1v2nsX5piRIibcqF2bK3JqYr3VAl6u1cRx+xpMfqfvV5FHZCbvsDs3azfmmJyOkYIIJqFET0\nT75/dxDRL8x/cRpTExEV+K5bAOCS1UHM/Dwzz2LmWaNGjQrrAlbq7IZlM/wyqM0SSgbwn0eNyWtm\nIWFmWIar34SEFeWVdfji//vOgF1/oBmW7sT35k3UIo/Mq+UUX/n1meNyDSVU1Ogvp9OJf5hXZLDz\nq4JYe0+cTDPRml2ICNmpTj+NYliGy09Y6Bthqe81n6u/CZRQOFDaTTITSqNQPVrH4z0QHa8CWAHg\np75/X4n1BazU2U27T1r0mvAXFr1hzvksey21DkHsqN28EFv3n7bsl3Gty4OK6kY8umAKJEnyWy3f\nOW00bhmfhw3LZxjKreSku7RwzA637LcqB/ra2Marj3Mgs0s4GkVbj0dL+Fy/tMSQXLeitBAbl9+g\nJZGq+RSJRKJoN8kODaStj4j+E8CdAEYCaAKwAcCfALwEoBBAHYCvMbPZ4W1g1qxZfPy4PVlmpc6q\nPxRASaB7csl03Lr1IK52ujE8w4VjT8zHkh2HcMpXwK84Pxt7Vs/FjI1vBGwMI+g/zm1bBAC4YeMb\n6Oz1avZ29XvNz0lF1ePzA04u5k5v5m1ZlrW8ApXpYzINJcCtusVFQzCzSzjmJ6s8BDVo408/nKvt\n27T7JHLTUzQHfKjPRDA0IKJ3mXlWyOOCCQoi2o0g5X6YeXlkw4st4QgKINCP5zAApdorEeFr/6sS\nZy614/7bJuCxu6ZpDhaSciwAABnqSURBVOKOHg+6dMJBor4KoCoZLgkfbbo7JiU0BMFxEHB6y91w\nOp3YXnEaLV1uLXHMagIMF/XZOFHf4vfazPG5ePmhuXGbQGOVbGY16QP+pTzUbZHkljzYFRShTE//\n4vv3qwDy0df+9JtQNIBBiZU6+/JDc7X/B4CXvl9myMxWWlNmorXbrWkWgCIkMlIc+GjjXbh16wFc\n6XCj0y1rGa396chONiQAXyjMg9OpPMZrFk6zVQBvsBArs4sdf4O6Tx9tBcBPkxGaRXISqoTHWwBA\nRP9qkjq7iag//RYxx+rHY9au9AXZZFlGW4/HICRUOnu9mmBQW1mq/MO8Ihw8/blBsxA+i9ggA7hh\nbJ7fZKonFpPazPF5ATSKvKjPHYr+diqHE20lzFPJg908ikwimqRuENFEAEOqHWqo5CYiwtX2npDn\nue16Y3z9obNXIHuNfSJiUaxPoPC3uqDuq6ghirSz4eDFTrRVtMmAgsGFXUGxBsBfiOgvRPQWgDcB\nPBq/YcUfvfbAzGjp6g2a3MTMtspt6xsbrSybgBP1LTh9qdNwTHO3F3npTnz05B2xvq0hT5pTSYRT\nK/B+2tId9+SrRxdMsYiIo4CdDQc7oZLcRA2m5MNuK9TXAEwB8AiAhwFMY+bX4zmweLK9ohabXj3p\nJyxmjs8NWFNGkiR8Y9ZYy1pBN4/NMRT4UyvKrl9aErBnsVOS0NMTWkMZSnxn9lic2jjfsK9mw5cN\n26c2zsfKIMX7pozOUIrWrZ6H4oJsXD8qK2TPhmjQJsFKU0mLyoEpaRFv7JTwEDWYkg+7rVAzAPwj\ngB8x8/sAColoaVxHFieYGb8/dgHlVXWasNj06knsqrqAhuZuw7H6h97r9eI/jl601CouXO2ELMt4\n+aG5+O6c8ZpJQnWAm/tPDMtw4XJHb9Ilwq1dNA2pqalI19XMKN70Z+3/052E1NRUrFtagjRf5b1i\nX/9o9TNsaO3Vai7tXT0PLz5YarhGNEX0rEi2pC+79ytqMCUXdpdiOwH0AlB/lZ8CeCouI4ozzIzh\nmcoqv7yqTsm09ZVj6PUafQf6FaPD4UBbt7UDurXLA4fDAVmWsfvDRvzh3XrtWjOuy/ULkU1WR/bT\nrytlrKfm5/i9Nml4Ch6443oQERwOB24al6t18AukQZg1iXjZzdcsnGqYBNVJcqiGitq5X1GDKbmw\nKyiuZ+b/CcANAMzcCfg1YRsU6CcdPWokUiB12+12I1BunRdAd3c3Zm09iGudbjS19mhVXPd++Fmc\n72jw8MI79Zj4+D6/ft8AcO5qL948dUmbaF76fpkmJAAE1CBU4m03T4SSFv1JsPtNtAqzgvhjt6dE\nLxGlw5d8R0TXAxi0BnYiwpyi4YaVfuHwdNw787qATer1ne6smL7xoPb/WSlKXSFZlnGlPXAZ8cqH\nb0HZL96L8m4Sh5oNXzaYkj7eche27KnBC0fqIQEhmyCd/KwVHo8HLpdSsTWc/tGxKKInsEcg8xQQ\n+zImgsTArqDYAOA1AOOJ6HcA5gJYGa9BxRPVJ2Gu/vn+xVZDXLw5uclrCnENRku3MiVu3lMdNFJq\nKAkJQGkOlJcmoblbBkEx15GkfH52AoJlhpY8Fwnqd6YKCUDYzeOFqMGUXIQ0PZHyzZ+Ckp29EsB/\nApjFzH+J68jiBDNjz4dKuwuzo3Tvh40B1WY5jNwHBjBp7X7LInVDhQk5DsNnBwDXP/EamrtlTBmV\nbvgM8tLtTf43jwvcFtYOwm7evySbOS6ZCSkoWPmV7WPmK8y8l5n3MHNkrdMSAEmScP2oLEtH6aSR\nmZp5Y3tFLTbt7guhDUejSAZ+c39JQH/P62vuNGx/t3QCivOzQp7z5iD9w0Mh7OYCQfywq+f/jYhu\nZeZjcR1NP/Hig6WQZdngKF1YPBptPV5tQnmr9pLS9e5CM17+4Vw8te/UQA454dCbHGYXDTP4e5bu\nOGQ4trXbgzmTRhiqr1rx/sXWqMYj7OYCQXywKyjmALifiM4D6ICvVQMz3xSvgcUbvWNUqdvv1Wzb\n65YUo8etaBAnLrZg4uP7BmKICU1OTo62it9VdUHrGLd0xyFUN7ShpCAbe1bPw+Y91YaOe4EoKcjG\nl6aMjGpCF3ZzgSA+2A2PvRvAJABfBrAMwFLfv0MCLamoTDFVTFq7HzWN7chLCx7pNFh5+evXYWxG\ndOfo6uryW8VLkoQFxWNQUpCNhSVjIEmSITt9ZdkEg19jZFYKzm1bhFVzi1Dd0Ia2Ho8IYxUIEpCg\nGgURpQH4PoDJAD4E8O/M7OmPgfU3zxz4WCu9odLcPTT9EllZWXjjx3dixpa/RHyOtLQ0AP6r+Mfu\nmqZ1kwMUze3bcwoNfSLuKhkDAFhY3CdMAGEiEggSlVCmp11Qkuz+CmARgBIo9Z6GFGpRQDsmkqFA\nfn5+1Ct3NdcB8F+1m/MdzH0i1iychkfm9wkTYSISCBKbUKanEma+n5l/DeDvANzeD2MaEE5caB7o\nIURE1SNfwHfnjA/rPampqUhJsS5WaJf09PSwjg8lTISQEAgSl1CCQitKNFRNThqDdKLKy8vDxntv\n0La/M3ucX0VWM7IsGzSCcMlMcYiJXSBIIkIJipuJqNX31wbgJvX/iSjyWMYEg4jwpSkjMSwj8slz\noHC5XJAkCQ9/eTK+M3scNt93U8jsZodDmegfmT8Z00YbvdrDbCTHFY7IEHkJAkESEVRQMLODmXN8\nf9nM7NT9v38J0EGEfqKTZRnP/eVswlV1/cYtY3DiJ3ODHtPRobRdfeyuadh83022V/rMjLdqP8fp\nS52GaKRrXR6MzErB2a33BOylkeIgoVEIBElE/Dq+JDDmctSyLMMjJ94KectXb0Zvb+CiggAMzY/U\nyTtUAcO+1335BlAm/lSH8jiMzUuHJEn45q3jMSzDqGFMH5OJO6aOFoJCIEgikk5QWJWjXv+nDwZ6\nWJY4HA7k5gYva2H1OhFp3fbMzByfCyJFMPzxB6VK7kil0qXsxMUWrCydgD/9UNFi2no8uNZpdE2d\nauoQ7S4FgiQj8lKdgxSttAMby1EnIh6PB6mpqUGPsXqdiHDH1NGYOT7PEPJbnJ+NO6aOAhGBmbFl\nbw2y04yPwIblM7TXT9QrfSNWlRVh/bISbN6ttARV9wsEguQg6TQKQEmu88qJl0z33j+XGRzqDocD\nzIxVc4sAwFDsTt0OtLJ/dMEUv301jW1o7fIYCugdqGkyHKPvi3zH1FFYWTYB65f5aictK8HKsgma\nsBEIBMlB0mkUzIzfH61DU1tw2/9AkJGRgeNPzMesrQeRkeLQfAnhFrtTBUF5ZZ32vk27T6K8sg47\nK89jZ+V5AEp9peqGNu0YVXgA8OuHLBAIkpekExSyLCekkFBxOBw4/sR8g0PaqtjduiXFfoUN9a+b\nhcuGZTMAwGCKWlgyBnMmjjAKIFYEEAC0drtRXlmnvaYXPvrrCQSCoU3SCYqurq6BHkJA1CQ4q6gl\n/aS8vaIWrd1ubYJXNYicNBfWLJwKwF+4AEp0k563ai/j5nHGKGe13pVoLSoQCFQS1kdBRPcQ0Wki\nOkNEP4nVecPpVNffuN1KHkewiCKrqC3VZGSORlInc+2YSl1Tn7IinKhvxq6qC1qDJlVjUM+jFxYq\nQkgIBMlHQmoUROQA8EsACwFcBHCMiF5l5urg7wxNZ2dntKcAAJz4yVzM/OnhmJwLAJykRDBZaQd6\nIlnpWzb1Waac4736ayivrNNMUvpjArUWFcJCIEguElWjmA3gDDOfY+ZeAL8HcG8sThyqvIVdYikk\nACAvMwWyLAfUDvREstJfs3Cq4RhVWLz8kDHz2ywkRGtRgUCQqIJiLIB63fZF3z4NInqAiI4T0fHP\nP//c9onDObY/udzei0lr92sTc7CJP9BK32xWM0/mVuezOo9qdrKKtlo1t0j0jRAIkoyEND3ZgZmf\nB/A8AMyaNcv28jaRndkqdoSEXqCo20fOXcGe1fMgSVJIE1aw86hjEK1FBQIBkLgaxacA9E0Wxvn2\nRU1GRpQ9QKG09Dy1cX7QY06uuxOpDnsTqrlqbTDTjtVKf92SYi0nYsvemqAO7mDnsdIYRGtRgUBA\niWhrJiIngFoA86EIiGMAvsXMJ62OnzVrFh8/ftzWuRsbG3HbM+9GPDaHBNRuuQderxdT11dEfB4z\nK8smYMOyGX6r/GCahf41WZaxZW+NoSSJnVBW83lEfoRAkDwQ0bvMPCvUcQmpUfiaJP0IwOsAagC8\nFEhIhMvly5ejer9XBrbsrbE9md50Xbbl/pFZKTi3bRFmjs8D0JfjYNcPYNUxLpJQVqExCASCUCSk\noAAAZt7HzFOZ+Xpm3hqr80YS9XRkzSx8Z/Y4bbu8sg5T1r1h6721lzos94/NSwcR4eWHyrCqrAg5\n6S6tqqvqHwiHQA7uRNQYBQLB4CJhBUW8GDVqVNjvyczMxLu+iqlWzXymj8nCmafuNnyYZ566G8UF\n2ej2yDC7KpSeDqP6BMMyo2AId1UvQlkFAkE8STpBkZWVFfZ7bnjqLVQ3tKG4IBtLbsw3vJaX7sTe\nh+fB4XBgVLZS8nt0VgocDgdmFw0DAHhZ8UGoE/ippg609Xi0CTxac48IZRUIBPEkIZ3Z4RKOM7ul\npQU3P30oouusKC3ErqoLfuGk6jYAbHz1I+yquqC9Z3RWCkZkpWDvw7dDkiTIsoylOw4hO82FFx8s\njWgcgRCOaYFAEA6D2pkdTzIzMyN+794PG7GitDDgqp2IsHH5DYb3LL6pADWN7VrY6pa9NahuaEPJ\ndTkxNwkJx7RAIIgHSSco9KW5rZier/gbUp3KcQ5J8TeUFGTjcnsvjp2/ZjAZ6R3PVg7lE/XNKM7P\nws7DSrvRnYfPo6QgGzlpTjGRCwSCQUHSCQoASHcptz09Pwvnti3C9Pw+v8XuH5bB6XTi5nG5cJBi\nOnI6ndizeh5KCrKRneYyCBu/Cq2HzRVaW1DT2G64fnVDG1q7PcLJLBAIBgVJJygkScIDX5qE6flZ\nONXYjklr9+NUYzvy0pWw2W2v1YKZMWNsLrwM3HPjdWBmSJKEPavnBfQrBKrQurJsgl+kVElBNtYt\nKRYahUAgGBQknTNbRZZlTFq7X9s+u/UePLXvVNiZzWb0DmRm1lqQmhENgAQCwUAjnNlBUCdwPZv3\nVGPdkmLDvkgmckNHOZ+WUVJgzM4uKchGdqrwUQgEgsFB0gkKZsZtTx/ErqoLWFFaiE+eXqyFvc7Y\n8Lrh2GiT1ZgZbT0eVDe0GRLhqhvaDHkUAoFAkMgM2jLjkcLM8MrKBK1GMB09fw0A0OWWsaK0EBuX\n3+BXcltvTrKbqxAoEQ6ASIQTCASDhqT0UahJb9UNbdq+kVkpWHJjPjYuv8HQ4U3fz2F7RS1au91+\nXeAC9XxQEYlwAoEgEbHro0g6jQKAFsGkd2YfXTtfS5oD/Jv0MDNau90GLUMfDhtKswi2LRAIBIlM\nUgoKVaPQs3THIexZPc/PGa3/f9VstPPweU1giOglgUAw1Ek6Z7be7FRSkI1z2xZp3eGW7jhk6Dtt\n1XM6kp4PAoFAMJhJOkEhSRKyfSGran/pPavnYWRWCq509PplWm+vqNXeK3o+CASCZCTpBAUAvPhg\nqSYkAEVTWHJjPppae7SJ39xzWvR8EAgEyUpS+igA+NVr2rj8BkiSFNT/IEJdBQJBMpKU4bGBYGZM\nfHyftv3J04v9BIAIdRUIBEMFUcIjTOz6H0Soq0AgSDaEoIDoOS0QCATBSFofhR611MbKsgkG/wMz\nC/+DQCBIev7/9u4+Vo6qDuP496G8lBTktVYsLaJCKiIWrMibWBEQNAZrhEBCaCUGmhSLgAioicWE\nSHxDEQOiVFHBFizWIgm0hCIi0JbSN9oKVqgRUiiIIBVSY/vzj3MWhuvu7OX2bnem+3ySm+7Ozs48\n96R3zs6Z2d/xGUWBUOlzM7Ne5I6CQnmOB9a+8fbYB16/PdbMrFd56AmX5zAzK+MziszlOczMmnNH\nkbk8h5lZc13pKCSdKmmlpM2SxvV57TJJayQ9JunjWyOPb481M2utW9coHgU+A/y4uFDSQcDpwHuB\ntwN3SzowIjZ1MoxnojMza60rHUVErIam32o+BZgRERuBJyWtAQ4HHux0pgtOOPAN5Tj6TlxkZtar\nqnaNYiTw98Lzp/KyrcLlOczM/l/Hzigk3Q28rclLX42I3w3C9s8BzgEYPXr0lm7OzMxa6FhHERHH\nD+BtTwOjCs/3zcuabf964HpI1WMHsC8zM+uHqg09zQFOl7STpP2BA4CFXc5kZtbTunV77ARJTwFH\nAndIugsgIlYCtwCrgDuBKZ2+48nMzMptExMXSXoO+NsA37438Pwgxum0OuWtU1aoV946ZYV65a1T\nVtiyvPtFxPB2K20THcWWkPRwf2Z4qoo65a1TVqhX3jplhXrlrVNW2Dp5q3aNwszMKsYdhZmZlXJH\nkW+xrZE65a1TVqhX3jplhXrlrVNW2Ap5e/4ahZmZlfMZhZmZlerpjkLSSbmc+RpJl3Y7TzuS1kpa\nIWmppIe7nadI0nRJ6yU9Wli2p6R5kv6S/92jmxmLWuSdJunp3L5LJX2imxkbJI2SNF/Sqlye//y8\nvHLtW5K1qm07VNJCScty3svz8v0lLcjHhpmSdqxw1p9LerLQtmMHfd+9OvQkaQjwOHACqfjgIuCM\niFhV+sYukrQWGBcRlbvHW9KxwAbgFxFxcF72LeCFiLgyd8R7RMQl3czZ0CLvNGBDRHynm9n6krQP\nsE9EPCJpV2Ax8GlgEhVr35Ksp1HNthUwLCI2SNoBuB84H7gQuC0iZki6DlgWEddWNOtk4PcR8ZtO\n7buXzygOB9ZExBMR8R9gBqnMuQ1ARNwHvNBn8SnAjfnxjaQDRiW0yFtJEbEuIh7Jj18GVpOqKleu\nfUuyVlIkG/LTHfJPAMcBjQNvVdq2VdaO6+WOoqslzQcogLmSFufquVU3IiLW5cfPACO6GaafzpO0\nPA9NdX0opy9J7wAOBRZQ8fbtkxUq2raShkhaCqwH5gF/BV6MiP/mVSpzbOibNSIabXtFbturJO00\n2Pvt5Y6ijo6JiMOAk4EpefikFiKNcVZ9nPNa4F3AWGAd8N3uxnkjSbsAs4AvRsS/iq9VrX2bZK1s\n20bEpogYS6pWfTgwpsuRWuqbVdLBwGWkzB8E9gQGffixlzuKfpc0r4qIeDr/ux74Lek/dZU9m8es\nG2PX67ucp1REPJv/EDcDP6FC7ZvHpGcBN0XEbXlxJdu3WdYqt21DRLwIzCcVK91dUmMahsodGwpZ\nT8rDfZFnBv0ZHWjbXu4oFgEH5LsbdiTN1T2ny5lakjQsXxxE0jDgRNLc41U2B5iYH08EtnjCqk5q\nHHSzCVSkffNFzBuA1RHxvcJLlWvfVlkr3LbDJe2eH+9MurllNekg/Nm8WlXatlnWPxc+LIh0LWXQ\n27Zn73oCyLfofR8YAkyPiCu6HKklSe8knUVAmnDq5irllfRrYDypkuWzwNeB2aSy8aNJ1X1Pi4hK\nXEBukXc8aWgkgLXAuYVrAF0j6Rjgj8AKYHNe/BXS2H+l2rck6xlUs20PIV2sHkL64HxLRHwj/73N\nIA3lLAHOzJ/Yu6Yk6z3AcEDAUmBy4aL34Oy7lzsKMzNrr5eHnszMrB/cUZiZWSl3FGZmVsodhZmZ\nlXJHYWZmpdxRmJlZKXcUVjuSNuVyyo9Kur3xJaQBbmutpL3brDNJ0jX58WRJZ5WsO17SUQPNsyVy\nzuck/fRNvu/bkp6R9KVOZbN62779KmaV82qud4OkG4EpwFb58mFEXNdmlfGk8uUPdD5NUzMj4rw3\n84aIuFjSvzsVyOrPZxRWdw9SqOwp6WJJi3IlzcsLy2fnqrsr+1N5V9LnJD0uaSFwdGH5tMYnb0lT\nlSboWS5pRq6WOhm4IJ/xfFjSp5QmwFki6W5JIwrbmS7pXklPSJpa2MdZeZvLJP0yLxsuaVb+3RZJ\nOpo28hnGbKVJjdZKOk/ShTnLQ5L2bN+8Zj6jsBpTmnzqY6TaQkg6ETiAVBRNwBxJx+a5J86OiBdy\njZxFkmZFxD9abHcf4HLgA8BLpLo/S5qseimwf0RslLR7RLyoNMnNaxP05HLaR0RESPo88GXgovz+\nMcBHgV2BxyRdCxwIfA04KiKeLxzMfwBcFRH3SxoN3AW8px/NdDCp1PdQYA1wSUQcKukq4CxSCRuz\nUu4orI52VqrJP5JUwG1eXn5i/mkc1HchdRz3AVMlTcjLR+XlTTsK4EPAvRHxHICkmaQDeF/LgZsk\nzSbVtWpmX2Bm7nx2BJ4svHZHrh+0UdJ60nwSxwG3NmYxLNRuOh44KNV9A+AtknbpR02f+XkCoZcl\nvQTcnpevAA5p814zwENPVk+NaxT7kc4cpuTlAr4ZEWPzz7sj4gZJ40kH2iMj4v2kjmToIOT4JPAj\n4DDSWUqzD14/BK6JiPcB5/bZb7HI3CbKP7htRzozafxuI/tZ+K24j82F55vb7M/sNe4orLYi4hVg\nKnBRPkjfBZytNGkOkkZKeiuwG/DPiHhF0hjgiDabXgB8RNJeSnMrnNp3BUnbAaMiYj5popjdSGcw\nL5OGkhp24/W5DCbS3j3AqZL2yvtpDD3NBb5Q2P/YfmzLbFC4o7Bai4glpCGgMyJiLnAz8KCkFaQ5\nj3cF7gS2l7QauBJ4qM021wHTSBfK/0Qa3uprCPCrvJ8lwNV5MpnbgQmNi9l5O7dKWgw834/fZyXp\nDq4/SFoGNOZ0mAqMyxe5V5EumpttFS4zbraNkDQJGPdmb4/N751G4SK8WZHPKMy2Ha8CJw/kC3fA\nmYC/S2FN+YzCzMxK+YzCzMxKuaMwM7NS7ijMzKyUOwozMyvljsLMzEr9D16BBBe85dvHAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SFicYuZffc3",
        "colab_type": "code",
        "outputId": "e362653b-c343-485c-d4a1-e1b53aa8d16c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "err = y-x\n",
        "err.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    15091.000000\n",
              "mean         0.025700\n",
              "std          2.191961\n",
              "min        -16.659808\n",
              "25%         -1.176861\n",
              "50%         -0.020850\n",
              "75%          1.166256\n",
              "max         24.798873\n",
              "Name: dist, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVCekFj_ffc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "srterr = sorted(abs(err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKaGYcRTffc6",
        "colab_type": "code",
        "outputId": "529c5dce-c1e4-4143-96b0-271c191e17ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "srterr[13580]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2127221694425643"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTTmboFiffc8",
        "colab_type": "code",
        "outputId": "fd6021b2-083e-484b-f0ff-7becc54bfacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "_=plt.hist(srterr, bins=1000, cumulative=True, density=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6xNsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTXJohNvYJUT",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch DNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCbPbewmYITP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60fdf801-4dc6-4603-c59f-bccf2417e78b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lin_1 = nn.Linear(16, 512)\n",
        "        self.lin_2 = nn.Linear(self.lin_1.out_features, 256)\n",
        "        self.lin_3 = nn.Linear(self.lin_2.out_features, 512)\n",
        "        self.lin_4 = nn.Linear(self.lin_3.out_features, 256)\n",
        "        self.lin_5 = nn.Linear(self.lin_4.out_features, 128)\n",
        "        self.lin_6 = nn.Linear(self.lin_5.out_features, 64)\n",
        "        self.lin_7 = nn.Linear(self.lin_6.out_features, 32)\n",
        "        self.lin_8 = nn.Linear(self.lin_7.out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.lin_1(x))\n",
        "        x = F.relu(self.lin_2(x))\n",
        "        x = F.relu(self.lin_3(x))\n",
        "        x = F.relu(self.lin_4(x))\n",
        "        x = F.relu(self.lin_5(x))\n",
        "        x = F.relu(self.lin_6(x))\n",
        "        x = F.relu(self.lin_7(x))\n",
        "        x = F.relu(self.lin_8(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "def main():\n",
        "    dists = np.arange(0, 35.5, 0.5)\n",
        "    dfs = []\n",
        "\n",
        "    # Data loading\n",
        "    for dist in dists:\n",
        "        if dist.is_integer():\n",
        "            fname = 'data/dist.' + '{0:.0f}'.format(dist) + 'm.txt'\n",
        "        else:\n",
        "            fname = 'data/dist.' + '{0:.1f}'.format(dist) + 'm.txt'\n",
        "    \n",
        "        df = pd.read_csv(fname, header = None, names=['time','tag id','anchor id','anchor1 id', 'anchor2 id', 'anchor3 id','a0ch0','a0ch1','a0ch2','a0ch3','a0rssi0','a0rssi1','a0rssi2','a0rssi3','a1ch0','a1ch1','a1ch2','a1ch3','a1rssi0','a1rssi1','a1rssi2','a1rssi3','a2ch0','a2ch1','a2ch2','a2ch3','a2rssi0','a2rssi1','a2rssi2','a2rssi3','a3ch0','a3ch1','a3ch2','a3ch3','a3rssi0','a3rssi1','a3rssi2','a3rssi3'])\n",
        "        df['dist'] = dist\n",
        "        dfs.append(df)\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "    df = df.drop(axis=1,labels = ['time'])\n",
        "    df = df.drop(axis=1,labels = ['tag id','anchor id','anchor1 id','anchor2 id','anchor3 id', 'a0ch0','a0ch1','a0ch2','a0ch3', 'a1ch0','a1ch1','a1ch2','a1ch3', 'a2ch0','a2ch1','a2ch2','a2ch3', 'a3ch0','a3ch1','a3ch2','a3ch3'])\n",
        "\n",
        "    # Data split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df.drop(axis=1,labels = ['dist']), df['dist'], test_size=0.3, random_state=0)\n",
        "\n",
        "    # Test and Train dataset preparations\n",
        "    torch_X_train = torch.tensor(X_train.values, dtype=torch.float)\n",
        "    torch_X_test = torch.tensor(X_test.values, dtype=torch.float)\n",
        "    torch_Y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
        "    torch_Y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
        "    train_dataset = TensorDataset(torch_X_train, torch_Y_train)\n",
        "    test_dataset = TensorDataset(torch_X_test, torch_Y_test)\n",
        "\n",
        "    # Model training parameters\n",
        "    model_path = \"model.pth\"\n",
        "    batch_size=20\n",
        "    learning_rate=0.0001\n",
        "    epochs=100\n",
        "    log_interval=100\n",
        "\n",
        "    # Create instance of the network\n",
        "    model = Net()\n",
        "\n",
        "    # Create a Stochastic Gradient Descent optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    # Create a Mean Squared Error loss function\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # Use CUDA if wanted and available in system (Don't, network is too small for benefits of GPU)\n",
        "    use_cuda = False and torch.cuda.is_available()\n",
        "\n",
        "    # Move model to GPU\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size)\n",
        "    test_loader = DataLoader(test_dataset)\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        # Run the main training loop\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                # Resize data according to batch_size and last input batch_size\n",
        "                target = target.view(data.shape[0], -1)\n",
        "\n",
        "                if use_cuda:\n",
        "                    data = data.cuda()\n",
        "                    target = target.cuda()\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward + Backward + Optimize\n",
        "                net_out = model(data)\n",
        "\n",
        "                if use_cuda:\n",
        "                    net_out.cpu()\n",
        "\n",
        "                loss = criterion(net_out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Print statistics\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), \n",
        "                                                                                   len(train_loader.dataset), \n",
        "                                                                                   100. * batch_idx / len(train_loader), \n",
        "                                                                                   loss.item()))\n",
        "        # Save the trained model\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    else:\n",
        "        # Load the trained model\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    x = list()\n",
        "    y = list()\n",
        "    # Run a test loop\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for id, (data, target) in enumerate(test_loader):\n",
        "            # Resize data according to batch_size and last input batch_size\n",
        "            target = target.view(data.shape[0], -1)\n",
        "            net_out = model(data)\n",
        "            # Sum up batch loss\n",
        "            test_loss += criterion(net_out, target).item()\n",
        "            # print(\"Expected/Predicted: {:.4f}/{:.4f}\".format(target.item(), net_out.item()))\n",
        "            x.append(target.item())\n",
        "            y.append(net_out.item())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
        "\n",
        "    # Plotting the predicted distances vs real distances:\n",
        "    plt.scatter(x,y,marker='x')\n",
        "    plt.title(\"PyTorch\")\n",
        "    plt.xlabel('Real distance [m]')\n",
        "    plt.ylabel('Predicted distance [m]')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    err = pd.Series(np.asarray(y) - np.asarray(x))\n",
        "    print(err.describe())\n",
        "\n",
        "    srterr = sorted(abs(err))\n",
        "\n",
        "    plt.hist(srterr, bins=1000, cumulative=True, density=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/35212 (0%)]\tLoss: 522.138550\n",
            "Train Epoch: 0 [2000/35212 (6%)]\tLoss: 457.557190\n",
            "Train Epoch: 0 [4000/35212 (11%)]\tLoss: 59.171970\n",
            "Train Epoch: 0 [6000/35212 (17%)]\tLoss: 16.255747\n",
            "Train Epoch: 0 [8000/35212 (23%)]\tLoss: 3.369078\n",
            "Train Epoch: 0 [10000/35212 (28%)]\tLoss: 6.133544\n",
            "Train Epoch: 0 [12000/35212 (34%)]\tLoss: 2.487626\n",
            "Train Epoch: 0 [14000/35212 (40%)]\tLoss: 4.110135\n",
            "Train Epoch: 0 [16000/35212 (45%)]\tLoss: 2.314056\n",
            "Train Epoch: 0 [18000/35212 (51%)]\tLoss: 26.574442\n",
            "Train Epoch: 0 [20000/35212 (57%)]\tLoss: 7.470139\n",
            "Train Epoch: 0 [22000/35212 (62%)]\tLoss: 9.272501\n",
            "Train Epoch: 0 [24000/35212 (68%)]\tLoss: 8.124796\n",
            "Train Epoch: 0 [26000/35212 (74%)]\tLoss: 32.857899\n",
            "Train Epoch: 0 [28000/35212 (80%)]\tLoss: 8.274250\n",
            "Train Epoch: 0 [30000/35212 (85%)]\tLoss: 2.981504\n",
            "Train Epoch: 0 [32000/35212 (91%)]\tLoss: 3.720030\n",
            "Train Epoch: 0 [34000/35212 (97%)]\tLoss: 6.789320\n",
            "Train Epoch: 1 [0/35212 (0%)]\tLoss: 6.599244\n",
            "Train Epoch: 1 [2000/35212 (6%)]\tLoss: 2.660553\n",
            "Train Epoch: 1 [4000/35212 (11%)]\tLoss: 11.289774\n",
            "Train Epoch: 1 [6000/35212 (17%)]\tLoss: 5.660295\n",
            "Train Epoch: 1 [8000/35212 (23%)]\tLoss: 1.416888\n",
            "Train Epoch: 1 [10000/35212 (28%)]\tLoss: 2.840426\n",
            "Train Epoch: 1 [12000/35212 (34%)]\tLoss: 1.722387\n",
            "Train Epoch: 1 [14000/35212 (40%)]\tLoss: 2.846601\n",
            "Train Epoch: 1 [16000/35212 (45%)]\tLoss: 1.645800\n",
            "Train Epoch: 1 [18000/35212 (51%)]\tLoss: 8.666286\n",
            "Train Epoch: 1 [20000/35212 (57%)]\tLoss: 2.567560\n",
            "Train Epoch: 1 [22000/35212 (62%)]\tLoss: 4.783712\n",
            "Train Epoch: 1 [24000/35212 (68%)]\tLoss: 7.828750\n",
            "Train Epoch: 1 [26000/35212 (74%)]\tLoss: 14.398273\n",
            "Train Epoch: 1 [28000/35212 (80%)]\tLoss: 2.647635\n",
            "Train Epoch: 1 [30000/35212 (85%)]\tLoss: 3.270180\n",
            "Train Epoch: 1 [32000/35212 (91%)]\tLoss: 2.491549\n",
            "Train Epoch: 1 [34000/35212 (97%)]\tLoss: 5.012409\n",
            "Train Epoch: 2 [0/35212 (0%)]\tLoss: 3.135113\n",
            "Train Epoch: 2 [2000/35212 (6%)]\tLoss: 4.267843\n",
            "Train Epoch: 2 [4000/35212 (11%)]\tLoss: 6.799160\n",
            "Train Epoch: 2 [6000/35212 (17%)]\tLoss: 7.780594\n",
            "Train Epoch: 2 [8000/35212 (23%)]\tLoss: 0.586690\n",
            "Train Epoch: 2 [10000/35212 (28%)]\tLoss: 5.329838\n",
            "Train Epoch: 2 [12000/35212 (34%)]\tLoss: 1.711455\n",
            "Train Epoch: 2 [14000/35212 (40%)]\tLoss: 1.798746\n",
            "Train Epoch: 2 [16000/35212 (45%)]\tLoss: 2.940810\n",
            "Train Epoch: 2 [18000/35212 (51%)]\tLoss: 1.485559\n",
            "Train Epoch: 2 [20000/35212 (57%)]\tLoss: 1.176349\n",
            "Train Epoch: 2 [22000/35212 (62%)]\tLoss: 3.511004\n",
            "Train Epoch: 2 [24000/35212 (68%)]\tLoss: 9.229991\n",
            "Train Epoch: 2 [26000/35212 (74%)]\tLoss: 12.358666\n",
            "Train Epoch: 2 [28000/35212 (80%)]\tLoss: 1.660701\n",
            "Train Epoch: 2 [30000/35212 (85%)]\tLoss: 1.503079\n",
            "Train Epoch: 2 [32000/35212 (91%)]\tLoss: 2.912083\n",
            "Train Epoch: 2 [34000/35212 (97%)]\tLoss: 1.624806\n",
            "Train Epoch: 3 [0/35212 (0%)]\tLoss: 1.683767\n",
            "Train Epoch: 3 [2000/35212 (6%)]\tLoss: 4.548649\n",
            "Train Epoch: 3 [4000/35212 (11%)]\tLoss: 2.993292\n",
            "Train Epoch: 3 [6000/35212 (17%)]\tLoss: 7.432108\n",
            "Train Epoch: 3 [8000/35212 (23%)]\tLoss: 0.389011\n",
            "Train Epoch: 3 [10000/35212 (28%)]\tLoss: 2.030286\n",
            "Train Epoch: 3 [12000/35212 (34%)]\tLoss: 1.698816\n",
            "Train Epoch: 3 [14000/35212 (40%)]\tLoss: 1.426826\n",
            "Train Epoch: 3 [16000/35212 (45%)]\tLoss: 1.199927\n",
            "Train Epoch: 3 [18000/35212 (51%)]\tLoss: 1.098905\n",
            "Train Epoch: 3 [20000/35212 (57%)]\tLoss: 1.589758\n",
            "Train Epoch: 3 [22000/35212 (62%)]\tLoss: 2.057329\n",
            "Train Epoch: 3 [24000/35212 (68%)]\tLoss: 6.363474\n",
            "Train Epoch: 3 [26000/35212 (74%)]\tLoss: 5.428640\n",
            "Train Epoch: 3 [28000/35212 (80%)]\tLoss: 1.941117\n",
            "Train Epoch: 3 [30000/35212 (85%)]\tLoss: 1.287568\n",
            "Train Epoch: 3 [32000/35212 (91%)]\tLoss: 2.062442\n",
            "Train Epoch: 3 [34000/35212 (97%)]\tLoss: 1.227063\n",
            "Train Epoch: 4 [0/35212 (0%)]\tLoss: 1.859723\n",
            "Train Epoch: 4 [2000/35212 (6%)]\tLoss: 1.120289\n",
            "Train Epoch: 4 [4000/35212 (11%)]\tLoss: 2.501194\n",
            "Train Epoch: 4 [6000/35212 (17%)]\tLoss: 4.874959\n",
            "Train Epoch: 4 [8000/35212 (23%)]\tLoss: 0.477361\n",
            "Train Epoch: 4 [10000/35212 (28%)]\tLoss: 1.447202\n",
            "Train Epoch: 4 [12000/35212 (34%)]\tLoss: 1.083220\n",
            "Train Epoch: 4 [14000/35212 (40%)]\tLoss: 1.503729\n",
            "Train Epoch: 4 [16000/35212 (45%)]\tLoss: 0.962315\n",
            "Train Epoch: 4 [18000/35212 (51%)]\tLoss: 1.274080\n",
            "Train Epoch: 4 [20000/35212 (57%)]\tLoss: 1.001949\n",
            "Train Epoch: 4 [22000/35212 (62%)]\tLoss: 1.606780\n",
            "Train Epoch: 4 [24000/35212 (68%)]\tLoss: 4.299562\n",
            "Train Epoch: 4 [26000/35212 (74%)]\tLoss: 2.183582\n",
            "Train Epoch: 4 [28000/35212 (80%)]\tLoss: 1.130054\n",
            "Train Epoch: 4 [30000/35212 (85%)]\tLoss: 1.317940\n",
            "Train Epoch: 4 [32000/35212 (91%)]\tLoss: 2.020294\n",
            "Train Epoch: 4 [34000/35212 (97%)]\tLoss: 0.852766\n",
            "Train Epoch: 5 [0/35212 (0%)]\tLoss: 0.936160\n",
            "Train Epoch: 5 [2000/35212 (6%)]\tLoss: 0.572682\n",
            "Train Epoch: 5 [4000/35212 (11%)]\tLoss: 0.518259\n",
            "Train Epoch: 5 [6000/35212 (17%)]\tLoss: 3.112466\n",
            "Train Epoch: 5 [8000/35212 (23%)]\tLoss: 0.442285\n",
            "Train Epoch: 5 [10000/35212 (28%)]\tLoss: 1.834352\n",
            "Train Epoch: 5 [12000/35212 (34%)]\tLoss: 1.115689\n",
            "Train Epoch: 5 [14000/35212 (40%)]\tLoss: 1.035098\n",
            "Train Epoch: 5 [16000/35212 (45%)]\tLoss: 0.413669\n",
            "Train Epoch: 5 [18000/35212 (51%)]\tLoss: 1.540323\n",
            "Train Epoch: 5 [20000/35212 (57%)]\tLoss: 0.354062\n",
            "Train Epoch: 5 [22000/35212 (62%)]\tLoss: 1.231103\n",
            "Train Epoch: 5 [24000/35212 (68%)]\tLoss: 4.182474\n",
            "Train Epoch: 5 [26000/35212 (74%)]\tLoss: 1.282209\n",
            "Train Epoch: 5 [28000/35212 (80%)]\tLoss: 1.198868\n",
            "Train Epoch: 5 [30000/35212 (85%)]\tLoss: 2.119640\n",
            "Train Epoch: 5 [32000/35212 (91%)]\tLoss: 2.411565\n",
            "Train Epoch: 5 [34000/35212 (97%)]\tLoss: 0.648073\n",
            "Train Epoch: 6 [0/35212 (0%)]\tLoss: 0.781029\n",
            "Train Epoch: 6 [2000/35212 (6%)]\tLoss: 0.606945\n",
            "Train Epoch: 6 [4000/35212 (11%)]\tLoss: 0.343786\n",
            "Train Epoch: 6 [6000/35212 (17%)]\tLoss: 2.488841\n",
            "Train Epoch: 6 [8000/35212 (23%)]\tLoss: 0.218316\n",
            "Train Epoch: 6 [10000/35212 (28%)]\tLoss: 2.718442\n",
            "Train Epoch: 6 [12000/35212 (34%)]\tLoss: 0.883940\n",
            "Train Epoch: 6 [14000/35212 (40%)]\tLoss: 0.845650\n",
            "Train Epoch: 6 [16000/35212 (45%)]\tLoss: 0.403823\n",
            "Train Epoch: 6 [18000/35212 (51%)]\tLoss: 1.016458\n",
            "Train Epoch: 6 [20000/35212 (57%)]\tLoss: 0.431661\n",
            "Train Epoch: 6 [22000/35212 (62%)]\tLoss: 0.905415\n",
            "Train Epoch: 6 [24000/35212 (68%)]\tLoss: 3.501091\n",
            "Train Epoch: 6 [26000/35212 (74%)]\tLoss: 1.627952\n",
            "Train Epoch: 6 [28000/35212 (80%)]\tLoss: 1.753870\n",
            "Train Epoch: 6 [30000/35212 (85%)]\tLoss: 1.804501\n",
            "Train Epoch: 6 [32000/35212 (91%)]\tLoss: 2.100445\n",
            "Train Epoch: 6 [34000/35212 (97%)]\tLoss: 0.703745\n",
            "Train Epoch: 7 [0/35212 (0%)]\tLoss: 1.182280\n",
            "Train Epoch: 7 [2000/35212 (6%)]\tLoss: 0.712632\n",
            "Train Epoch: 7 [4000/35212 (11%)]\tLoss: 0.159468\n",
            "Train Epoch: 7 [6000/35212 (17%)]\tLoss: 2.187774\n",
            "Train Epoch: 7 [8000/35212 (23%)]\tLoss: 0.227899\n",
            "Train Epoch: 7 [10000/35212 (28%)]\tLoss: 2.273858\n",
            "Train Epoch: 7 [12000/35212 (34%)]\tLoss: 0.641399\n",
            "Train Epoch: 7 [14000/35212 (40%)]\tLoss: 0.727838\n",
            "Train Epoch: 7 [16000/35212 (45%)]\tLoss: 0.483505\n",
            "Train Epoch: 7 [18000/35212 (51%)]\tLoss: 1.213740\n",
            "Train Epoch: 7 [20000/35212 (57%)]\tLoss: 0.306764\n",
            "Train Epoch: 7 [22000/35212 (62%)]\tLoss: 0.592833\n",
            "Train Epoch: 7 [24000/35212 (68%)]\tLoss: 2.517647\n",
            "Train Epoch: 7 [26000/35212 (74%)]\tLoss: 0.892272\n",
            "Train Epoch: 7 [28000/35212 (80%)]\tLoss: 1.030059\n",
            "Train Epoch: 7 [30000/35212 (85%)]\tLoss: 1.086454\n",
            "Train Epoch: 7 [32000/35212 (91%)]\tLoss: 2.223110\n",
            "Train Epoch: 7 [34000/35212 (97%)]\tLoss: 0.516074\n",
            "Train Epoch: 8 [0/35212 (0%)]\tLoss: 1.394400\n",
            "Train Epoch: 8 [2000/35212 (6%)]\tLoss: 0.825676\n",
            "Train Epoch: 8 [4000/35212 (11%)]\tLoss: 0.327196\n",
            "Train Epoch: 8 [6000/35212 (17%)]\tLoss: 1.964385\n",
            "Train Epoch: 8 [8000/35212 (23%)]\tLoss: 0.242507\n",
            "Train Epoch: 8 [10000/35212 (28%)]\tLoss: 2.029675\n",
            "Train Epoch: 8 [12000/35212 (34%)]\tLoss: 0.363428\n",
            "Train Epoch: 8 [14000/35212 (40%)]\tLoss: 0.740438\n",
            "Train Epoch: 8 [16000/35212 (45%)]\tLoss: 0.488082\n",
            "Train Epoch: 8 [18000/35212 (51%)]\tLoss: 0.822902\n",
            "Train Epoch: 8 [20000/35212 (57%)]\tLoss: 0.383238\n",
            "Train Epoch: 8 [22000/35212 (62%)]\tLoss: 0.522977\n",
            "Train Epoch: 8 [24000/35212 (68%)]\tLoss: 2.722916\n",
            "Train Epoch: 8 [26000/35212 (74%)]\tLoss: 0.913519\n",
            "Train Epoch: 8 [28000/35212 (80%)]\tLoss: 1.213699\n",
            "Train Epoch: 8 [30000/35212 (85%)]\tLoss: 1.605877\n",
            "Train Epoch: 8 [32000/35212 (91%)]\tLoss: 1.851326\n",
            "Train Epoch: 8 [34000/35212 (97%)]\tLoss: 0.475079\n",
            "Train Epoch: 9 [0/35212 (0%)]\tLoss: 1.589399\n",
            "Train Epoch: 9 [2000/35212 (6%)]\tLoss: 0.586698\n",
            "Train Epoch: 9 [4000/35212 (11%)]\tLoss: 0.479982\n",
            "Train Epoch: 9 [6000/35212 (17%)]\tLoss: 2.183858\n",
            "Train Epoch: 9 [8000/35212 (23%)]\tLoss: 0.245702\n",
            "Train Epoch: 9 [10000/35212 (28%)]\tLoss: 1.750894\n",
            "Train Epoch: 9 [12000/35212 (34%)]\tLoss: 0.351765\n",
            "Train Epoch: 9 [14000/35212 (40%)]\tLoss: 0.610544\n",
            "Train Epoch: 9 [16000/35212 (45%)]\tLoss: 0.455059\n",
            "Train Epoch: 9 [18000/35212 (51%)]\tLoss: 0.640721\n",
            "Train Epoch: 9 [20000/35212 (57%)]\tLoss: 0.346235\n",
            "Train Epoch: 9 [22000/35212 (62%)]\tLoss: 0.567929\n",
            "Train Epoch: 9 [24000/35212 (68%)]\tLoss: 2.341444\n",
            "Train Epoch: 9 [26000/35212 (74%)]\tLoss: 0.562142\n",
            "Train Epoch: 9 [28000/35212 (80%)]\tLoss: 0.937678\n",
            "Train Epoch: 9 [30000/35212 (85%)]\tLoss: 1.018835\n",
            "Train Epoch: 9 [32000/35212 (91%)]\tLoss: 1.841825\n",
            "Train Epoch: 9 [34000/35212 (97%)]\tLoss: 0.506169\n",
            "Train Epoch: 10 [0/35212 (0%)]\tLoss: 0.578410\n",
            "Train Epoch: 10 [2000/35212 (6%)]\tLoss: 0.395214\n",
            "Train Epoch: 10 [4000/35212 (11%)]\tLoss: 0.378177\n",
            "Train Epoch: 10 [6000/35212 (17%)]\tLoss: 1.042468\n",
            "Train Epoch: 10 [8000/35212 (23%)]\tLoss: 0.371521\n",
            "Train Epoch: 10 [10000/35212 (28%)]\tLoss: 1.456765\n",
            "Train Epoch: 10 [12000/35212 (34%)]\tLoss: 0.318418\n",
            "Train Epoch: 10 [14000/35212 (40%)]\tLoss: 0.668738\n",
            "Train Epoch: 10 [16000/35212 (45%)]\tLoss: 0.452191\n",
            "Train Epoch: 10 [18000/35212 (51%)]\tLoss: 0.742468\n",
            "Train Epoch: 10 [20000/35212 (57%)]\tLoss: 0.460352\n",
            "Train Epoch: 10 [22000/35212 (62%)]\tLoss: 0.379295\n",
            "Train Epoch: 10 [24000/35212 (68%)]\tLoss: 2.141211\n",
            "Train Epoch: 10 [26000/35212 (74%)]\tLoss: 0.430488\n",
            "Train Epoch: 10 [28000/35212 (80%)]\tLoss: 0.778441\n",
            "Train Epoch: 10 [30000/35212 (85%)]\tLoss: 0.807364\n",
            "Train Epoch: 10 [32000/35212 (91%)]\tLoss: 1.845141\n",
            "Train Epoch: 10 [34000/35212 (97%)]\tLoss: 0.778461\n",
            "Train Epoch: 11 [0/35212 (0%)]\tLoss: 0.697496\n",
            "Train Epoch: 11 [2000/35212 (6%)]\tLoss: 0.307240\n",
            "Train Epoch: 11 [4000/35212 (11%)]\tLoss: 0.138222\n",
            "Train Epoch: 11 [6000/35212 (17%)]\tLoss: 0.778685\n",
            "Train Epoch: 11 [8000/35212 (23%)]\tLoss: 0.258682\n",
            "Train Epoch: 11 [10000/35212 (28%)]\tLoss: 1.010820\n",
            "Train Epoch: 11 [12000/35212 (34%)]\tLoss: 0.296033\n",
            "Train Epoch: 11 [14000/35212 (40%)]\tLoss: 0.725369\n",
            "Train Epoch: 11 [16000/35212 (45%)]\tLoss: 0.474234\n",
            "Train Epoch: 11 [18000/35212 (51%)]\tLoss: 0.985347\n",
            "Train Epoch: 11 [20000/35212 (57%)]\tLoss: 0.338685\n",
            "Train Epoch: 11 [22000/35212 (62%)]\tLoss: 0.445088\n",
            "Train Epoch: 11 [24000/35212 (68%)]\tLoss: 1.519662\n",
            "Train Epoch: 11 [26000/35212 (74%)]\tLoss: 0.357357\n",
            "Train Epoch: 11 [28000/35212 (80%)]\tLoss: 0.888401\n",
            "Train Epoch: 11 [30000/35212 (85%)]\tLoss: 0.717435\n",
            "Train Epoch: 11 [32000/35212 (91%)]\tLoss: 1.053216\n",
            "Train Epoch: 11 [34000/35212 (97%)]\tLoss: 0.882628\n",
            "Train Epoch: 12 [0/35212 (0%)]\tLoss: 0.881284\n",
            "Train Epoch: 12 [2000/35212 (6%)]\tLoss: 0.227749\n",
            "Train Epoch: 12 [4000/35212 (11%)]\tLoss: 0.083494\n",
            "Train Epoch: 12 [6000/35212 (17%)]\tLoss: 0.581076\n",
            "Train Epoch: 12 [8000/35212 (23%)]\tLoss: 0.220640\n",
            "Train Epoch: 12 [10000/35212 (28%)]\tLoss: 1.819351\n",
            "Train Epoch: 12 [12000/35212 (34%)]\tLoss: 0.301311\n",
            "Train Epoch: 12 [14000/35212 (40%)]\tLoss: 0.530772\n",
            "Train Epoch: 12 [16000/35212 (45%)]\tLoss: 0.382804\n",
            "Train Epoch: 12 [18000/35212 (51%)]\tLoss: 0.909065\n",
            "Train Epoch: 12 [20000/35212 (57%)]\tLoss: 0.574361\n",
            "Train Epoch: 12 [22000/35212 (62%)]\tLoss: 0.590200\n",
            "Train Epoch: 12 [24000/35212 (68%)]\tLoss: 1.220985\n",
            "Train Epoch: 12 [26000/35212 (74%)]\tLoss: 0.246729\n",
            "Train Epoch: 12 [28000/35212 (80%)]\tLoss: 0.622914\n",
            "Train Epoch: 12 [30000/35212 (85%)]\tLoss: 0.680594\n",
            "Train Epoch: 12 [32000/35212 (91%)]\tLoss: 1.210186\n",
            "Train Epoch: 12 [34000/35212 (97%)]\tLoss: 0.535496\n",
            "Train Epoch: 13 [0/35212 (0%)]\tLoss: 0.567726\n",
            "Train Epoch: 13 [2000/35212 (6%)]\tLoss: 0.212328\n",
            "Train Epoch: 13 [4000/35212 (11%)]\tLoss: 0.141753\n",
            "Train Epoch: 13 [6000/35212 (17%)]\tLoss: 0.501206\n",
            "Train Epoch: 13 [8000/35212 (23%)]\tLoss: 0.217712\n",
            "Train Epoch: 13 [10000/35212 (28%)]\tLoss: 1.795739\n",
            "Train Epoch: 13 [12000/35212 (34%)]\tLoss: 0.235086\n",
            "Train Epoch: 13 [14000/35212 (40%)]\tLoss: 0.607012\n",
            "Train Epoch: 13 [16000/35212 (45%)]\tLoss: 0.304832\n",
            "Train Epoch: 13 [18000/35212 (51%)]\tLoss: 0.642971\n",
            "Train Epoch: 13 [20000/35212 (57%)]\tLoss: 0.393513\n",
            "Train Epoch: 13 [22000/35212 (62%)]\tLoss: 0.494510\n",
            "Train Epoch: 13 [24000/35212 (68%)]\tLoss: 1.150429\n",
            "Train Epoch: 13 [26000/35212 (74%)]\tLoss: 0.193124\n",
            "Train Epoch: 13 [28000/35212 (80%)]\tLoss: 0.474667\n",
            "Train Epoch: 13 [30000/35212 (85%)]\tLoss: 0.645085\n",
            "Train Epoch: 13 [32000/35212 (91%)]\tLoss: 1.335281\n",
            "Train Epoch: 13 [34000/35212 (97%)]\tLoss: 0.529502\n",
            "Train Epoch: 14 [0/35212 (0%)]\tLoss: 0.636050\n",
            "Train Epoch: 14 [2000/35212 (6%)]\tLoss: 0.325552\n",
            "Train Epoch: 14 [4000/35212 (11%)]\tLoss: 0.149099\n",
            "Train Epoch: 14 [6000/35212 (17%)]\tLoss: 0.422252\n",
            "Train Epoch: 14 [8000/35212 (23%)]\tLoss: 0.225459\n",
            "Train Epoch: 14 [10000/35212 (28%)]\tLoss: 1.509454\n",
            "Train Epoch: 14 [12000/35212 (34%)]\tLoss: 0.251008\n",
            "Train Epoch: 14 [14000/35212 (40%)]\tLoss: 0.642317\n",
            "Train Epoch: 14 [16000/35212 (45%)]\tLoss: 0.301200\n",
            "Train Epoch: 14 [18000/35212 (51%)]\tLoss: 0.617775\n",
            "Train Epoch: 14 [20000/35212 (57%)]\tLoss: 0.421468\n",
            "Train Epoch: 14 [22000/35212 (62%)]\tLoss: 0.413273\n",
            "Train Epoch: 14 [24000/35212 (68%)]\tLoss: 0.907843\n",
            "Train Epoch: 14 [26000/35212 (74%)]\tLoss: 0.207972\n",
            "Train Epoch: 14 [28000/35212 (80%)]\tLoss: 0.424885\n",
            "Train Epoch: 14 [30000/35212 (85%)]\tLoss: 0.611249\n",
            "Train Epoch: 14 [32000/35212 (91%)]\tLoss: 0.597967\n",
            "Train Epoch: 14 [34000/35212 (97%)]\tLoss: 0.369677\n",
            "Train Epoch: 15 [0/35212 (0%)]\tLoss: 0.551914\n",
            "Train Epoch: 15 [2000/35212 (6%)]\tLoss: 0.257209\n",
            "Train Epoch: 15 [4000/35212 (11%)]\tLoss: 0.122846\n",
            "Train Epoch: 15 [6000/35212 (17%)]\tLoss: 0.567932\n",
            "Train Epoch: 15 [8000/35212 (23%)]\tLoss: 0.182906\n",
            "Train Epoch: 15 [10000/35212 (28%)]\tLoss: 1.369619\n",
            "Train Epoch: 15 [12000/35212 (34%)]\tLoss: 0.283591\n",
            "Train Epoch: 15 [14000/35212 (40%)]\tLoss: 0.432512\n",
            "Train Epoch: 15 [16000/35212 (45%)]\tLoss: 0.159923\n",
            "Train Epoch: 15 [18000/35212 (51%)]\tLoss: 0.533899\n",
            "Train Epoch: 15 [20000/35212 (57%)]\tLoss: 0.364477\n",
            "Train Epoch: 15 [22000/35212 (62%)]\tLoss: 0.494614\n",
            "Train Epoch: 15 [24000/35212 (68%)]\tLoss: 0.943498\n",
            "Train Epoch: 15 [26000/35212 (74%)]\tLoss: 0.125781\n",
            "Train Epoch: 15 [28000/35212 (80%)]\tLoss: 0.594003\n",
            "Train Epoch: 15 [30000/35212 (85%)]\tLoss: 0.491298\n",
            "Train Epoch: 15 [32000/35212 (91%)]\tLoss: 0.558639\n",
            "Train Epoch: 15 [34000/35212 (97%)]\tLoss: 0.321878\n",
            "Train Epoch: 16 [0/35212 (0%)]\tLoss: 0.436287\n",
            "Train Epoch: 16 [2000/35212 (6%)]\tLoss: 0.507921\n",
            "Train Epoch: 16 [4000/35212 (11%)]\tLoss: 0.129582\n",
            "Train Epoch: 16 [6000/35212 (17%)]\tLoss: 0.337158\n",
            "Train Epoch: 16 [8000/35212 (23%)]\tLoss: 0.174908\n",
            "Train Epoch: 16 [10000/35212 (28%)]\tLoss: 1.128386\n",
            "Train Epoch: 16 [12000/35212 (34%)]\tLoss: 0.227737\n",
            "Train Epoch: 16 [14000/35212 (40%)]\tLoss: 0.476792\n",
            "Train Epoch: 16 [16000/35212 (45%)]\tLoss: 0.234939\n",
            "Train Epoch: 16 [18000/35212 (51%)]\tLoss: 0.691333\n",
            "Train Epoch: 16 [20000/35212 (57%)]\tLoss: 0.357293\n",
            "Train Epoch: 16 [22000/35212 (62%)]\tLoss: 0.290092\n",
            "Train Epoch: 16 [24000/35212 (68%)]\tLoss: 0.803232\n",
            "Train Epoch: 16 [26000/35212 (74%)]\tLoss: 0.101290\n",
            "Train Epoch: 16 [28000/35212 (80%)]\tLoss: 0.436390\n",
            "Train Epoch: 16 [30000/35212 (85%)]\tLoss: 0.317787\n",
            "Train Epoch: 16 [32000/35212 (91%)]\tLoss: 0.596393\n",
            "Train Epoch: 16 [34000/35212 (97%)]\tLoss: 0.374143\n",
            "Train Epoch: 17 [0/35212 (0%)]\tLoss: 0.477180\n",
            "Train Epoch: 17 [2000/35212 (6%)]\tLoss: 0.464508\n",
            "Train Epoch: 17 [4000/35212 (11%)]\tLoss: 0.202802\n",
            "Train Epoch: 17 [6000/35212 (17%)]\tLoss: 0.250944\n",
            "Train Epoch: 17 [8000/35212 (23%)]\tLoss: 0.166936\n",
            "Train Epoch: 17 [10000/35212 (28%)]\tLoss: 1.243970\n",
            "Train Epoch: 17 [12000/35212 (34%)]\tLoss: 0.280971\n",
            "Train Epoch: 17 [14000/35212 (40%)]\tLoss: 0.674967\n",
            "Train Epoch: 17 [16000/35212 (45%)]\tLoss: 0.121483\n",
            "Train Epoch: 17 [18000/35212 (51%)]\tLoss: 0.390524\n",
            "Train Epoch: 17 [20000/35212 (57%)]\tLoss: 0.247920\n",
            "Train Epoch: 17 [22000/35212 (62%)]\tLoss: 0.374950\n",
            "Train Epoch: 17 [24000/35212 (68%)]\tLoss: 0.551327\n",
            "Train Epoch: 17 [26000/35212 (74%)]\tLoss: 0.131029\n",
            "Train Epoch: 17 [28000/35212 (80%)]\tLoss: 0.418116\n",
            "Train Epoch: 17 [30000/35212 (85%)]\tLoss: 0.484657\n",
            "Train Epoch: 17 [32000/35212 (91%)]\tLoss: 0.792587\n",
            "Train Epoch: 17 [34000/35212 (97%)]\tLoss: 0.223929\n",
            "Train Epoch: 18 [0/35212 (0%)]\tLoss: 0.473659\n",
            "Train Epoch: 18 [2000/35212 (6%)]\tLoss: 0.233618\n",
            "Train Epoch: 18 [4000/35212 (11%)]\tLoss: 0.103606\n",
            "Train Epoch: 18 [6000/35212 (17%)]\tLoss: 0.255214\n",
            "Train Epoch: 18 [8000/35212 (23%)]\tLoss: 0.210089\n",
            "Train Epoch: 18 [10000/35212 (28%)]\tLoss: 0.244919\n",
            "Train Epoch: 18 [12000/35212 (34%)]\tLoss: 0.188521\n",
            "Train Epoch: 18 [14000/35212 (40%)]\tLoss: 0.512024\n",
            "Train Epoch: 18 [16000/35212 (45%)]\tLoss: 0.155730\n",
            "Train Epoch: 18 [18000/35212 (51%)]\tLoss: 0.591655\n",
            "Train Epoch: 18 [20000/35212 (57%)]\tLoss: 0.329592\n",
            "Train Epoch: 18 [22000/35212 (62%)]\tLoss: 0.321860\n",
            "Train Epoch: 18 [24000/35212 (68%)]\tLoss: 0.601044\n",
            "Train Epoch: 18 [26000/35212 (74%)]\tLoss: 0.131270\n",
            "Train Epoch: 18 [28000/35212 (80%)]\tLoss: 0.613987\n",
            "Train Epoch: 18 [30000/35212 (85%)]\tLoss: 0.359392\n",
            "Train Epoch: 18 [32000/35212 (91%)]\tLoss: 0.757593\n",
            "Train Epoch: 18 [34000/35212 (97%)]\tLoss: 0.170915\n",
            "Train Epoch: 19 [0/35212 (0%)]\tLoss: 0.289450\n",
            "Train Epoch: 19 [2000/35212 (6%)]\tLoss: 0.218977\n",
            "Train Epoch: 19 [4000/35212 (11%)]\tLoss: 0.123301\n",
            "Train Epoch: 19 [6000/35212 (17%)]\tLoss: 0.241945\n",
            "Train Epoch: 19 [8000/35212 (23%)]\tLoss: 0.171626\n",
            "Train Epoch: 19 [10000/35212 (28%)]\tLoss: 0.698616\n",
            "Train Epoch: 19 [12000/35212 (34%)]\tLoss: 0.190710\n",
            "Train Epoch: 19 [14000/35212 (40%)]\tLoss: 0.399845\n",
            "Train Epoch: 19 [16000/35212 (45%)]\tLoss: 0.118127\n",
            "Train Epoch: 19 [18000/35212 (51%)]\tLoss: 0.619525\n",
            "Train Epoch: 19 [20000/35212 (57%)]\tLoss: 0.172886\n",
            "Train Epoch: 19 [22000/35212 (62%)]\tLoss: 0.213309\n",
            "Train Epoch: 19 [24000/35212 (68%)]\tLoss: 0.435019\n",
            "Train Epoch: 19 [26000/35212 (74%)]\tLoss: 0.201084\n",
            "Train Epoch: 19 [28000/35212 (80%)]\tLoss: 0.244389\n",
            "Train Epoch: 19 [30000/35212 (85%)]\tLoss: 0.162439\n",
            "Train Epoch: 19 [32000/35212 (91%)]\tLoss: 0.792151\n",
            "Train Epoch: 19 [34000/35212 (97%)]\tLoss: 0.189113\n",
            "Train Epoch: 20 [0/35212 (0%)]\tLoss: 0.380460\n",
            "Train Epoch: 20 [2000/35212 (6%)]\tLoss: 0.127502\n",
            "Train Epoch: 20 [4000/35212 (11%)]\tLoss: 0.089404\n",
            "Train Epoch: 20 [6000/35212 (17%)]\tLoss: 0.095603\n",
            "Train Epoch: 20 [8000/35212 (23%)]\tLoss: 0.127759\n",
            "Train Epoch: 20 [10000/35212 (28%)]\tLoss: 0.589032\n",
            "Train Epoch: 20 [12000/35212 (34%)]\tLoss: 0.138228\n",
            "Train Epoch: 20 [14000/35212 (40%)]\tLoss: 0.593261\n",
            "Train Epoch: 20 [16000/35212 (45%)]\tLoss: 0.170233\n",
            "Train Epoch: 20 [18000/35212 (51%)]\tLoss: 0.572941\n",
            "Train Epoch: 20 [20000/35212 (57%)]\tLoss: 0.126992\n",
            "Train Epoch: 20 [22000/35212 (62%)]\tLoss: 0.273497\n",
            "Train Epoch: 20 [24000/35212 (68%)]\tLoss: 0.475351\n",
            "Train Epoch: 20 [26000/35212 (74%)]\tLoss: 0.119869\n",
            "Train Epoch: 20 [28000/35212 (80%)]\tLoss: 0.211856\n",
            "Train Epoch: 20 [30000/35212 (85%)]\tLoss: 0.198560\n",
            "Train Epoch: 20 [32000/35212 (91%)]\tLoss: 0.319032\n",
            "Train Epoch: 20 [34000/35212 (97%)]\tLoss: 0.331868\n",
            "Train Epoch: 21 [0/35212 (0%)]\tLoss: 0.442228\n",
            "Train Epoch: 21 [2000/35212 (6%)]\tLoss: 0.159905\n",
            "Train Epoch: 21 [4000/35212 (11%)]\tLoss: 0.066838\n",
            "Train Epoch: 21 [6000/35212 (17%)]\tLoss: 0.126381\n",
            "Train Epoch: 21 [8000/35212 (23%)]\tLoss: 0.101568\n",
            "Train Epoch: 21 [10000/35212 (28%)]\tLoss: 0.319254\n",
            "Train Epoch: 21 [12000/35212 (34%)]\tLoss: 0.155401\n",
            "Train Epoch: 21 [14000/35212 (40%)]\tLoss: 0.424265\n",
            "Train Epoch: 21 [16000/35212 (45%)]\tLoss: 0.107512\n",
            "Train Epoch: 21 [18000/35212 (51%)]\tLoss: 0.555585\n",
            "Train Epoch: 21 [20000/35212 (57%)]\tLoss: 0.154551\n",
            "Train Epoch: 21 [22000/35212 (62%)]\tLoss: 0.222290\n",
            "Train Epoch: 21 [24000/35212 (68%)]\tLoss: 0.354149\n",
            "Train Epoch: 21 [26000/35212 (74%)]\tLoss: 0.151473\n",
            "Train Epoch: 21 [28000/35212 (80%)]\tLoss: 0.197029\n",
            "Train Epoch: 21 [30000/35212 (85%)]\tLoss: 0.226882\n",
            "Train Epoch: 21 [32000/35212 (91%)]\tLoss: 0.784869\n",
            "Train Epoch: 21 [34000/35212 (97%)]\tLoss: 0.196273\n",
            "Train Epoch: 22 [0/35212 (0%)]\tLoss: 0.405265\n",
            "Train Epoch: 22 [2000/35212 (6%)]\tLoss: 0.118648\n",
            "Train Epoch: 22 [4000/35212 (11%)]\tLoss: 0.135003\n",
            "Train Epoch: 22 [6000/35212 (17%)]\tLoss: 0.423080\n",
            "Train Epoch: 22 [8000/35212 (23%)]\tLoss: 0.069872\n",
            "Train Epoch: 22 [10000/35212 (28%)]\tLoss: 0.237691\n",
            "Train Epoch: 22 [12000/35212 (34%)]\tLoss: 0.115331\n",
            "Train Epoch: 22 [14000/35212 (40%)]\tLoss: 0.362599\n",
            "Train Epoch: 22 [16000/35212 (45%)]\tLoss: 0.134888\n",
            "Train Epoch: 22 [18000/35212 (51%)]\tLoss: 0.390083\n",
            "Train Epoch: 22 [20000/35212 (57%)]\tLoss: 0.151749\n",
            "Train Epoch: 22 [22000/35212 (62%)]\tLoss: 0.267669\n",
            "Train Epoch: 22 [24000/35212 (68%)]\tLoss: 0.656918\n",
            "Train Epoch: 22 [26000/35212 (74%)]\tLoss: 0.144075\n",
            "Train Epoch: 22 [28000/35212 (80%)]\tLoss: 0.286033\n",
            "Train Epoch: 22 [30000/35212 (85%)]\tLoss: 0.160156\n",
            "Train Epoch: 22 [32000/35212 (91%)]\tLoss: 0.955751\n",
            "Train Epoch: 22 [34000/35212 (97%)]\tLoss: 0.163503\n",
            "Train Epoch: 23 [0/35212 (0%)]\tLoss: 0.468241\n",
            "Train Epoch: 23 [2000/35212 (6%)]\tLoss: 0.121821\n",
            "Train Epoch: 23 [4000/35212 (11%)]\tLoss: 0.110164\n",
            "Train Epoch: 23 [6000/35212 (17%)]\tLoss: 0.391094\n",
            "Train Epoch: 23 [8000/35212 (23%)]\tLoss: 0.073877\n",
            "Train Epoch: 23 [10000/35212 (28%)]\tLoss: 0.175344\n",
            "Train Epoch: 23 [12000/35212 (34%)]\tLoss: 0.132723\n",
            "Train Epoch: 23 [14000/35212 (40%)]\tLoss: 0.310412\n",
            "Train Epoch: 23 [16000/35212 (45%)]\tLoss: 0.108289\n",
            "Train Epoch: 23 [18000/35212 (51%)]\tLoss: 0.492249\n",
            "Train Epoch: 23 [20000/35212 (57%)]\tLoss: 0.120446\n",
            "Train Epoch: 23 [22000/35212 (62%)]\tLoss: 0.277290\n",
            "Train Epoch: 23 [24000/35212 (68%)]\tLoss: 0.287810\n",
            "Train Epoch: 23 [26000/35212 (74%)]\tLoss: 0.085358\n",
            "Train Epoch: 23 [28000/35212 (80%)]\tLoss: 0.144817\n",
            "Train Epoch: 23 [30000/35212 (85%)]\tLoss: 0.172688\n",
            "Train Epoch: 23 [32000/35212 (91%)]\tLoss: 0.479591\n",
            "Train Epoch: 23 [34000/35212 (97%)]\tLoss: 0.155366\n",
            "Train Epoch: 24 [0/35212 (0%)]\tLoss: 0.309832\n",
            "Train Epoch: 24 [2000/35212 (6%)]\tLoss: 0.101409\n",
            "Train Epoch: 24 [4000/35212 (11%)]\tLoss: 0.110479\n",
            "Train Epoch: 24 [6000/35212 (17%)]\tLoss: 0.069442\n",
            "Train Epoch: 24 [8000/35212 (23%)]\tLoss: 0.059601\n",
            "Train Epoch: 24 [10000/35212 (28%)]\tLoss: 0.223933\n",
            "Train Epoch: 24 [12000/35212 (34%)]\tLoss: 0.122959\n",
            "Train Epoch: 24 [14000/35212 (40%)]\tLoss: 0.253275\n",
            "Train Epoch: 24 [16000/35212 (45%)]\tLoss: 0.098712\n",
            "Train Epoch: 24 [18000/35212 (51%)]\tLoss: 0.376778\n",
            "Train Epoch: 24 [20000/35212 (57%)]\tLoss: 0.131455\n",
            "Train Epoch: 24 [22000/35212 (62%)]\tLoss: 0.227286\n",
            "Train Epoch: 24 [24000/35212 (68%)]\tLoss: 0.198412\n",
            "Train Epoch: 24 [26000/35212 (74%)]\tLoss: 0.195505\n",
            "Train Epoch: 24 [28000/35212 (80%)]\tLoss: 0.141792\n",
            "Train Epoch: 24 [30000/35212 (85%)]\tLoss: 0.111437\n",
            "Train Epoch: 24 [32000/35212 (91%)]\tLoss: 0.340089\n",
            "Train Epoch: 24 [34000/35212 (97%)]\tLoss: 0.161891\n",
            "Train Epoch: 25 [0/35212 (0%)]\tLoss: 0.360908\n",
            "Train Epoch: 25 [2000/35212 (6%)]\tLoss: 0.093602\n",
            "Train Epoch: 25 [4000/35212 (11%)]\tLoss: 0.128878\n",
            "Train Epoch: 25 [6000/35212 (17%)]\tLoss: 0.078749\n",
            "Train Epoch: 25 [8000/35212 (23%)]\tLoss: 0.058657\n",
            "Train Epoch: 25 [10000/35212 (28%)]\tLoss: 0.194157\n",
            "Train Epoch: 25 [12000/35212 (34%)]\tLoss: 0.080803\n",
            "Train Epoch: 25 [14000/35212 (40%)]\tLoss: 0.246546\n",
            "Train Epoch: 25 [16000/35212 (45%)]\tLoss: 0.109103\n",
            "Train Epoch: 25 [18000/35212 (51%)]\tLoss: 0.440035\n",
            "Train Epoch: 25 [20000/35212 (57%)]\tLoss: 0.134390\n",
            "Train Epoch: 25 [22000/35212 (62%)]\tLoss: 0.176499\n",
            "Train Epoch: 25 [24000/35212 (68%)]\tLoss: 0.238917\n",
            "Train Epoch: 25 [26000/35212 (74%)]\tLoss: 0.075209\n",
            "Train Epoch: 25 [28000/35212 (80%)]\tLoss: 0.137645\n",
            "Train Epoch: 25 [30000/35212 (85%)]\tLoss: 0.243038\n",
            "Train Epoch: 25 [32000/35212 (91%)]\tLoss: 0.227109\n",
            "Train Epoch: 25 [34000/35212 (97%)]\tLoss: 0.111048\n",
            "Train Epoch: 26 [0/35212 (0%)]\tLoss: 0.289620\n",
            "Train Epoch: 26 [2000/35212 (6%)]\tLoss: 0.066650\n",
            "Train Epoch: 26 [4000/35212 (11%)]\tLoss: 0.099447\n",
            "Train Epoch: 26 [6000/35212 (17%)]\tLoss: 0.107189\n",
            "Train Epoch: 26 [8000/35212 (23%)]\tLoss: 0.064101\n",
            "Train Epoch: 26 [10000/35212 (28%)]\tLoss: 0.179640\n",
            "Train Epoch: 26 [12000/35212 (34%)]\tLoss: 0.070376\n",
            "Train Epoch: 26 [14000/35212 (40%)]\tLoss: 0.198074\n",
            "Train Epoch: 26 [16000/35212 (45%)]\tLoss: 0.101354\n",
            "Train Epoch: 26 [18000/35212 (51%)]\tLoss: 0.297060\n",
            "Train Epoch: 26 [20000/35212 (57%)]\tLoss: 0.105066\n",
            "Train Epoch: 26 [22000/35212 (62%)]\tLoss: 0.146999\n",
            "Train Epoch: 26 [24000/35212 (68%)]\tLoss: 0.210071\n",
            "Train Epoch: 26 [26000/35212 (74%)]\tLoss: 0.225910\n",
            "Train Epoch: 26 [28000/35212 (80%)]\tLoss: 0.155632\n",
            "Train Epoch: 26 [30000/35212 (85%)]\tLoss: 0.125710\n",
            "Train Epoch: 26 [32000/35212 (91%)]\tLoss: 0.124251\n",
            "Train Epoch: 26 [34000/35212 (97%)]\tLoss: 0.190759\n",
            "Train Epoch: 27 [0/35212 (0%)]\tLoss: 0.347428\n",
            "Train Epoch: 27 [2000/35212 (6%)]\tLoss: 0.097571\n",
            "Train Epoch: 27 [4000/35212 (11%)]\tLoss: 0.195139\n",
            "Train Epoch: 27 [6000/35212 (17%)]\tLoss: 0.167853\n",
            "Train Epoch: 27 [8000/35212 (23%)]\tLoss: 0.040971\n",
            "Train Epoch: 27 [10000/35212 (28%)]\tLoss: 0.539592\n",
            "Train Epoch: 27 [12000/35212 (34%)]\tLoss: 0.124473\n",
            "Train Epoch: 27 [14000/35212 (40%)]\tLoss: 0.188974\n",
            "Train Epoch: 27 [16000/35212 (45%)]\tLoss: 0.080808\n",
            "Train Epoch: 27 [18000/35212 (51%)]\tLoss: 0.265538\n",
            "Train Epoch: 27 [20000/35212 (57%)]\tLoss: 0.072602\n",
            "Train Epoch: 27 [22000/35212 (62%)]\tLoss: 0.123888\n",
            "Train Epoch: 27 [24000/35212 (68%)]\tLoss: 0.131707\n",
            "Train Epoch: 27 [26000/35212 (74%)]\tLoss: 0.072451\n",
            "Train Epoch: 27 [28000/35212 (80%)]\tLoss: 0.133487\n",
            "Train Epoch: 27 [30000/35212 (85%)]\tLoss: 0.088254\n",
            "Train Epoch: 27 [32000/35212 (91%)]\tLoss: 0.156147\n",
            "Train Epoch: 27 [34000/35212 (97%)]\tLoss: 0.109023\n",
            "Train Epoch: 28 [0/35212 (0%)]\tLoss: 0.356825\n",
            "Train Epoch: 28 [2000/35212 (6%)]\tLoss: 0.074738\n",
            "Train Epoch: 28 [4000/35212 (11%)]\tLoss: 0.033423\n",
            "Train Epoch: 28 [6000/35212 (17%)]\tLoss: 0.206373\n",
            "Train Epoch: 28 [8000/35212 (23%)]\tLoss: 0.037574\n",
            "Train Epoch: 28 [10000/35212 (28%)]\tLoss: 0.106666\n",
            "Train Epoch: 28 [12000/35212 (34%)]\tLoss: 0.100764\n",
            "Train Epoch: 28 [14000/35212 (40%)]\tLoss: 0.214913\n",
            "Train Epoch: 28 [16000/35212 (45%)]\tLoss: 0.079322\n",
            "Train Epoch: 28 [18000/35212 (51%)]\tLoss: 0.223783\n",
            "Train Epoch: 28 [20000/35212 (57%)]\tLoss: 0.077468\n",
            "Train Epoch: 28 [22000/35212 (62%)]\tLoss: 0.126063\n",
            "Train Epoch: 28 [24000/35212 (68%)]\tLoss: 0.136326\n",
            "Train Epoch: 28 [26000/35212 (74%)]\tLoss: 0.148593\n",
            "Train Epoch: 28 [28000/35212 (80%)]\tLoss: 0.149703\n",
            "Train Epoch: 28 [30000/35212 (85%)]\tLoss: 0.112087\n",
            "Train Epoch: 28 [32000/35212 (91%)]\tLoss: 0.122915\n",
            "Train Epoch: 28 [34000/35212 (97%)]\tLoss: 0.118136\n",
            "Train Epoch: 29 [0/35212 (0%)]\tLoss: 0.406842\n",
            "Train Epoch: 29 [2000/35212 (6%)]\tLoss: 0.072086\n",
            "Train Epoch: 29 [4000/35212 (11%)]\tLoss: 0.119218\n",
            "Train Epoch: 29 [6000/35212 (17%)]\tLoss: 0.080415\n",
            "Train Epoch: 29 [8000/35212 (23%)]\tLoss: 0.060118\n",
            "Train Epoch: 29 [10000/35212 (28%)]\tLoss: 0.488947\n",
            "Train Epoch: 29 [12000/35212 (34%)]\tLoss: 0.084379\n",
            "Train Epoch: 29 [14000/35212 (40%)]\tLoss: 0.159492\n",
            "Train Epoch: 29 [16000/35212 (45%)]\tLoss: 0.094800\n",
            "Train Epoch: 29 [18000/35212 (51%)]\tLoss: 0.241172\n",
            "Train Epoch: 29 [20000/35212 (57%)]\tLoss: 0.064967\n",
            "Train Epoch: 29 [22000/35212 (62%)]\tLoss: 0.137177\n",
            "Train Epoch: 29 [24000/35212 (68%)]\tLoss: 0.132163\n",
            "Train Epoch: 29 [26000/35212 (74%)]\tLoss: 0.081276\n",
            "Train Epoch: 29 [28000/35212 (80%)]\tLoss: 0.102237\n",
            "Train Epoch: 29 [30000/35212 (85%)]\tLoss: 0.055965\n",
            "Train Epoch: 29 [32000/35212 (91%)]\tLoss: 0.173327\n",
            "Train Epoch: 29 [34000/35212 (97%)]\tLoss: 0.112996\n",
            "Train Epoch: 30 [0/35212 (0%)]\tLoss: 0.236273\n",
            "Train Epoch: 30 [2000/35212 (6%)]\tLoss: 0.069455\n",
            "Train Epoch: 30 [4000/35212 (11%)]\tLoss: 0.075232\n",
            "Train Epoch: 30 [6000/35212 (17%)]\tLoss: 0.130739\n",
            "Train Epoch: 30 [8000/35212 (23%)]\tLoss: 0.043227\n",
            "Train Epoch: 30 [10000/35212 (28%)]\tLoss: 0.132728\n",
            "Train Epoch: 30 [12000/35212 (34%)]\tLoss: 0.073401\n",
            "Train Epoch: 30 [14000/35212 (40%)]\tLoss: 0.265203\n",
            "Train Epoch: 30 [16000/35212 (45%)]\tLoss: 0.093022\n",
            "Train Epoch: 30 [18000/35212 (51%)]\tLoss: 0.274855\n",
            "Train Epoch: 30 [20000/35212 (57%)]\tLoss: 0.088308\n",
            "Train Epoch: 30 [22000/35212 (62%)]\tLoss: 0.097487\n",
            "Train Epoch: 30 [24000/35212 (68%)]\tLoss: 0.082964\n",
            "Train Epoch: 30 [26000/35212 (74%)]\tLoss: 0.209108\n",
            "Train Epoch: 30 [28000/35212 (80%)]\tLoss: 0.179828\n",
            "Train Epoch: 30 [30000/35212 (85%)]\tLoss: 0.085457\n",
            "Train Epoch: 30 [32000/35212 (91%)]\tLoss: 0.164852\n",
            "Train Epoch: 30 [34000/35212 (97%)]\tLoss: 0.117339\n",
            "Train Epoch: 31 [0/35212 (0%)]\tLoss: 0.283055\n",
            "Train Epoch: 31 [2000/35212 (6%)]\tLoss: 0.085184\n",
            "Train Epoch: 31 [4000/35212 (11%)]\tLoss: 0.070065\n",
            "Train Epoch: 31 [6000/35212 (17%)]\tLoss: 0.116128\n",
            "Train Epoch: 31 [8000/35212 (23%)]\tLoss: 0.076989\n",
            "Train Epoch: 31 [10000/35212 (28%)]\tLoss: 0.292678\n",
            "Train Epoch: 31 [12000/35212 (34%)]\tLoss: 0.069771\n",
            "Train Epoch: 31 [14000/35212 (40%)]\tLoss: 0.153632\n",
            "Train Epoch: 31 [16000/35212 (45%)]\tLoss: 0.052497\n",
            "Train Epoch: 31 [18000/35212 (51%)]\tLoss: 0.241233\n",
            "Train Epoch: 31 [20000/35212 (57%)]\tLoss: 0.077688\n",
            "Train Epoch: 31 [22000/35212 (62%)]\tLoss: 0.104225\n",
            "Train Epoch: 31 [24000/35212 (68%)]\tLoss: 0.173770\n",
            "Train Epoch: 31 [26000/35212 (74%)]\tLoss: 0.054333\n",
            "Train Epoch: 31 [28000/35212 (80%)]\tLoss: 0.105244\n",
            "Train Epoch: 31 [30000/35212 (85%)]\tLoss: 0.084428\n",
            "Train Epoch: 31 [32000/35212 (91%)]\tLoss: 0.136875\n",
            "Train Epoch: 31 [34000/35212 (97%)]\tLoss: 0.123104\n",
            "Train Epoch: 32 [0/35212 (0%)]\tLoss: 0.200612\n",
            "Train Epoch: 32 [2000/35212 (6%)]\tLoss: 0.083178\n",
            "Train Epoch: 32 [4000/35212 (11%)]\tLoss: 0.042679\n",
            "Train Epoch: 32 [6000/35212 (17%)]\tLoss: 0.160003\n",
            "Train Epoch: 32 [8000/35212 (23%)]\tLoss: 0.034738\n",
            "Train Epoch: 32 [10000/35212 (28%)]\tLoss: 0.080839\n",
            "Train Epoch: 32 [12000/35212 (34%)]\tLoss: 0.078370\n",
            "Train Epoch: 32 [14000/35212 (40%)]\tLoss: 0.223171\n",
            "Train Epoch: 32 [16000/35212 (45%)]\tLoss: 0.069110\n",
            "Train Epoch: 32 [18000/35212 (51%)]\tLoss: 0.206353\n",
            "Train Epoch: 32 [20000/35212 (57%)]\tLoss: 0.050195\n",
            "Train Epoch: 32 [22000/35212 (62%)]\tLoss: 0.107396\n",
            "Train Epoch: 32 [24000/35212 (68%)]\tLoss: 0.154582\n",
            "Train Epoch: 32 [26000/35212 (74%)]\tLoss: 0.109918\n",
            "Train Epoch: 32 [28000/35212 (80%)]\tLoss: 0.161998\n",
            "Train Epoch: 32 [30000/35212 (85%)]\tLoss: 0.138305\n",
            "Train Epoch: 32 [32000/35212 (91%)]\tLoss: 0.182858\n",
            "Train Epoch: 32 [34000/35212 (97%)]\tLoss: 0.108861\n",
            "Train Epoch: 33 [0/35212 (0%)]\tLoss: 0.169591\n",
            "Train Epoch: 33 [2000/35212 (6%)]\tLoss: 0.079913\n",
            "Train Epoch: 33 [4000/35212 (11%)]\tLoss: 0.031408\n",
            "Train Epoch: 33 [6000/35212 (17%)]\tLoss: 0.200154\n",
            "Train Epoch: 33 [8000/35212 (23%)]\tLoss: 0.044359\n",
            "Train Epoch: 33 [10000/35212 (28%)]\tLoss: 0.177588\n",
            "Train Epoch: 33 [12000/35212 (34%)]\tLoss: 0.060414\n",
            "Train Epoch: 33 [14000/35212 (40%)]\tLoss: 0.130331\n",
            "Train Epoch: 33 [16000/35212 (45%)]\tLoss: 0.065341\n",
            "Train Epoch: 33 [18000/35212 (51%)]\tLoss: 0.289746\n",
            "Train Epoch: 33 [20000/35212 (57%)]\tLoss: 0.052976\n",
            "Train Epoch: 33 [22000/35212 (62%)]\tLoss: 0.074309\n",
            "Train Epoch: 33 [24000/35212 (68%)]\tLoss: 0.093687\n",
            "Train Epoch: 33 [26000/35212 (74%)]\tLoss: 0.048022\n",
            "Train Epoch: 33 [28000/35212 (80%)]\tLoss: 0.141437\n",
            "Train Epoch: 33 [30000/35212 (85%)]\tLoss: 0.075661\n",
            "Train Epoch: 33 [32000/35212 (91%)]\tLoss: 0.171446\n",
            "Train Epoch: 33 [34000/35212 (97%)]\tLoss: 0.126262\n",
            "Train Epoch: 34 [0/35212 (0%)]\tLoss: 0.166132\n",
            "Train Epoch: 34 [2000/35212 (6%)]\tLoss: 0.070599\n",
            "Train Epoch: 34 [4000/35212 (11%)]\tLoss: 0.107813\n",
            "Train Epoch: 34 [6000/35212 (17%)]\tLoss: 0.209973\n",
            "Train Epoch: 34 [8000/35212 (23%)]\tLoss: 0.042582\n",
            "Train Epoch: 34 [10000/35212 (28%)]\tLoss: 0.112665\n",
            "Train Epoch: 34 [12000/35212 (34%)]\tLoss: 0.056398\n",
            "Train Epoch: 34 [14000/35212 (40%)]\tLoss: 0.151129\n",
            "Train Epoch: 34 [16000/35212 (45%)]\tLoss: 0.061266\n",
            "Train Epoch: 34 [18000/35212 (51%)]\tLoss: 0.251143\n",
            "Train Epoch: 34 [20000/35212 (57%)]\tLoss: 0.030645\n",
            "Train Epoch: 34 [22000/35212 (62%)]\tLoss: 0.076686\n",
            "Train Epoch: 34 [24000/35212 (68%)]\tLoss: 0.085376\n",
            "Train Epoch: 34 [26000/35212 (74%)]\tLoss: 0.097003\n",
            "Train Epoch: 34 [28000/35212 (80%)]\tLoss: 0.102753\n",
            "Train Epoch: 34 [30000/35212 (85%)]\tLoss: 0.094426\n",
            "Train Epoch: 34 [32000/35212 (91%)]\tLoss: 0.198762\n",
            "Train Epoch: 34 [34000/35212 (97%)]\tLoss: 0.074994\n",
            "Train Epoch: 35 [0/35212 (0%)]\tLoss: 0.170337\n",
            "Train Epoch: 35 [2000/35212 (6%)]\tLoss: 0.065719\n",
            "Train Epoch: 35 [4000/35212 (11%)]\tLoss: 0.023513\n",
            "Train Epoch: 35 [6000/35212 (17%)]\tLoss: 0.052238\n",
            "Train Epoch: 35 [8000/35212 (23%)]\tLoss: 0.042689\n",
            "Train Epoch: 35 [10000/35212 (28%)]\tLoss: 0.073814\n",
            "Train Epoch: 35 [12000/35212 (34%)]\tLoss: 0.067889\n",
            "Train Epoch: 35 [14000/35212 (40%)]\tLoss: 0.130182\n",
            "Train Epoch: 35 [16000/35212 (45%)]\tLoss: 0.034085\n",
            "Train Epoch: 35 [18000/35212 (51%)]\tLoss: 0.183653\n",
            "Train Epoch: 35 [20000/35212 (57%)]\tLoss: 0.041295\n",
            "Train Epoch: 35 [22000/35212 (62%)]\tLoss: 0.051354\n",
            "Train Epoch: 35 [24000/35212 (68%)]\tLoss: 0.057399\n",
            "Train Epoch: 35 [26000/35212 (74%)]\tLoss: 0.082407\n",
            "Train Epoch: 35 [28000/35212 (80%)]\tLoss: 0.168618\n",
            "Train Epoch: 35 [30000/35212 (85%)]\tLoss: 0.071548\n",
            "Train Epoch: 35 [32000/35212 (91%)]\tLoss: 0.161686\n",
            "Train Epoch: 35 [34000/35212 (97%)]\tLoss: 0.116389\n",
            "Train Epoch: 36 [0/35212 (0%)]\tLoss: 0.158597\n",
            "Train Epoch: 36 [2000/35212 (6%)]\tLoss: 0.057255\n",
            "Train Epoch: 36 [4000/35212 (11%)]\tLoss: 0.088737\n",
            "Train Epoch: 36 [6000/35212 (17%)]\tLoss: 0.103692\n",
            "Train Epoch: 36 [8000/35212 (23%)]\tLoss: 0.053917\n",
            "Train Epoch: 36 [10000/35212 (28%)]\tLoss: 0.144294\n",
            "Train Epoch: 36 [12000/35212 (34%)]\tLoss: 0.055952\n",
            "Train Epoch: 36 [14000/35212 (40%)]\tLoss: 0.159885\n",
            "Train Epoch: 36 [16000/35212 (45%)]\tLoss: 0.039479\n",
            "Train Epoch: 36 [18000/35212 (51%)]\tLoss: 0.232960\n",
            "Train Epoch: 36 [20000/35212 (57%)]\tLoss: 0.044015\n",
            "Train Epoch: 36 [22000/35212 (62%)]\tLoss: 0.054214\n",
            "Train Epoch: 36 [24000/35212 (68%)]\tLoss: 0.054453\n",
            "Train Epoch: 36 [26000/35212 (74%)]\tLoss: 0.054184\n",
            "Train Epoch: 36 [28000/35212 (80%)]\tLoss: 0.128457\n",
            "Train Epoch: 36 [30000/35212 (85%)]\tLoss: 0.014931\n",
            "Train Epoch: 36 [32000/35212 (91%)]\tLoss: 0.169020\n",
            "Train Epoch: 36 [34000/35212 (97%)]\tLoss: 0.058866\n",
            "Train Epoch: 37 [0/35212 (0%)]\tLoss: 0.118626\n",
            "Train Epoch: 37 [2000/35212 (6%)]\tLoss: 0.049511\n",
            "Train Epoch: 37 [4000/35212 (11%)]\tLoss: 0.043438\n",
            "Train Epoch: 37 [6000/35212 (17%)]\tLoss: 0.118280\n",
            "Train Epoch: 37 [8000/35212 (23%)]\tLoss: 0.041003\n",
            "Train Epoch: 37 [10000/35212 (28%)]\tLoss: 0.096419\n",
            "Train Epoch: 37 [12000/35212 (34%)]\tLoss: 0.071611\n",
            "Train Epoch: 37 [14000/35212 (40%)]\tLoss: 0.095584\n",
            "Train Epoch: 37 [16000/35212 (45%)]\tLoss: 0.025368\n",
            "Train Epoch: 37 [18000/35212 (51%)]\tLoss: 0.129291\n",
            "Train Epoch: 37 [20000/35212 (57%)]\tLoss: 0.038968\n",
            "Train Epoch: 37 [22000/35212 (62%)]\tLoss: 0.032871\n",
            "Train Epoch: 37 [24000/35212 (68%)]\tLoss: 0.039133\n",
            "Train Epoch: 37 [26000/35212 (74%)]\tLoss: 0.141159\n",
            "Train Epoch: 37 [28000/35212 (80%)]\tLoss: 0.179361\n",
            "Train Epoch: 37 [30000/35212 (85%)]\tLoss: 0.022603\n",
            "Train Epoch: 37 [32000/35212 (91%)]\tLoss: 0.197184\n",
            "Train Epoch: 37 [34000/35212 (97%)]\tLoss: 0.092374\n",
            "Train Epoch: 38 [0/35212 (0%)]\tLoss: 0.125409\n",
            "Train Epoch: 38 [2000/35212 (6%)]\tLoss: 0.070806\n",
            "Train Epoch: 38 [4000/35212 (11%)]\tLoss: 0.026252\n",
            "Train Epoch: 38 [6000/35212 (17%)]\tLoss: 0.135606\n",
            "Train Epoch: 38 [8000/35212 (23%)]\tLoss: 0.031893\n",
            "Train Epoch: 38 [10000/35212 (28%)]\tLoss: 0.130718\n",
            "Train Epoch: 38 [12000/35212 (34%)]\tLoss: 0.033193\n",
            "Train Epoch: 38 [14000/35212 (40%)]\tLoss: 0.136448\n",
            "Train Epoch: 38 [16000/35212 (45%)]\tLoss: 0.039808\n",
            "Train Epoch: 38 [18000/35212 (51%)]\tLoss: 0.104729\n",
            "Train Epoch: 38 [20000/35212 (57%)]\tLoss: 0.054431\n",
            "Train Epoch: 38 [22000/35212 (62%)]\tLoss: 0.043067\n",
            "Train Epoch: 38 [24000/35212 (68%)]\tLoss: 0.080537\n",
            "Train Epoch: 38 [26000/35212 (74%)]\tLoss: 0.028852\n",
            "Train Epoch: 38 [28000/35212 (80%)]\tLoss: 0.115216\n",
            "Train Epoch: 38 [30000/35212 (85%)]\tLoss: 0.017689\n",
            "Train Epoch: 38 [32000/35212 (91%)]\tLoss: 0.144966\n",
            "Train Epoch: 38 [34000/35212 (97%)]\tLoss: 0.047050\n",
            "Train Epoch: 39 [0/35212 (0%)]\tLoss: 0.112471\n",
            "Train Epoch: 39 [2000/35212 (6%)]\tLoss: 0.054737\n",
            "Train Epoch: 39 [4000/35212 (11%)]\tLoss: 0.071016\n",
            "Train Epoch: 39 [6000/35212 (17%)]\tLoss: 0.045636\n",
            "Train Epoch: 39 [8000/35212 (23%)]\tLoss: 0.040246\n",
            "Train Epoch: 39 [10000/35212 (28%)]\tLoss: 0.109108\n",
            "Train Epoch: 39 [12000/35212 (34%)]\tLoss: 0.046771\n",
            "Train Epoch: 39 [14000/35212 (40%)]\tLoss: 0.098521\n",
            "Train Epoch: 39 [16000/35212 (45%)]\tLoss: 0.033575\n",
            "Train Epoch: 39 [18000/35212 (51%)]\tLoss: 0.134853\n",
            "Train Epoch: 39 [20000/35212 (57%)]\tLoss: 0.046265\n",
            "Train Epoch: 39 [22000/35212 (62%)]\tLoss: 0.054532\n",
            "Train Epoch: 39 [24000/35212 (68%)]\tLoss: 0.041453\n",
            "Train Epoch: 39 [26000/35212 (74%)]\tLoss: 0.146622\n",
            "Train Epoch: 39 [28000/35212 (80%)]\tLoss: 0.163654\n",
            "Train Epoch: 39 [30000/35212 (85%)]\tLoss: 0.054993\n",
            "Train Epoch: 39 [32000/35212 (91%)]\tLoss: 0.117647\n",
            "Train Epoch: 39 [34000/35212 (97%)]\tLoss: 0.059994\n",
            "Train Epoch: 40 [0/35212 (0%)]\tLoss: 0.090542\n",
            "Train Epoch: 40 [2000/35212 (6%)]\tLoss: 0.050682\n",
            "Train Epoch: 40 [4000/35212 (11%)]\tLoss: 0.038189\n",
            "Train Epoch: 40 [6000/35212 (17%)]\tLoss: 0.153395\n",
            "Train Epoch: 40 [8000/35212 (23%)]\tLoss: 0.034384\n",
            "Train Epoch: 40 [10000/35212 (28%)]\tLoss: 0.100670\n",
            "Train Epoch: 40 [12000/35212 (34%)]\tLoss: 0.031809\n",
            "Train Epoch: 40 [14000/35212 (40%)]\tLoss: 0.131204\n",
            "Train Epoch: 40 [16000/35212 (45%)]\tLoss: 0.017391\n",
            "Train Epoch: 40 [18000/35212 (51%)]\tLoss: 0.146121\n",
            "Train Epoch: 40 [20000/35212 (57%)]\tLoss: 0.032375\n",
            "Train Epoch: 40 [22000/35212 (62%)]\tLoss: 0.034596\n",
            "Train Epoch: 40 [24000/35212 (68%)]\tLoss: 0.045661\n",
            "Train Epoch: 40 [26000/35212 (74%)]\tLoss: 0.048414\n",
            "Train Epoch: 40 [28000/35212 (80%)]\tLoss: 0.109434\n",
            "Train Epoch: 40 [30000/35212 (85%)]\tLoss: 0.156088\n",
            "Train Epoch: 40 [32000/35212 (91%)]\tLoss: 0.146813\n",
            "Train Epoch: 40 [34000/35212 (97%)]\tLoss: 0.036691\n",
            "Train Epoch: 41 [0/35212 (0%)]\tLoss: 0.095535\n",
            "Train Epoch: 41 [2000/35212 (6%)]\tLoss: 0.047168\n",
            "Train Epoch: 41 [4000/35212 (11%)]\tLoss: 0.033857\n",
            "Train Epoch: 41 [6000/35212 (17%)]\tLoss: 0.078169\n",
            "Train Epoch: 41 [8000/35212 (23%)]\tLoss: 0.018577\n",
            "Train Epoch: 41 [10000/35212 (28%)]\tLoss: 0.481032\n",
            "Train Epoch: 41 [12000/35212 (34%)]\tLoss: 0.067298\n",
            "Train Epoch: 41 [14000/35212 (40%)]\tLoss: 0.085679\n",
            "Train Epoch: 41 [16000/35212 (45%)]\tLoss: 0.063429\n",
            "Train Epoch: 41 [18000/35212 (51%)]\tLoss: 0.097862\n",
            "Train Epoch: 41 [20000/35212 (57%)]\tLoss: 0.036170\n",
            "Train Epoch: 41 [22000/35212 (62%)]\tLoss: 0.054824\n",
            "Train Epoch: 41 [24000/35212 (68%)]\tLoss: 0.054410\n",
            "Train Epoch: 41 [26000/35212 (74%)]\tLoss: 0.082251\n",
            "Train Epoch: 41 [28000/35212 (80%)]\tLoss: 0.097126\n",
            "Train Epoch: 41 [30000/35212 (85%)]\tLoss: 0.153687\n",
            "Train Epoch: 41 [32000/35212 (91%)]\tLoss: 0.156363\n",
            "Train Epoch: 41 [34000/35212 (97%)]\tLoss: 0.082310\n",
            "Train Epoch: 42 [0/35212 (0%)]\tLoss: 0.090984\n",
            "Train Epoch: 42 [2000/35212 (6%)]\tLoss: 0.047933\n",
            "Train Epoch: 42 [4000/35212 (11%)]\tLoss: 0.027895\n",
            "Train Epoch: 42 [6000/35212 (17%)]\tLoss: 0.089317\n",
            "Train Epoch: 42 [8000/35212 (23%)]\tLoss: 0.032705\n",
            "Train Epoch: 42 [10000/35212 (28%)]\tLoss: 0.064020\n",
            "Train Epoch: 42 [12000/35212 (34%)]\tLoss: 0.041465\n",
            "Train Epoch: 42 [14000/35212 (40%)]\tLoss: 0.080526\n",
            "Train Epoch: 42 [16000/35212 (45%)]\tLoss: 0.019142\n",
            "Train Epoch: 42 [18000/35212 (51%)]\tLoss: 0.145437\n",
            "Train Epoch: 42 [20000/35212 (57%)]\tLoss: 0.032851\n",
            "Train Epoch: 42 [22000/35212 (62%)]\tLoss: 0.043224\n",
            "Train Epoch: 42 [24000/35212 (68%)]\tLoss: 0.029451\n",
            "Train Epoch: 42 [26000/35212 (74%)]\tLoss: 0.029591\n",
            "Train Epoch: 42 [28000/35212 (80%)]\tLoss: 0.069887\n",
            "Train Epoch: 42 [30000/35212 (85%)]\tLoss: 0.040281\n",
            "Train Epoch: 42 [32000/35212 (91%)]\tLoss: 0.111898\n",
            "Train Epoch: 42 [34000/35212 (97%)]\tLoss: 0.029466\n",
            "Train Epoch: 43 [0/35212 (0%)]\tLoss: 0.100658\n",
            "Train Epoch: 43 [2000/35212 (6%)]\tLoss: 0.042163\n",
            "Train Epoch: 43 [4000/35212 (11%)]\tLoss: 0.034872\n",
            "Train Epoch: 43 [6000/35212 (17%)]\tLoss: 0.048076\n",
            "Train Epoch: 43 [8000/35212 (23%)]\tLoss: 0.028186\n",
            "Train Epoch: 43 [10000/35212 (28%)]\tLoss: 0.072465\n",
            "Train Epoch: 43 [12000/35212 (34%)]\tLoss: 0.031920\n",
            "Train Epoch: 43 [14000/35212 (40%)]\tLoss: 0.078211\n",
            "Train Epoch: 43 [16000/35212 (45%)]\tLoss: 0.059038\n",
            "Train Epoch: 43 [18000/35212 (51%)]\tLoss: 0.106037\n",
            "Train Epoch: 43 [20000/35212 (57%)]\tLoss: 0.035786\n",
            "Train Epoch: 43 [22000/35212 (62%)]\tLoss: 0.033985\n",
            "Train Epoch: 43 [24000/35212 (68%)]\tLoss: 0.028799\n",
            "Train Epoch: 43 [26000/35212 (74%)]\tLoss: 0.046223\n",
            "Train Epoch: 43 [28000/35212 (80%)]\tLoss: 0.097860\n",
            "Train Epoch: 43 [30000/35212 (85%)]\tLoss: 0.028447\n",
            "Train Epoch: 43 [32000/35212 (91%)]\tLoss: 0.153780\n",
            "Train Epoch: 43 [34000/35212 (97%)]\tLoss: 0.043268\n",
            "Train Epoch: 44 [0/35212 (0%)]\tLoss: 0.080743\n",
            "Train Epoch: 44 [2000/35212 (6%)]\tLoss: 0.040471\n",
            "Train Epoch: 44 [4000/35212 (11%)]\tLoss: 0.031735\n",
            "Train Epoch: 44 [6000/35212 (17%)]\tLoss: 0.045640\n",
            "Train Epoch: 44 [8000/35212 (23%)]\tLoss: 0.031139\n",
            "Train Epoch: 44 [10000/35212 (28%)]\tLoss: 0.098659\n",
            "Train Epoch: 44 [12000/35212 (34%)]\tLoss: 0.052974\n",
            "Train Epoch: 44 [14000/35212 (40%)]\tLoss: 0.076323\n",
            "Train Epoch: 44 [16000/35212 (45%)]\tLoss: 0.016516\n",
            "Train Epoch: 44 [18000/35212 (51%)]\tLoss: 0.123507\n",
            "Train Epoch: 44 [20000/35212 (57%)]\tLoss: 0.033408\n",
            "Train Epoch: 44 [22000/35212 (62%)]\tLoss: 0.063983\n",
            "Train Epoch: 44 [24000/35212 (68%)]\tLoss: 0.083805\n",
            "Train Epoch: 44 [26000/35212 (74%)]\tLoss: 0.157202\n",
            "Train Epoch: 44 [28000/35212 (80%)]\tLoss: 0.100064\n",
            "Train Epoch: 44 [30000/35212 (85%)]\tLoss: 0.059517\n",
            "Train Epoch: 44 [32000/35212 (91%)]\tLoss: 0.099579\n",
            "Train Epoch: 44 [34000/35212 (97%)]\tLoss: 0.015396\n",
            "Train Epoch: 45 [0/35212 (0%)]\tLoss: 0.066300\n",
            "Train Epoch: 45 [2000/35212 (6%)]\tLoss: 0.063647\n",
            "Train Epoch: 45 [4000/35212 (11%)]\tLoss: 0.049753\n",
            "Train Epoch: 45 [6000/35212 (17%)]\tLoss: 0.048439\n",
            "Train Epoch: 45 [8000/35212 (23%)]\tLoss: 0.021250\n",
            "Train Epoch: 45 [10000/35212 (28%)]\tLoss: 0.103889\n",
            "Train Epoch: 45 [12000/35212 (34%)]\tLoss: 0.050869\n",
            "Train Epoch: 45 [14000/35212 (40%)]\tLoss: 0.072943\n",
            "Train Epoch: 45 [16000/35212 (45%)]\tLoss: 0.024145\n",
            "Train Epoch: 45 [18000/35212 (51%)]\tLoss: 0.083531\n",
            "Train Epoch: 45 [20000/35212 (57%)]\tLoss: 0.046067\n",
            "Train Epoch: 45 [22000/35212 (62%)]\tLoss: 0.030926\n",
            "Train Epoch: 45 [24000/35212 (68%)]\tLoss: 0.127486\n",
            "Train Epoch: 45 [26000/35212 (74%)]\tLoss: 0.087840\n",
            "Train Epoch: 45 [28000/35212 (80%)]\tLoss: 0.139796\n",
            "Train Epoch: 45 [30000/35212 (85%)]\tLoss: 0.050417\n",
            "Train Epoch: 45 [32000/35212 (91%)]\tLoss: 0.117321\n",
            "Train Epoch: 45 [34000/35212 (97%)]\tLoss: 0.047878\n",
            "Train Epoch: 46 [0/35212 (0%)]\tLoss: 0.080239\n",
            "Train Epoch: 46 [2000/35212 (6%)]\tLoss: 0.041114\n",
            "Train Epoch: 46 [4000/35212 (11%)]\tLoss: 0.034703\n",
            "Train Epoch: 46 [6000/35212 (17%)]\tLoss: 0.026089\n",
            "Train Epoch: 46 [8000/35212 (23%)]\tLoss: 0.027839\n",
            "Train Epoch: 46 [10000/35212 (28%)]\tLoss: 0.069506\n",
            "Train Epoch: 46 [12000/35212 (34%)]\tLoss: 0.041722\n",
            "Train Epoch: 46 [14000/35212 (40%)]\tLoss: 0.088895\n",
            "Train Epoch: 46 [16000/35212 (45%)]\tLoss: 0.037315\n",
            "Train Epoch: 46 [18000/35212 (51%)]\tLoss: 0.086003\n",
            "Train Epoch: 46 [20000/35212 (57%)]\tLoss: 0.022206\n",
            "Train Epoch: 46 [22000/35212 (62%)]\tLoss: 0.038623\n",
            "Train Epoch: 46 [24000/35212 (68%)]\tLoss: 0.033491\n",
            "Train Epoch: 46 [26000/35212 (74%)]\tLoss: 0.041130\n",
            "Train Epoch: 46 [28000/35212 (80%)]\tLoss: 0.091674\n",
            "Train Epoch: 46 [30000/35212 (85%)]\tLoss: 0.035497\n",
            "Train Epoch: 46 [32000/35212 (91%)]\tLoss: 0.087396\n",
            "Train Epoch: 46 [34000/35212 (97%)]\tLoss: 0.028347\n",
            "Train Epoch: 47 [0/35212 (0%)]\tLoss: 0.071198\n",
            "Train Epoch: 47 [2000/35212 (6%)]\tLoss: 0.039997\n",
            "Train Epoch: 47 [4000/35212 (11%)]\tLoss: 0.031981\n",
            "Train Epoch: 47 [6000/35212 (17%)]\tLoss: 0.042374\n",
            "Train Epoch: 47 [8000/35212 (23%)]\tLoss: 0.025737\n",
            "Train Epoch: 47 [10000/35212 (28%)]\tLoss: 0.088135\n",
            "Train Epoch: 47 [12000/35212 (34%)]\tLoss: 0.036776\n",
            "Train Epoch: 47 [14000/35212 (40%)]\tLoss: 0.083651\n",
            "Train Epoch: 47 [16000/35212 (45%)]\tLoss: 0.028286\n",
            "Train Epoch: 47 [18000/35212 (51%)]\tLoss: 0.075282\n",
            "Train Epoch: 47 [20000/35212 (57%)]\tLoss: 0.027743\n",
            "Train Epoch: 47 [22000/35212 (62%)]\tLoss: 0.033379\n",
            "Train Epoch: 47 [24000/35212 (68%)]\tLoss: 0.037159\n",
            "Train Epoch: 47 [26000/35212 (74%)]\tLoss: 0.026518\n",
            "Train Epoch: 47 [28000/35212 (80%)]\tLoss: 0.056176\n",
            "Train Epoch: 47 [30000/35212 (85%)]\tLoss: 0.013006\n",
            "Train Epoch: 47 [32000/35212 (91%)]\tLoss: 0.102675\n",
            "Train Epoch: 47 [34000/35212 (97%)]\tLoss: 0.024134\n",
            "Train Epoch: 48 [0/35212 (0%)]\tLoss: 0.053458\n",
            "Train Epoch: 48 [2000/35212 (6%)]\tLoss: 0.043902\n",
            "Train Epoch: 48 [4000/35212 (11%)]\tLoss: 0.030668\n",
            "Train Epoch: 48 [6000/35212 (17%)]\tLoss: 0.030102\n",
            "Train Epoch: 48 [8000/35212 (23%)]\tLoss: 0.018386\n",
            "Train Epoch: 48 [10000/35212 (28%)]\tLoss: 0.070425\n",
            "Train Epoch: 48 [12000/35212 (34%)]\tLoss: 0.038083\n",
            "Train Epoch: 48 [14000/35212 (40%)]\tLoss: 0.057184\n",
            "Train Epoch: 48 [16000/35212 (45%)]\tLoss: 0.021447\n",
            "Train Epoch: 48 [18000/35212 (51%)]\tLoss: 0.049976\n",
            "Train Epoch: 48 [20000/35212 (57%)]\tLoss: 0.030947\n",
            "Train Epoch: 48 [22000/35212 (62%)]\tLoss: 0.028248\n",
            "Train Epoch: 48 [24000/35212 (68%)]\tLoss: 0.051054\n",
            "Train Epoch: 48 [26000/35212 (74%)]\tLoss: 0.051874\n",
            "Train Epoch: 48 [28000/35212 (80%)]\tLoss: 0.105636\n",
            "Train Epoch: 48 [30000/35212 (85%)]\tLoss: 0.022365\n",
            "Train Epoch: 48 [32000/35212 (91%)]\tLoss: 0.078538\n",
            "Train Epoch: 48 [34000/35212 (97%)]\tLoss: 0.034086\n",
            "Train Epoch: 49 [0/35212 (0%)]\tLoss: 0.051749\n",
            "Train Epoch: 49 [2000/35212 (6%)]\tLoss: 0.060613\n",
            "Train Epoch: 49 [4000/35212 (11%)]\tLoss: 0.025365\n",
            "Train Epoch: 49 [6000/35212 (17%)]\tLoss: 0.029560\n",
            "Train Epoch: 49 [8000/35212 (23%)]\tLoss: 0.029212\n",
            "Train Epoch: 49 [10000/35212 (28%)]\tLoss: 0.089361\n",
            "Train Epoch: 49 [12000/35212 (34%)]\tLoss: 0.037861\n",
            "Train Epoch: 49 [14000/35212 (40%)]\tLoss: 0.076100\n",
            "Train Epoch: 49 [16000/35212 (45%)]\tLoss: 0.040867\n",
            "Train Epoch: 49 [18000/35212 (51%)]\tLoss: 0.082235\n",
            "Train Epoch: 49 [20000/35212 (57%)]\tLoss: 0.033603\n",
            "Train Epoch: 49 [22000/35212 (62%)]\tLoss: 0.027369\n",
            "Train Epoch: 49 [24000/35212 (68%)]\tLoss: 0.025952\n",
            "Train Epoch: 49 [26000/35212 (74%)]\tLoss: 0.024262\n",
            "Train Epoch: 49 [28000/35212 (80%)]\tLoss: 0.064781\n",
            "Train Epoch: 49 [30000/35212 (85%)]\tLoss: 0.014246\n",
            "Train Epoch: 49 [32000/35212 (91%)]\tLoss: 0.051905\n",
            "Train Epoch: 49 [34000/35212 (97%)]\tLoss: 0.014099\n",
            "Train Epoch: 50 [0/35212 (0%)]\tLoss: 0.050714\n",
            "Train Epoch: 50 [2000/35212 (6%)]\tLoss: 0.026691\n",
            "Train Epoch: 50 [4000/35212 (11%)]\tLoss: 0.021047\n",
            "Train Epoch: 50 [6000/35212 (17%)]\tLoss: 0.077403\n",
            "Train Epoch: 50 [8000/35212 (23%)]\tLoss: 0.015687\n",
            "Train Epoch: 50 [10000/35212 (28%)]\tLoss: 0.067962\n",
            "Train Epoch: 50 [12000/35212 (34%)]\tLoss: 0.032023\n",
            "Train Epoch: 50 [14000/35212 (40%)]\tLoss: 0.054772\n",
            "Train Epoch: 50 [16000/35212 (45%)]\tLoss: 0.025121\n",
            "Train Epoch: 50 [18000/35212 (51%)]\tLoss: 0.076910\n",
            "Train Epoch: 50 [20000/35212 (57%)]\tLoss: 0.025452\n",
            "Train Epoch: 50 [22000/35212 (62%)]\tLoss: 0.019578\n",
            "Train Epoch: 50 [24000/35212 (68%)]\tLoss: 0.013147\n",
            "Train Epoch: 50 [26000/35212 (74%)]\tLoss: 0.042463\n",
            "Train Epoch: 50 [28000/35212 (80%)]\tLoss: 0.061068\n",
            "Train Epoch: 50 [30000/35212 (85%)]\tLoss: 0.011933\n",
            "Train Epoch: 50 [32000/35212 (91%)]\tLoss: 0.075781\n",
            "Train Epoch: 50 [34000/35212 (97%)]\tLoss: 0.020405\n",
            "Train Epoch: 51 [0/35212 (0%)]\tLoss: 0.047499\n",
            "Train Epoch: 51 [2000/35212 (6%)]\tLoss: 0.037392\n",
            "Train Epoch: 51 [4000/35212 (11%)]\tLoss: 0.051891\n",
            "Train Epoch: 51 [6000/35212 (17%)]\tLoss: 0.041200\n",
            "Train Epoch: 51 [8000/35212 (23%)]\tLoss: 0.016607\n",
            "Train Epoch: 51 [10000/35212 (28%)]\tLoss: 0.084107\n",
            "Train Epoch: 51 [12000/35212 (34%)]\tLoss: 0.038307\n",
            "Train Epoch: 51 [14000/35212 (40%)]\tLoss: 0.060098\n",
            "Train Epoch: 51 [16000/35212 (45%)]\tLoss: 0.026027\n",
            "Train Epoch: 51 [18000/35212 (51%)]\tLoss: 0.061158\n",
            "Train Epoch: 51 [20000/35212 (57%)]\tLoss: 0.028500\n",
            "Train Epoch: 51 [22000/35212 (62%)]\tLoss: 0.028689\n",
            "Train Epoch: 51 [24000/35212 (68%)]\tLoss: 0.028698\n",
            "Train Epoch: 51 [26000/35212 (74%)]\tLoss: 0.037423\n",
            "Train Epoch: 51 [28000/35212 (80%)]\tLoss: 0.061110\n",
            "Train Epoch: 51 [30000/35212 (85%)]\tLoss: 0.009555\n",
            "Train Epoch: 51 [32000/35212 (91%)]\tLoss: 0.061823\n",
            "Train Epoch: 51 [34000/35212 (97%)]\tLoss: 0.013590\n",
            "Train Epoch: 52 [0/35212 (0%)]\tLoss: 0.044396\n",
            "Train Epoch: 52 [2000/35212 (6%)]\tLoss: 0.029676\n",
            "Train Epoch: 52 [4000/35212 (11%)]\tLoss: 0.012316\n",
            "Train Epoch: 52 [6000/35212 (17%)]\tLoss: 0.036065\n",
            "Train Epoch: 52 [8000/35212 (23%)]\tLoss: 0.014668\n",
            "Train Epoch: 52 [10000/35212 (28%)]\tLoss: 0.068751\n",
            "Train Epoch: 52 [12000/35212 (34%)]\tLoss: 0.035880\n",
            "Train Epoch: 52 [14000/35212 (40%)]\tLoss: 0.053712\n",
            "Train Epoch: 52 [16000/35212 (45%)]\tLoss: 0.018064\n",
            "Train Epoch: 52 [18000/35212 (51%)]\tLoss: 0.035554\n",
            "Train Epoch: 52 [20000/35212 (57%)]\tLoss: 0.027547\n",
            "Train Epoch: 52 [22000/35212 (62%)]\tLoss: 0.020823\n",
            "Train Epoch: 52 [24000/35212 (68%)]\tLoss: 0.042392\n",
            "Train Epoch: 52 [26000/35212 (74%)]\tLoss: 0.033476\n",
            "Train Epoch: 52 [28000/35212 (80%)]\tLoss: 0.072953\n",
            "Train Epoch: 52 [30000/35212 (85%)]\tLoss: 0.019013\n",
            "Train Epoch: 52 [32000/35212 (91%)]\tLoss: 0.065428\n",
            "Train Epoch: 52 [34000/35212 (97%)]\tLoss: 0.012693\n",
            "Train Epoch: 53 [0/35212 (0%)]\tLoss: 0.040859\n",
            "Train Epoch: 53 [2000/35212 (6%)]\tLoss: 0.031150\n",
            "Train Epoch: 53 [4000/35212 (11%)]\tLoss: 0.027625\n",
            "Train Epoch: 53 [6000/35212 (17%)]\tLoss: 0.041079\n",
            "Train Epoch: 53 [8000/35212 (23%)]\tLoss: 0.017198\n",
            "Train Epoch: 53 [10000/35212 (28%)]\tLoss: 0.071393\n",
            "Train Epoch: 53 [12000/35212 (34%)]\tLoss: 0.037394\n",
            "Train Epoch: 53 [14000/35212 (40%)]\tLoss: 0.051347\n",
            "Train Epoch: 53 [16000/35212 (45%)]\tLoss: 0.046025\n",
            "Train Epoch: 53 [18000/35212 (51%)]\tLoss: 0.054600\n",
            "Train Epoch: 53 [20000/35212 (57%)]\tLoss: 0.031395\n",
            "Train Epoch: 53 [22000/35212 (62%)]\tLoss: 0.024804\n",
            "Train Epoch: 53 [24000/35212 (68%)]\tLoss: 0.043522\n",
            "Train Epoch: 53 [26000/35212 (74%)]\tLoss: 0.029744\n",
            "Train Epoch: 53 [28000/35212 (80%)]\tLoss: 0.047888\n",
            "Train Epoch: 53 [30000/35212 (85%)]\tLoss: 0.014106\n",
            "Train Epoch: 53 [32000/35212 (91%)]\tLoss: 0.068603\n",
            "Train Epoch: 53 [34000/35212 (97%)]\tLoss: 0.012577\n",
            "Train Epoch: 54 [0/35212 (0%)]\tLoss: 0.038985\n",
            "Train Epoch: 54 [2000/35212 (6%)]\tLoss: 0.030713\n",
            "Train Epoch: 54 [4000/35212 (11%)]\tLoss: 0.017211\n",
            "Train Epoch: 54 [6000/35212 (17%)]\tLoss: 0.033137\n",
            "Train Epoch: 54 [8000/35212 (23%)]\tLoss: 0.016773\n",
            "Train Epoch: 54 [10000/35212 (28%)]\tLoss: 0.061368\n",
            "Train Epoch: 54 [12000/35212 (34%)]\tLoss: 0.027583\n",
            "Train Epoch: 54 [14000/35212 (40%)]\tLoss: 0.044208\n",
            "Train Epoch: 54 [16000/35212 (45%)]\tLoss: 0.019355\n",
            "Train Epoch: 54 [18000/35212 (51%)]\tLoss: 0.030693\n",
            "Train Epoch: 54 [20000/35212 (57%)]\tLoss: 0.016668\n",
            "Train Epoch: 54 [22000/35212 (62%)]\tLoss: 0.028779\n",
            "Train Epoch: 54 [24000/35212 (68%)]\tLoss: 0.064341\n",
            "Train Epoch: 54 [26000/35212 (74%)]\tLoss: 0.023075\n",
            "Train Epoch: 54 [28000/35212 (80%)]\tLoss: 0.061357\n",
            "Train Epoch: 54 [30000/35212 (85%)]\tLoss: 0.015430\n",
            "Train Epoch: 54 [32000/35212 (91%)]\tLoss: 0.056814\n",
            "Train Epoch: 54 [34000/35212 (97%)]\tLoss: 0.020889\n",
            "Train Epoch: 55 [0/35212 (0%)]\tLoss: 0.045224\n",
            "Train Epoch: 55 [2000/35212 (6%)]\tLoss: 0.024948\n",
            "Train Epoch: 55 [4000/35212 (11%)]\tLoss: 0.020795\n",
            "Train Epoch: 55 [6000/35212 (17%)]\tLoss: 0.033429\n",
            "Train Epoch: 55 [8000/35212 (23%)]\tLoss: 0.012376\n",
            "Train Epoch: 55 [10000/35212 (28%)]\tLoss: 0.078371\n",
            "Train Epoch: 55 [12000/35212 (34%)]\tLoss: 0.032323\n",
            "Train Epoch: 55 [14000/35212 (40%)]\tLoss: 0.047702\n",
            "Train Epoch: 55 [16000/35212 (45%)]\tLoss: 0.034213\n",
            "Train Epoch: 55 [18000/35212 (51%)]\tLoss: 0.067158\n",
            "Train Epoch: 55 [20000/35212 (57%)]\tLoss: 0.031670\n",
            "Train Epoch: 55 [22000/35212 (62%)]\tLoss: 0.024104\n",
            "Train Epoch: 55 [24000/35212 (68%)]\tLoss: 0.010805\n",
            "Train Epoch: 55 [26000/35212 (74%)]\tLoss: 0.026069\n",
            "Train Epoch: 55 [28000/35212 (80%)]\tLoss: 0.057547\n",
            "Train Epoch: 55 [30000/35212 (85%)]\tLoss: 0.009122\n",
            "Train Epoch: 55 [32000/35212 (91%)]\tLoss: 0.058324\n",
            "Train Epoch: 55 [34000/35212 (97%)]\tLoss: 0.008947\n",
            "Train Epoch: 56 [0/35212 (0%)]\tLoss: 0.044887\n",
            "Train Epoch: 56 [2000/35212 (6%)]\tLoss: 0.034874\n",
            "Train Epoch: 56 [4000/35212 (11%)]\tLoss: 0.034859\n",
            "Train Epoch: 56 [6000/35212 (17%)]\tLoss: 0.015223\n",
            "Train Epoch: 56 [8000/35212 (23%)]\tLoss: 0.021365\n",
            "Train Epoch: 56 [10000/35212 (28%)]\tLoss: 0.076441\n",
            "Train Epoch: 56 [12000/35212 (34%)]\tLoss: 0.029978\n",
            "Train Epoch: 56 [14000/35212 (40%)]\tLoss: 0.042178\n",
            "Train Epoch: 56 [16000/35212 (45%)]\tLoss: 0.027399\n",
            "Train Epoch: 56 [18000/35212 (51%)]\tLoss: 0.031864\n",
            "Train Epoch: 56 [20000/35212 (57%)]\tLoss: 0.015307\n",
            "Train Epoch: 56 [22000/35212 (62%)]\tLoss: 0.022118\n",
            "Train Epoch: 56 [24000/35212 (68%)]\tLoss: 0.026123\n",
            "Train Epoch: 56 [26000/35212 (74%)]\tLoss: 0.031455\n",
            "Train Epoch: 56 [28000/35212 (80%)]\tLoss: 0.055131\n",
            "Train Epoch: 56 [30000/35212 (85%)]\tLoss: 0.021483\n",
            "Train Epoch: 56 [32000/35212 (91%)]\tLoss: 0.054179\n",
            "Train Epoch: 56 [34000/35212 (97%)]\tLoss: 0.014557\n",
            "Train Epoch: 57 [0/35212 (0%)]\tLoss: 0.043268\n",
            "Train Epoch: 57 [2000/35212 (6%)]\tLoss: 0.023772\n",
            "Train Epoch: 57 [4000/35212 (11%)]\tLoss: 0.015067\n",
            "Train Epoch: 57 [6000/35212 (17%)]\tLoss: 0.054894\n",
            "Train Epoch: 57 [8000/35212 (23%)]\tLoss: 0.014376\n",
            "Train Epoch: 57 [10000/35212 (28%)]\tLoss: 0.055296\n",
            "Train Epoch: 57 [12000/35212 (34%)]\tLoss: 0.030108\n",
            "Train Epoch: 57 [14000/35212 (40%)]\tLoss: 0.045471\n",
            "Train Epoch: 57 [16000/35212 (45%)]\tLoss: 0.011195\n",
            "Train Epoch: 57 [18000/35212 (51%)]\tLoss: 0.050739\n",
            "Train Epoch: 57 [20000/35212 (57%)]\tLoss: 0.022188\n",
            "Train Epoch: 57 [22000/35212 (62%)]\tLoss: 0.020454\n",
            "Train Epoch: 57 [24000/35212 (68%)]\tLoss: 0.052053\n",
            "Train Epoch: 57 [26000/35212 (74%)]\tLoss: 0.033736\n",
            "Train Epoch: 57 [28000/35212 (80%)]\tLoss: 0.054524\n",
            "Train Epoch: 57 [30000/35212 (85%)]\tLoss: 0.008969\n",
            "Train Epoch: 57 [32000/35212 (91%)]\tLoss: 0.060830\n",
            "Train Epoch: 57 [34000/35212 (97%)]\tLoss: 0.012983\n",
            "Train Epoch: 58 [0/35212 (0%)]\tLoss: 0.030300\n",
            "Train Epoch: 58 [2000/35212 (6%)]\tLoss: 0.028066\n",
            "Train Epoch: 58 [4000/35212 (11%)]\tLoss: 0.021633\n",
            "Train Epoch: 58 [6000/35212 (17%)]\tLoss: 0.043874\n",
            "Train Epoch: 58 [8000/35212 (23%)]\tLoss: 0.013311\n",
            "Train Epoch: 58 [10000/35212 (28%)]\tLoss: 0.073332\n",
            "Train Epoch: 58 [12000/35212 (34%)]\tLoss: 0.024181\n",
            "Train Epoch: 58 [14000/35212 (40%)]\tLoss: 0.044037\n",
            "Train Epoch: 58 [16000/35212 (45%)]\tLoss: 0.025135\n",
            "Train Epoch: 58 [18000/35212 (51%)]\tLoss: 0.031766\n",
            "Train Epoch: 58 [20000/35212 (57%)]\tLoss: 0.019321\n",
            "Train Epoch: 58 [22000/35212 (62%)]\tLoss: 0.018804\n",
            "Train Epoch: 58 [24000/35212 (68%)]\tLoss: 0.048801\n",
            "Train Epoch: 58 [26000/35212 (74%)]\tLoss: 0.020219\n",
            "Train Epoch: 58 [28000/35212 (80%)]\tLoss: 0.069049\n",
            "Train Epoch: 58 [30000/35212 (85%)]\tLoss: 0.017003\n",
            "Train Epoch: 58 [32000/35212 (91%)]\tLoss: 0.051811\n",
            "Train Epoch: 58 [34000/35212 (97%)]\tLoss: 0.014734\n",
            "Train Epoch: 59 [0/35212 (0%)]\tLoss: 0.026507\n",
            "Train Epoch: 59 [2000/35212 (6%)]\tLoss: 0.034575\n",
            "Train Epoch: 59 [4000/35212 (11%)]\tLoss: 0.014038\n",
            "Train Epoch: 59 [6000/35212 (17%)]\tLoss: 0.077340\n",
            "Train Epoch: 59 [8000/35212 (23%)]\tLoss: 0.014905\n",
            "Train Epoch: 59 [10000/35212 (28%)]\tLoss: 0.063664\n",
            "Train Epoch: 59 [12000/35212 (34%)]\tLoss: 0.025142\n",
            "Train Epoch: 59 [14000/35212 (40%)]\tLoss: 0.049182\n",
            "Train Epoch: 59 [16000/35212 (45%)]\tLoss: 0.012092\n",
            "Train Epoch: 59 [18000/35212 (51%)]\tLoss: 0.035100\n",
            "Train Epoch: 59 [20000/35212 (57%)]\tLoss: 0.023730\n",
            "Train Epoch: 59 [22000/35212 (62%)]\tLoss: 0.024206\n",
            "Train Epoch: 59 [24000/35212 (68%)]\tLoss: 0.052491\n",
            "Train Epoch: 59 [26000/35212 (74%)]\tLoss: 0.029091\n",
            "Train Epoch: 59 [28000/35212 (80%)]\tLoss: 0.050869\n",
            "Train Epoch: 59 [30000/35212 (85%)]\tLoss: 0.016086\n",
            "Train Epoch: 59 [32000/35212 (91%)]\tLoss: 0.052709\n",
            "Train Epoch: 59 [34000/35212 (97%)]\tLoss: 0.011259\n",
            "Train Epoch: 60 [0/35212 (0%)]\tLoss: 0.033020\n",
            "Train Epoch: 60 [2000/35212 (6%)]\tLoss: 0.029653\n",
            "Train Epoch: 60 [4000/35212 (11%)]\tLoss: 0.012765\n",
            "Train Epoch: 60 [6000/35212 (17%)]\tLoss: 0.026590\n",
            "Train Epoch: 60 [8000/35212 (23%)]\tLoss: 0.011229\n",
            "Train Epoch: 60 [10000/35212 (28%)]\tLoss: 0.068766\n",
            "Train Epoch: 60 [12000/35212 (34%)]\tLoss: 0.021354\n",
            "Train Epoch: 60 [14000/35212 (40%)]\tLoss: 0.042210\n",
            "Train Epoch: 60 [16000/35212 (45%)]\tLoss: 0.017588\n",
            "Train Epoch: 60 [18000/35212 (51%)]\tLoss: 0.040024\n",
            "Train Epoch: 60 [20000/35212 (57%)]\tLoss: 0.031831\n",
            "Train Epoch: 60 [22000/35212 (62%)]\tLoss: 0.012479\n",
            "Train Epoch: 60 [24000/35212 (68%)]\tLoss: 0.073297\n",
            "Train Epoch: 60 [26000/35212 (74%)]\tLoss: 0.018848\n",
            "Train Epoch: 60 [28000/35212 (80%)]\tLoss: 0.058984\n",
            "Train Epoch: 60 [30000/35212 (85%)]\tLoss: 0.012836\n",
            "Train Epoch: 60 [32000/35212 (91%)]\tLoss: 0.043421\n",
            "Train Epoch: 60 [34000/35212 (97%)]\tLoss: 0.009210\n",
            "Train Epoch: 61 [0/35212 (0%)]\tLoss: 0.035044\n",
            "Train Epoch: 61 [2000/35212 (6%)]\tLoss: 0.020295\n",
            "Train Epoch: 61 [4000/35212 (11%)]\tLoss: 0.014933\n",
            "Train Epoch: 61 [6000/35212 (17%)]\tLoss: 0.030289\n",
            "Train Epoch: 61 [8000/35212 (23%)]\tLoss: 0.011864\n",
            "Train Epoch: 61 [10000/35212 (28%)]\tLoss: 0.055561\n",
            "Train Epoch: 61 [12000/35212 (34%)]\tLoss: 0.021026\n",
            "Train Epoch: 61 [14000/35212 (40%)]\tLoss: 0.041930\n",
            "Train Epoch: 61 [16000/35212 (45%)]\tLoss: 0.019400\n",
            "Train Epoch: 61 [18000/35212 (51%)]\tLoss: 0.039757\n",
            "Train Epoch: 61 [20000/35212 (57%)]\tLoss: 0.024562\n",
            "Train Epoch: 61 [22000/35212 (62%)]\tLoss: 0.016518\n",
            "Train Epoch: 61 [24000/35212 (68%)]\tLoss: 0.069376\n",
            "Train Epoch: 61 [26000/35212 (74%)]\tLoss: 0.023781\n",
            "Train Epoch: 61 [28000/35212 (80%)]\tLoss: 0.047694\n",
            "Train Epoch: 61 [30000/35212 (85%)]\tLoss: 0.019023\n",
            "Train Epoch: 61 [32000/35212 (91%)]\tLoss: 0.042308\n",
            "Train Epoch: 61 [34000/35212 (97%)]\tLoss: 0.006437\n",
            "Train Epoch: 62 [0/35212 (0%)]\tLoss: 0.022239\n",
            "Train Epoch: 62 [2000/35212 (6%)]\tLoss: 0.032374\n",
            "Train Epoch: 62 [4000/35212 (11%)]\tLoss: 0.013326\n",
            "Train Epoch: 62 [6000/35212 (17%)]\tLoss: 0.027846\n",
            "Train Epoch: 62 [8000/35212 (23%)]\tLoss: 0.010795\n",
            "Train Epoch: 62 [10000/35212 (28%)]\tLoss: 0.059983\n",
            "Train Epoch: 62 [12000/35212 (34%)]\tLoss: 0.019812\n",
            "Train Epoch: 62 [14000/35212 (40%)]\tLoss: 0.040029\n",
            "Train Epoch: 62 [16000/35212 (45%)]\tLoss: 0.014822\n",
            "Train Epoch: 62 [18000/35212 (51%)]\tLoss: 0.034790\n",
            "Train Epoch: 62 [20000/35212 (57%)]\tLoss: 0.024086\n",
            "Train Epoch: 62 [22000/35212 (62%)]\tLoss: 0.031723\n",
            "Train Epoch: 62 [24000/35212 (68%)]\tLoss: 0.078159\n",
            "Train Epoch: 62 [26000/35212 (74%)]\tLoss: 0.029904\n",
            "Train Epoch: 62 [28000/35212 (80%)]\tLoss: 0.056032\n",
            "Train Epoch: 62 [30000/35212 (85%)]\tLoss: 0.010887\n",
            "Train Epoch: 62 [32000/35212 (91%)]\tLoss: 0.060519\n",
            "Train Epoch: 62 [34000/35212 (97%)]\tLoss: 0.011978\n",
            "Train Epoch: 63 [0/35212 (0%)]\tLoss: 0.028574\n",
            "Train Epoch: 63 [2000/35212 (6%)]\tLoss: 0.021261\n",
            "Train Epoch: 63 [4000/35212 (11%)]\tLoss: 0.016782\n",
            "Train Epoch: 63 [6000/35212 (17%)]\tLoss: 0.025790\n",
            "Train Epoch: 63 [8000/35212 (23%)]\tLoss: 0.009919\n",
            "Train Epoch: 63 [10000/35212 (28%)]\tLoss: 0.051638\n",
            "Train Epoch: 63 [12000/35212 (34%)]\tLoss: 0.024317\n",
            "Train Epoch: 63 [14000/35212 (40%)]\tLoss: 0.037238\n",
            "Train Epoch: 63 [16000/35212 (45%)]\tLoss: 0.018965\n",
            "Train Epoch: 63 [18000/35212 (51%)]\tLoss: 0.022920\n",
            "Train Epoch: 63 [20000/35212 (57%)]\tLoss: 0.028973\n",
            "Train Epoch: 63 [22000/35212 (62%)]\tLoss: 0.020140\n",
            "Train Epoch: 63 [24000/35212 (68%)]\tLoss: 0.059349\n",
            "Train Epoch: 63 [26000/35212 (74%)]\tLoss: 0.021381\n",
            "Train Epoch: 63 [28000/35212 (80%)]\tLoss: 0.054728\n",
            "Train Epoch: 63 [30000/35212 (85%)]\tLoss: 0.019832\n",
            "Train Epoch: 63 [32000/35212 (91%)]\tLoss: 0.036979\n",
            "Train Epoch: 63 [34000/35212 (97%)]\tLoss: 0.009781\n",
            "Train Epoch: 64 [0/35212 (0%)]\tLoss: 0.022013\n",
            "Train Epoch: 64 [2000/35212 (6%)]\tLoss: 0.023621\n",
            "Train Epoch: 64 [4000/35212 (11%)]\tLoss: 0.016959\n",
            "Train Epoch: 64 [6000/35212 (17%)]\tLoss: 0.030081\n",
            "Train Epoch: 64 [8000/35212 (23%)]\tLoss: 0.008499\n",
            "Train Epoch: 64 [10000/35212 (28%)]\tLoss: 0.052670\n",
            "Train Epoch: 64 [12000/35212 (34%)]\tLoss: 0.016583\n",
            "Train Epoch: 64 [14000/35212 (40%)]\tLoss: 0.034027\n",
            "Train Epoch: 64 [16000/35212 (45%)]\tLoss: 0.017496\n",
            "Train Epoch: 64 [18000/35212 (51%)]\tLoss: 0.039546\n",
            "Train Epoch: 64 [20000/35212 (57%)]\tLoss: 0.013216\n",
            "Train Epoch: 64 [22000/35212 (62%)]\tLoss: 0.012482\n",
            "Train Epoch: 64 [24000/35212 (68%)]\tLoss: 0.023792\n",
            "Train Epoch: 64 [26000/35212 (74%)]\tLoss: 0.019054\n",
            "Train Epoch: 64 [28000/35212 (80%)]\tLoss: 0.045761\n",
            "Train Epoch: 64 [30000/35212 (85%)]\tLoss: 0.011446\n",
            "Train Epoch: 64 [32000/35212 (91%)]\tLoss: 0.046981\n",
            "Train Epoch: 64 [34000/35212 (97%)]\tLoss: 0.008464\n",
            "Train Epoch: 65 [0/35212 (0%)]\tLoss: 0.022102\n",
            "Train Epoch: 65 [2000/35212 (6%)]\tLoss: 0.030177\n",
            "Train Epoch: 65 [4000/35212 (11%)]\tLoss: 0.010139\n",
            "Train Epoch: 65 [6000/35212 (17%)]\tLoss: 0.017291\n",
            "Train Epoch: 65 [8000/35212 (23%)]\tLoss: 0.011881\n",
            "Train Epoch: 65 [10000/35212 (28%)]\tLoss: 0.055180\n",
            "Train Epoch: 65 [12000/35212 (34%)]\tLoss: 0.022827\n",
            "Train Epoch: 65 [14000/35212 (40%)]\tLoss: 0.034022\n",
            "Train Epoch: 65 [16000/35212 (45%)]\tLoss: 0.016748\n",
            "Train Epoch: 65 [18000/35212 (51%)]\tLoss: 0.024184\n",
            "Train Epoch: 65 [20000/35212 (57%)]\tLoss: 0.017580\n",
            "Train Epoch: 65 [22000/35212 (62%)]\tLoss: 0.015775\n",
            "Train Epoch: 65 [24000/35212 (68%)]\tLoss: 0.020462\n",
            "Train Epoch: 65 [26000/35212 (74%)]\tLoss: 0.015975\n",
            "Train Epoch: 65 [28000/35212 (80%)]\tLoss: 0.047762\n",
            "Train Epoch: 65 [30000/35212 (85%)]\tLoss: 0.015659\n",
            "Train Epoch: 65 [32000/35212 (91%)]\tLoss: 0.044588\n",
            "Train Epoch: 65 [34000/35212 (97%)]\tLoss: 0.009836\n",
            "Train Epoch: 66 [0/35212 (0%)]\tLoss: 0.021476\n",
            "Train Epoch: 66 [2000/35212 (6%)]\tLoss: 0.016476\n",
            "Train Epoch: 66 [4000/35212 (11%)]\tLoss: 0.011133\n",
            "Train Epoch: 66 [6000/35212 (17%)]\tLoss: 0.014097\n",
            "Train Epoch: 66 [8000/35212 (23%)]\tLoss: 0.010225\n",
            "Train Epoch: 66 [10000/35212 (28%)]\tLoss: 0.055274\n",
            "Train Epoch: 66 [12000/35212 (34%)]\tLoss: 0.013961\n",
            "Train Epoch: 66 [14000/35212 (40%)]\tLoss: 0.034385\n",
            "Train Epoch: 66 [16000/35212 (45%)]\tLoss: 0.027679\n",
            "Train Epoch: 66 [18000/35212 (51%)]\tLoss: 0.023597\n",
            "Train Epoch: 66 [20000/35212 (57%)]\tLoss: 0.023638\n",
            "Train Epoch: 66 [22000/35212 (62%)]\tLoss: 0.007269\n",
            "Train Epoch: 66 [24000/35212 (68%)]\tLoss: 0.011001\n",
            "Train Epoch: 66 [26000/35212 (74%)]\tLoss: 0.022348\n",
            "Train Epoch: 66 [28000/35212 (80%)]\tLoss: 0.045319\n",
            "Train Epoch: 66 [30000/35212 (85%)]\tLoss: 0.012691\n",
            "Train Epoch: 66 [32000/35212 (91%)]\tLoss: 0.043854\n",
            "Train Epoch: 66 [34000/35212 (97%)]\tLoss: 0.007243\n",
            "Train Epoch: 67 [0/35212 (0%)]\tLoss: 0.018507\n",
            "Train Epoch: 67 [2000/35212 (6%)]\tLoss: 0.028651\n",
            "Train Epoch: 67 [4000/35212 (11%)]\tLoss: 0.008943\n",
            "Train Epoch: 67 [6000/35212 (17%)]\tLoss: 0.021527\n",
            "Train Epoch: 67 [8000/35212 (23%)]\tLoss: 0.008662\n",
            "Train Epoch: 67 [10000/35212 (28%)]\tLoss: 0.046795\n",
            "Train Epoch: 67 [12000/35212 (34%)]\tLoss: 0.018432\n",
            "Train Epoch: 67 [14000/35212 (40%)]\tLoss: 0.026447\n",
            "Train Epoch: 67 [16000/35212 (45%)]\tLoss: 0.011422\n",
            "Train Epoch: 67 [18000/35212 (51%)]\tLoss: 0.027345\n",
            "Train Epoch: 67 [20000/35212 (57%)]\tLoss: 0.017047\n",
            "Train Epoch: 67 [22000/35212 (62%)]\tLoss: 0.016320\n",
            "Train Epoch: 67 [24000/35212 (68%)]\tLoss: 0.026976\n",
            "Train Epoch: 67 [26000/35212 (74%)]\tLoss: 0.022620\n",
            "Train Epoch: 67 [28000/35212 (80%)]\tLoss: 0.044092\n",
            "Train Epoch: 67 [30000/35212 (85%)]\tLoss: 0.013828\n",
            "Train Epoch: 67 [32000/35212 (91%)]\tLoss: 0.048376\n",
            "Train Epoch: 67 [34000/35212 (97%)]\tLoss: 0.011815\n",
            "Train Epoch: 68 [0/35212 (0%)]\tLoss: 0.015900\n",
            "Train Epoch: 68 [2000/35212 (6%)]\tLoss: 0.024348\n",
            "Train Epoch: 68 [4000/35212 (11%)]\tLoss: 0.010788\n",
            "Train Epoch: 68 [6000/35212 (17%)]\tLoss: 0.031952\n",
            "Train Epoch: 68 [8000/35212 (23%)]\tLoss: 0.008594\n",
            "Train Epoch: 68 [10000/35212 (28%)]\tLoss: 0.048934\n",
            "Train Epoch: 68 [12000/35212 (34%)]\tLoss: 0.018036\n",
            "Train Epoch: 68 [14000/35212 (40%)]\tLoss: 0.031030\n",
            "Train Epoch: 68 [16000/35212 (45%)]\tLoss: 0.012170\n",
            "Train Epoch: 68 [18000/35212 (51%)]\tLoss: 0.022342\n",
            "Train Epoch: 68 [20000/35212 (57%)]\tLoss: 0.019698\n",
            "Train Epoch: 68 [22000/35212 (62%)]\tLoss: 0.016148\n",
            "Train Epoch: 68 [24000/35212 (68%)]\tLoss: 0.017095\n",
            "Train Epoch: 68 [26000/35212 (74%)]\tLoss: 0.020683\n",
            "Train Epoch: 68 [28000/35212 (80%)]\tLoss: 0.041364\n",
            "Train Epoch: 68 [30000/35212 (85%)]\tLoss: 0.015667\n",
            "Train Epoch: 68 [32000/35212 (91%)]\tLoss: 0.044125\n",
            "Train Epoch: 68 [34000/35212 (97%)]\tLoss: 0.008130\n",
            "Train Epoch: 69 [0/35212 (0%)]\tLoss: 0.018445\n",
            "Train Epoch: 69 [2000/35212 (6%)]\tLoss: 0.014246\n",
            "Train Epoch: 69 [4000/35212 (11%)]\tLoss: 0.011044\n",
            "Train Epoch: 69 [6000/35212 (17%)]\tLoss: 0.018322\n",
            "Train Epoch: 69 [8000/35212 (23%)]\tLoss: 0.010461\n",
            "Train Epoch: 69 [10000/35212 (28%)]\tLoss: 0.047517\n",
            "Train Epoch: 69 [12000/35212 (34%)]\tLoss: 0.014808\n",
            "Train Epoch: 69 [14000/35212 (40%)]\tLoss: 0.031206\n",
            "Train Epoch: 69 [16000/35212 (45%)]\tLoss: 0.019447\n",
            "Train Epoch: 69 [18000/35212 (51%)]\tLoss: 0.026755\n",
            "Train Epoch: 69 [20000/35212 (57%)]\tLoss: 0.016197\n",
            "Train Epoch: 69 [22000/35212 (62%)]\tLoss: 0.014323\n",
            "Train Epoch: 69 [24000/35212 (68%)]\tLoss: 0.009083\n",
            "Train Epoch: 69 [26000/35212 (74%)]\tLoss: 0.017148\n",
            "Train Epoch: 69 [28000/35212 (80%)]\tLoss: 0.041798\n",
            "Train Epoch: 69 [30000/35212 (85%)]\tLoss: 0.021310\n",
            "Train Epoch: 69 [32000/35212 (91%)]\tLoss: 0.039828\n",
            "Train Epoch: 69 [34000/35212 (97%)]\tLoss: 0.011771\n",
            "Train Epoch: 70 [0/35212 (0%)]\tLoss: 0.015077\n",
            "Train Epoch: 70 [2000/35212 (6%)]\tLoss: 0.032058\n",
            "Train Epoch: 70 [4000/35212 (11%)]\tLoss: 0.010936\n",
            "Train Epoch: 70 [6000/35212 (17%)]\tLoss: 0.028282\n",
            "Train Epoch: 70 [8000/35212 (23%)]\tLoss: 0.008562\n",
            "Train Epoch: 70 [10000/35212 (28%)]\tLoss: 0.045120\n",
            "Train Epoch: 70 [12000/35212 (34%)]\tLoss: 0.017555\n",
            "Train Epoch: 70 [14000/35212 (40%)]\tLoss: 0.025574\n",
            "Train Epoch: 70 [16000/35212 (45%)]\tLoss: 0.010543\n",
            "Train Epoch: 70 [18000/35212 (51%)]\tLoss: 0.026567\n",
            "Train Epoch: 70 [20000/35212 (57%)]\tLoss: 0.020501\n",
            "Train Epoch: 70 [22000/35212 (62%)]\tLoss: 0.014145\n",
            "Train Epoch: 70 [24000/35212 (68%)]\tLoss: 0.011098\n",
            "Train Epoch: 70 [26000/35212 (74%)]\tLoss: 0.019822\n",
            "Train Epoch: 70 [28000/35212 (80%)]\tLoss: 0.042797\n",
            "Train Epoch: 70 [30000/35212 (85%)]\tLoss: 0.014148\n",
            "Train Epoch: 70 [32000/35212 (91%)]\tLoss: 0.040174\n",
            "Train Epoch: 70 [34000/35212 (97%)]\tLoss: 0.007422\n",
            "Train Epoch: 71 [0/35212 (0%)]\tLoss: 0.016355\n",
            "Train Epoch: 71 [2000/35212 (6%)]\tLoss: 0.012763\n",
            "Train Epoch: 71 [4000/35212 (11%)]\tLoss: 0.014428\n",
            "Train Epoch: 71 [6000/35212 (17%)]\tLoss: 0.031075\n",
            "Train Epoch: 71 [8000/35212 (23%)]\tLoss: 0.009388\n",
            "Train Epoch: 71 [10000/35212 (28%)]\tLoss: 0.056308\n",
            "Train Epoch: 71 [12000/35212 (34%)]\tLoss: 0.017977\n",
            "Train Epoch: 71 [14000/35212 (40%)]\tLoss: 0.028006\n",
            "Train Epoch: 71 [16000/35212 (45%)]\tLoss: 0.021346\n",
            "Train Epoch: 71 [18000/35212 (51%)]\tLoss: 0.021420\n",
            "Train Epoch: 71 [20000/35212 (57%)]\tLoss: 0.017448\n",
            "Train Epoch: 71 [22000/35212 (62%)]\tLoss: 0.012686\n",
            "Train Epoch: 71 [24000/35212 (68%)]\tLoss: 0.011771\n",
            "Train Epoch: 71 [26000/35212 (74%)]\tLoss: 0.015810\n",
            "Train Epoch: 71 [28000/35212 (80%)]\tLoss: 0.034238\n",
            "Train Epoch: 71 [30000/35212 (85%)]\tLoss: 0.013266\n",
            "Train Epoch: 71 [32000/35212 (91%)]\tLoss: 0.033210\n",
            "Train Epoch: 71 [34000/35212 (97%)]\tLoss: 0.009934\n",
            "Train Epoch: 72 [0/35212 (0%)]\tLoss: 0.013886\n",
            "Train Epoch: 72 [2000/35212 (6%)]\tLoss: 0.019176\n",
            "Train Epoch: 72 [4000/35212 (11%)]\tLoss: 0.009153\n",
            "Train Epoch: 72 [6000/35212 (17%)]\tLoss: 0.034358\n",
            "Train Epoch: 72 [8000/35212 (23%)]\tLoss: 0.005832\n",
            "Train Epoch: 72 [10000/35212 (28%)]\tLoss: 0.039602\n",
            "Train Epoch: 72 [12000/35212 (34%)]\tLoss: 0.016384\n",
            "Train Epoch: 72 [14000/35212 (40%)]\tLoss: 0.028790\n",
            "Train Epoch: 72 [16000/35212 (45%)]\tLoss: 0.009710\n",
            "Train Epoch: 72 [18000/35212 (51%)]\tLoss: 0.033252\n",
            "Train Epoch: 72 [20000/35212 (57%)]\tLoss: 0.012067\n",
            "Train Epoch: 72 [22000/35212 (62%)]\tLoss: 0.021336\n",
            "Train Epoch: 72 [24000/35212 (68%)]\tLoss: 0.015957\n",
            "Train Epoch: 72 [26000/35212 (74%)]\tLoss: 0.019186\n",
            "Train Epoch: 72 [28000/35212 (80%)]\tLoss: 0.038137\n",
            "Train Epoch: 72 [30000/35212 (85%)]\tLoss: 0.017387\n",
            "Train Epoch: 72 [32000/35212 (91%)]\tLoss: 0.040763\n",
            "Train Epoch: 72 [34000/35212 (97%)]\tLoss: 0.008727\n",
            "Train Epoch: 73 [0/35212 (0%)]\tLoss: 0.016377\n",
            "Train Epoch: 73 [2000/35212 (6%)]\tLoss: 0.018828\n",
            "Train Epoch: 73 [4000/35212 (11%)]\tLoss: 0.008990\n",
            "Train Epoch: 73 [6000/35212 (17%)]\tLoss: 0.032612\n",
            "Train Epoch: 73 [8000/35212 (23%)]\tLoss: 0.010630\n",
            "Train Epoch: 73 [10000/35212 (28%)]\tLoss: 0.049831\n",
            "Train Epoch: 73 [12000/35212 (34%)]\tLoss: 0.015220\n",
            "Train Epoch: 73 [14000/35212 (40%)]\tLoss: 0.022695\n",
            "Train Epoch: 73 [16000/35212 (45%)]\tLoss: 0.009829\n",
            "Train Epoch: 73 [18000/35212 (51%)]\tLoss: 0.020854\n",
            "Train Epoch: 73 [20000/35212 (57%)]\tLoss: 0.014838\n",
            "Train Epoch: 73 [22000/35212 (62%)]\tLoss: 0.025288\n",
            "Train Epoch: 73 [24000/35212 (68%)]\tLoss: 0.010738\n",
            "Train Epoch: 73 [26000/35212 (74%)]\tLoss: 0.020983\n",
            "Train Epoch: 73 [28000/35212 (80%)]\tLoss: 0.039738\n",
            "Train Epoch: 73 [30000/35212 (85%)]\tLoss: 0.032395\n",
            "Train Epoch: 73 [32000/35212 (91%)]\tLoss: 0.034834\n",
            "Train Epoch: 73 [34000/35212 (97%)]\tLoss: 0.010282\n",
            "Train Epoch: 74 [0/35212 (0%)]\tLoss: 0.016921\n",
            "Train Epoch: 74 [2000/35212 (6%)]\tLoss: 0.012427\n",
            "Train Epoch: 74 [4000/35212 (11%)]\tLoss: 0.010918\n",
            "Train Epoch: 74 [6000/35212 (17%)]\tLoss: 0.022191\n",
            "Train Epoch: 74 [8000/35212 (23%)]\tLoss: 0.006717\n",
            "Train Epoch: 74 [10000/35212 (28%)]\tLoss: 0.053214\n",
            "Train Epoch: 74 [12000/35212 (34%)]\tLoss: 0.019817\n",
            "Train Epoch: 74 [14000/35212 (40%)]\tLoss: 0.017511\n",
            "Train Epoch: 74 [16000/35212 (45%)]\tLoss: 0.015010\n",
            "Train Epoch: 74 [18000/35212 (51%)]\tLoss: 0.025469\n",
            "Train Epoch: 74 [20000/35212 (57%)]\tLoss: 0.016151\n",
            "Train Epoch: 74 [22000/35212 (62%)]\tLoss: 0.012657\n",
            "Train Epoch: 74 [24000/35212 (68%)]\tLoss: 0.012823\n",
            "Train Epoch: 74 [26000/35212 (74%)]\tLoss: 0.022823\n",
            "Train Epoch: 74 [28000/35212 (80%)]\tLoss: 0.042036\n",
            "Train Epoch: 74 [30000/35212 (85%)]\tLoss: 0.027636\n",
            "Train Epoch: 74 [32000/35212 (91%)]\tLoss: 0.033920\n",
            "Train Epoch: 74 [34000/35212 (97%)]\tLoss: 0.009186\n",
            "Train Epoch: 75 [0/35212 (0%)]\tLoss: 0.017910\n",
            "Train Epoch: 75 [2000/35212 (6%)]\tLoss: 0.014800\n",
            "Train Epoch: 75 [4000/35212 (11%)]\tLoss: 0.006287\n",
            "Train Epoch: 75 [6000/35212 (17%)]\tLoss: 0.013666\n",
            "Train Epoch: 75 [8000/35212 (23%)]\tLoss: 0.007131\n",
            "Train Epoch: 75 [10000/35212 (28%)]\tLoss: 0.043437\n",
            "Train Epoch: 75 [12000/35212 (34%)]\tLoss: 0.013890\n",
            "Train Epoch: 75 [14000/35212 (40%)]\tLoss: 0.017114\n",
            "Train Epoch: 75 [16000/35212 (45%)]\tLoss: 0.010228\n",
            "Train Epoch: 75 [18000/35212 (51%)]\tLoss: 0.019970\n",
            "Train Epoch: 75 [20000/35212 (57%)]\tLoss: 0.014375\n",
            "Train Epoch: 75 [22000/35212 (62%)]\tLoss: 0.028976\n",
            "Train Epoch: 75 [24000/35212 (68%)]\tLoss: 0.027404\n",
            "Train Epoch: 75 [26000/35212 (74%)]\tLoss: 0.011669\n",
            "Train Epoch: 75 [28000/35212 (80%)]\tLoss: 0.038104\n",
            "Train Epoch: 75 [30000/35212 (85%)]\tLoss: 0.015555\n",
            "Train Epoch: 75 [32000/35212 (91%)]\tLoss: 0.039761\n",
            "Train Epoch: 75 [34000/35212 (97%)]\tLoss: 0.008930\n",
            "Train Epoch: 76 [0/35212 (0%)]\tLoss: 0.017960\n",
            "Train Epoch: 76 [2000/35212 (6%)]\tLoss: 0.015117\n",
            "Train Epoch: 76 [4000/35212 (11%)]\tLoss: 0.013892\n",
            "Train Epoch: 76 [6000/35212 (17%)]\tLoss: 0.013216\n",
            "Train Epoch: 76 [8000/35212 (23%)]\tLoss: 0.008704\n",
            "Train Epoch: 76 [10000/35212 (28%)]\tLoss: 0.036998\n",
            "Train Epoch: 76 [12000/35212 (34%)]\tLoss: 0.018071\n",
            "Train Epoch: 76 [14000/35212 (40%)]\tLoss: 0.024346\n",
            "Train Epoch: 76 [16000/35212 (45%)]\tLoss: 0.010484\n",
            "Train Epoch: 76 [18000/35212 (51%)]\tLoss: 0.026835\n",
            "Train Epoch: 76 [20000/35212 (57%)]\tLoss: 0.012462\n",
            "Train Epoch: 76 [22000/35212 (62%)]\tLoss: 0.032836\n",
            "Train Epoch: 76 [24000/35212 (68%)]\tLoss: 0.016990\n",
            "Train Epoch: 76 [26000/35212 (74%)]\tLoss: 0.017976\n",
            "Train Epoch: 76 [28000/35212 (80%)]\tLoss: 0.038189\n",
            "Train Epoch: 76 [30000/35212 (85%)]\tLoss: 0.015217\n",
            "Train Epoch: 76 [32000/35212 (91%)]\tLoss: 0.031548\n",
            "Train Epoch: 76 [34000/35212 (97%)]\tLoss: 0.010495\n",
            "Train Epoch: 77 [0/35212 (0%)]\tLoss: 0.016435\n",
            "Train Epoch: 77 [2000/35212 (6%)]\tLoss: 0.019423\n",
            "Train Epoch: 77 [4000/35212 (11%)]\tLoss: 0.006722\n",
            "Train Epoch: 77 [6000/35212 (17%)]\tLoss: 0.018140\n",
            "Train Epoch: 77 [8000/35212 (23%)]\tLoss: 0.008780\n",
            "Train Epoch: 77 [10000/35212 (28%)]\tLoss: 0.044097\n",
            "Train Epoch: 77 [12000/35212 (34%)]\tLoss: 0.012493\n",
            "Train Epoch: 77 [14000/35212 (40%)]\tLoss: 0.029130\n",
            "Train Epoch: 77 [16000/35212 (45%)]\tLoss: 0.025091\n",
            "Train Epoch: 77 [18000/35212 (51%)]\tLoss: 0.021077\n",
            "Train Epoch: 77 [20000/35212 (57%)]\tLoss: 0.013920\n",
            "Train Epoch: 77 [22000/35212 (62%)]\tLoss: 0.032093\n",
            "Train Epoch: 77 [24000/35212 (68%)]\tLoss: 0.014452\n",
            "Train Epoch: 77 [26000/35212 (74%)]\tLoss: 0.017710\n",
            "Train Epoch: 77 [28000/35212 (80%)]\tLoss: 0.048231\n",
            "Train Epoch: 77 [30000/35212 (85%)]\tLoss: 0.017610\n",
            "Train Epoch: 77 [32000/35212 (91%)]\tLoss: 0.028874\n",
            "Train Epoch: 77 [34000/35212 (97%)]\tLoss: 0.007707\n",
            "Train Epoch: 78 [0/35212 (0%)]\tLoss: 0.011116\n",
            "Train Epoch: 78 [2000/35212 (6%)]\tLoss: 0.014410\n",
            "Train Epoch: 78 [4000/35212 (11%)]\tLoss: 0.008192\n",
            "Train Epoch: 78 [6000/35212 (17%)]\tLoss: 0.019641\n",
            "Train Epoch: 78 [8000/35212 (23%)]\tLoss: 0.007360\n",
            "Train Epoch: 78 [10000/35212 (28%)]\tLoss: 0.036061\n",
            "Train Epoch: 78 [12000/35212 (34%)]\tLoss: 0.021720\n",
            "Train Epoch: 78 [14000/35212 (40%)]\tLoss: 0.016146\n",
            "Train Epoch: 78 [16000/35212 (45%)]\tLoss: 0.007552\n",
            "Train Epoch: 78 [18000/35212 (51%)]\tLoss: 0.024200\n",
            "Train Epoch: 78 [20000/35212 (57%)]\tLoss: 0.013135\n",
            "Train Epoch: 78 [22000/35212 (62%)]\tLoss: 0.021881\n",
            "Train Epoch: 78 [24000/35212 (68%)]\tLoss: 0.030066\n",
            "Train Epoch: 78 [26000/35212 (74%)]\tLoss: 0.017132\n",
            "Train Epoch: 78 [28000/35212 (80%)]\tLoss: 0.044129\n",
            "Train Epoch: 78 [30000/35212 (85%)]\tLoss: 0.016282\n",
            "Train Epoch: 78 [32000/35212 (91%)]\tLoss: 0.033215\n",
            "Train Epoch: 78 [34000/35212 (97%)]\tLoss: 0.009494\n",
            "Train Epoch: 79 [0/35212 (0%)]\tLoss: 0.012424\n",
            "Train Epoch: 79 [2000/35212 (6%)]\tLoss: 0.028619\n",
            "Train Epoch: 79 [4000/35212 (11%)]\tLoss: 0.011122\n",
            "Train Epoch: 79 [6000/35212 (17%)]\tLoss: 0.013687\n",
            "Train Epoch: 79 [8000/35212 (23%)]\tLoss: 0.006491\n",
            "Train Epoch: 79 [10000/35212 (28%)]\tLoss: 0.041917\n",
            "Train Epoch: 79 [12000/35212 (34%)]\tLoss: 0.011482\n",
            "Train Epoch: 79 [14000/35212 (40%)]\tLoss: 0.018878\n",
            "Train Epoch: 79 [16000/35212 (45%)]\tLoss: 0.018742\n",
            "Train Epoch: 79 [18000/35212 (51%)]\tLoss: 0.020303\n",
            "Train Epoch: 79 [20000/35212 (57%)]\tLoss: 0.011935\n",
            "Train Epoch: 79 [22000/35212 (62%)]\tLoss: 0.019534\n",
            "Train Epoch: 79 [24000/35212 (68%)]\tLoss: 0.015614\n",
            "Train Epoch: 79 [26000/35212 (74%)]\tLoss: 0.016519\n",
            "Train Epoch: 79 [28000/35212 (80%)]\tLoss: 0.032589\n",
            "Train Epoch: 79 [30000/35212 (85%)]\tLoss: 0.013034\n",
            "Train Epoch: 79 [32000/35212 (91%)]\tLoss: 0.039088\n",
            "Train Epoch: 79 [34000/35212 (97%)]\tLoss: 0.007618\n",
            "Train Epoch: 80 [0/35212 (0%)]\tLoss: 0.012968\n",
            "Train Epoch: 80 [2000/35212 (6%)]\tLoss: 0.012007\n",
            "Train Epoch: 80 [4000/35212 (11%)]\tLoss: 0.008460\n",
            "Train Epoch: 80 [6000/35212 (17%)]\tLoss: 0.014870\n",
            "Train Epoch: 80 [8000/35212 (23%)]\tLoss: 0.006191\n",
            "Train Epoch: 80 [10000/35212 (28%)]\tLoss: 0.027892\n",
            "Train Epoch: 80 [12000/35212 (34%)]\tLoss: 0.016094\n",
            "Train Epoch: 80 [14000/35212 (40%)]\tLoss: 0.011360\n",
            "Train Epoch: 80 [16000/35212 (45%)]\tLoss: 0.013352\n",
            "Train Epoch: 80 [18000/35212 (51%)]\tLoss: 0.025212\n",
            "Train Epoch: 80 [20000/35212 (57%)]\tLoss: 0.012819\n",
            "Train Epoch: 80 [22000/35212 (62%)]\tLoss: 0.019861\n",
            "Train Epoch: 80 [24000/35212 (68%)]\tLoss: 0.010299\n",
            "Train Epoch: 80 [26000/35212 (74%)]\tLoss: 0.017770\n",
            "Train Epoch: 80 [28000/35212 (80%)]\tLoss: 0.037156\n",
            "Train Epoch: 80 [30000/35212 (85%)]\tLoss: 0.016531\n",
            "Train Epoch: 80 [32000/35212 (91%)]\tLoss: 0.029370\n",
            "Train Epoch: 80 [34000/35212 (97%)]\tLoss: 0.008624\n",
            "Train Epoch: 81 [0/35212 (0%)]\tLoss: 0.010561\n",
            "Train Epoch: 81 [2000/35212 (6%)]\tLoss: 0.017512\n",
            "Train Epoch: 81 [4000/35212 (11%)]\tLoss: 0.010443\n",
            "Train Epoch: 81 [6000/35212 (17%)]\tLoss: 0.017225\n",
            "Train Epoch: 81 [8000/35212 (23%)]\tLoss: 0.011693\n",
            "Train Epoch: 81 [10000/35212 (28%)]\tLoss: 0.048052\n",
            "Train Epoch: 81 [12000/35212 (34%)]\tLoss: 0.012367\n",
            "Train Epoch: 81 [14000/35212 (40%)]\tLoss: 0.018353\n",
            "Train Epoch: 81 [16000/35212 (45%)]\tLoss: 0.014907\n",
            "Train Epoch: 81 [18000/35212 (51%)]\tLoss: 0.022052\n",
            "Train Epoch: 81 [20000/35212 (57%)]\tLoss: 0.017973\n",
            "Train Epoch: 81 [22000/35212 (62%)]\tLoss: 0.037601\n",
            "Train Epoch: 81 [24000/35212 (68%)]\tLoss: 0.014513\n",
            "Train Epoch: 81 [26000/35212 (74%)]\tLoss: 0.021155\n",
            "Train Epoch: 81 [28000/35212 (80%)]\tLoss: 0.046583\n",
            "Train Epoch: 81 [30000/35212 (85%)]\tLoss: 0.013028\n",
            "Train Epoch: 81 [32000/35212 (91%)]\tLoss: 0.038577\n",
            "Train Epoch: 81 [34000/35212 (97%)]\tLoss: 0.009580\n",
            "Train Epoch: 82 [0/35212 (0%)]\tLoss: 0.012707\n",
            "Train Epoch: 82 [2000/35212 (6%)]\tLoss: 0.022022\n",
            "Train Epoch: 82 [4000/35212 (11%)]\tLoss: 0.008987\n",
            "Train Epoch: 82 [6000/35212 (17%)]\tLoss: 0.017607\n",
            "Train Epoch: 82 [8000/35212 (23%)]\tLoss: 0.010098\n",
            "Train Epoch: 82 [10000/35212 (28%)]\tLoss: 0.036051\n",
            "Train Epoch: 82 [12000/35212 (34%)]\tLoss: 0.020491\n",
            "Train Epoch: 82 [14000/35212 (40%)]\tLoss: 0.013484\n",
            "Train Epoch: 82 [16000/35212 (45%)]\tLoss: 0.017792\n",
            "Train Epoch: 82 [18000/35212 (51%)]\tLoss: 0.042688\n",
            "Train Epoch: 82 [20000/35212 (57%)]\tLoss: 0.010997\n",
            "Train Epoch: 82 [22000/35212 (62%)]\tLoss: 0.023869\n",
            "Train Epoch: 82 [24000/35212 (68%)]\tLoss: 0.021897\n",
            "Train Epoch: 82 [26000/35212 (74%)]\tLoss: 0.019318\n",
            "Train Epoch: 82 [28000/35212 (80%)]\tLoss: 0.037480\n",
            "Train Epoch: 82 [30000/35212 (85%)]\tLoss: 0.011267\n",
            "Train Epoch: 82 [32000/35212 (91%)]\tLoss: 0.028132\n",
            "Train Epoch: 82 [34000/35212 (97%)]\tLoss: 0.008943\n",
            "Train Epoch: 83 [0/35212 (0%)]\tLoss: 0.009226\n",
            "Train Epoch: 83 [2000/35212 (6%)]\tLoss: 0.021764\n",
            "Train Epoch: 83 [4000/35212 (11%)]\tLoss: 0.006775\n",
            "Train Epoch: 83 [6000/35212 (17%)]\tLoss: 0.036146\n",
            "Train Epoch: 83 [8000/35212 (23%)]\tLoss: 0.010493\n",
            "Train Epoch: 83 [10000/35212 (28%)]\tLoss: 0.041191\n",
            "Train Epoch: 83 [12000/35212 (34%)]\tLoss: 0.013097\n",
            "Train Epoch: 83 [14000/35212 (40%)]\tLoss: 0.014772\n",
            "Train Epoch: 83 [16000/35212 (45%)]\tLoss: 0.017702\n",
            "Train Epoch: 83 [18000/35212 (51%)]\tLoss: 0.036134\n",
            "Train Epoch: 83 [20000/35212 (57%)]\tLoss: 0.017794\n",
            "Train Epoch: 83 [22000/35212 (62%)]\tLoss: 0.018502\n",
            "Train Epoch: 83 [24000/35212 (68%)]\tLoss: 0.013871\n",
            "Train Epoch: 83 [26000/35212 (74%)]\tLoss: 0.012463\n",
            "Train Epoch: 83 [28000/35212 (80%)]\tLoss: 0.042755\n",
            "Train Epoch: 83 [30000/35212 (85%)]\tLoss: 0.013386\n",
            "Train Epoch: 83 [32000/35212 (91%)]\tLoss: 0.027107\n",
            "Train Epoch: 83 [34000/35212 (97%)]\tLoss: 0.004008\n",
            "Train Epoch: 84 [0/35212 (0%)]\tLoss: 0.010858\n",
            "Train Epoch: 84 [2000/35212 (6%)]\tLoss: 0.012813\n",
            "Train Epoch: 84 [4000/35212 (11%)]\tLoss: 0.010807\n",
            "Train Epoch: 84 [6000/35212 (17%)]\tLoss: 0.034976\n",
            "Train Epoch: 84 [8000/35212 (23%)]\tLoss: 0.008079\n",
            "Train Epoch: 84 [10000/35212 (28%)]\tLoss: 0.046923\n",
            "Train Epoch: 84 [12000/35212 (34%)]\tLoss: 0.015468\n",
            "Train Epoch: 84 [14000/35212 (40%)]\tLoss: 0.018153\n",
            "Train Epoch: 84 [16000/35212 (45%)]\tLoss: 0.007504\n",
            "Train Epoch: 84 [18000/35212 (51%)]\tLoss: 0.015352\n",
            "Train Epoch: 84 [20000/35212 (57%)]\tLoss: 0.015148\n",
            "Train Epoch: 84 [22000/35212 (62%)]\tLoss: 0.031959\n",
            "Train Epoch: 84 [24000/35212 (68%)]\tLoss: 0.013229\n",
            "Train Epoch: 84 [26000/35212 (74%)]\tLoss: 0.027065\n",
            "Train Epoch: 84 [28000/35212 (80%)]\tLoss: 0.037869\n",
            "Train Epoch: 84 [30000/35212 (85%)]\tLoss: 0.027214\n",
            "Train Epoch: 84 [32000/35212 (91%)]\tLoss: 0.035973\n",
            "Train Epoch: 84 [34000/35212 (97%)]\tLoss: 0.015256\n",
            "Train Epoch: 85 [0/35212 (0%)]\tLoss: 0.011224\n",
            "Train Epoch: 85 [2000/35212 (6%)]\tLoss: 0.015904\n",
            "Train Epoch: 85 [4000/35212 (11%)]\tLoss: 0.006955\n",
            "Train Epoch: 85 [6000/35212 (17%)]\tLoss: 0.019525\n",
            "Train Epoch: 85 [8000/35212 (23%)]\tLoss: 0.007552\n",
            "Train Epoch: 85 [10000/35212 (28%)]\tLoss: 0.036362\n",
            "Train Epoch: 85 [12000/35212 (34%)]\tLoss: 0.015772\n",
            "Train Epoch: 85 [14000/35212 (40%)]\tLoss: 0.021054\n",
            "Train Epoch: 85 [16000/35212 (45%)]\tLoss: 0.032208\n",
            "Train Epoch: 85 [18000/35212 (51%)]\tLoss: 0.024083\n",
            "Train Epoch: 85 [20000/35212 (57%)]\tLoss: 0.019116\n",
            "Train Epoch: 85 [22000/35212 (62%)]\tLoss: 0.039552\n",
            "Train Epoch: 85 [24000/35212 (68%)]\tLoss: 0.016275\n",
            "Train Epoch: 85 [26000/35212 (74%)]\tLoss: 0.038083\n",
            "Train Epoch: 85 [28000/35212 (80%)]\tLoss: 0.037804\n",
            "Train Epoch: 85 [30000/35212 (85%)]\tLoss: 0.018592\n",
            "Train Epoch: 85 [32000/35212 (91%)]\tLoss: 0.030091\n",
            "Train Epoch: 85 [34000/35212 (97%)]\tLoss: 0.009172\n",
            "Train Epoch: 86 [0/35212 (0%)]\tLoss: 0.015588\n",
            "Train Epoch: 86 [2000/35212 (6%)]\tLoss: 0.016989\n",
            "Train Epoch: 86 [4000/35212 (11%)]\tLoss: 0.012710\n",
            "Train Epoch: 86 [6000/35212 (17%)]\tLoss: 0.007217\n",
            "Train Epoch: 86 [8000/35212 (23%)]\tLoss: 0.005239\n",
            "Train Epoch: 86 [10000/35212 (28%)]\tLoss: 0.041451\n",
            "Train Epoch: 86 [12000/35212 (34%)]\tLoss: 0.018339\n",
            "Train Epoch: 86 [14000/35212 (40%)]\tLoss: 0.013484\n",
            "Train Epoch: 86 [16000/35212 (45%)]\tLoss: 0.008355\n",
            "Train Epoch: 86 [18000/35212 (51%)]\tLoss: 0.014318\n",
            "Train Epoch: 86 [20000/35212 (57%)]\tLoss: 0.019649\n",
            "Train Epoch: 86 [22000/35212 (62%)]\tLoss: 0.016570\n",
            "Train Epoch: 86 [24000/35212 (68%)]\tLoss: 0.019114\n",
            "Train Epoch: 86 [26000/35212 (74%)]\tLoss: 0.036741\n",
            "Train Epoch: 86 [28000/35212 (80%)]\tLoss: 0.037645\n",
            "Train Epoch: 86 [30000/35212 (85%)]\tLoss: 0.013814\n",
            "Train Epoch: 86 [32000/35212 (91%)]\tLoss: 0.038749\n",
            "Train Epoch: 86 [34000/35212 (97%)]\tLoss: 0.007575\n",
            "Train Epoch: 87 [0/35212 (0%)]\tLoss: 0.013102\n",
            "Train Epoch: 87 [2000/35212 (6%)]\tLoss: 0.023620\n",
            "Train Epoch: 87 [4000/35212 (11%)]\tLoss: 0.008681\n",
            "Train Epoch: 87 [6000/35212 (17%)]\tLoss: 0.018995\n",
            "Train Epoch: 87 [8000/35212 (23%)]\tLoss: 0.010290\n",
            "Train Epoch: 87 [10000/35212 (28%)]\tLoss: 0.030782\n",
            "Train Epoch: 87 [12000/35212 (34%)]\tLoss: 0.013476\n",
            "Train Epoch: 87 [14000/35212 (40%)]\tLoss: 0.012449\n",
            "Train Epoch: 87 [16000/35212 (45%)]\tLoss: 0.015843\n",
            "Train Epoch: 87 [18000/35212 (51%)]\tLoss: 0.023152\n",
            "Train Epoch: 87 [20000/35212 (57%)]\tLoss: 0.012574\n",
            "Train Epoch: 87 [22000/35212 (62%)]\tLoss: 0.031942\n",
            "Train Epoch: 87 [24000/35212 (68%)]\tLoss: 0.051043\n",
            "Train Epoch: 87 [26000/35212 (74%)]\tLoss: 0.019228\n",
            "Train Epoch: 87 [28000/35212 (80%)]\tLoss: 0.028566\n",
            "Train Epoch: 87 [30000/35212 (85%)]\tLoss: 0.012405\n",
            "Train Epoch: 87 [32000/35212 (91%)]\tLoss: 0.025956\n",
            "Train Epoch: 87 [34000/35212 (97%)]\tLoss: 0.012591\n",
            "Train Epoch: 88 [0/35212 (0%)]\tLoss: 0.015118\n",
            "Train Epoch: 88 [2000/35212 (6%)]\tLoss: 0.011039\n",
            "Train Epoch: 88 [4000/35212 (11%)]\tLoss: 0.010555\n",
            "Train Epoch: 88 [6000/35212 (17%)]\tLoss: 0.007423\n",
            "Train Epoch: 88 [8000/35212 (23%)]\tLoss: 0.006408\n",
            "Train Epoch: 88 [10000/35212 (28%)]\tLoss: 0.036943\n",
            "Train Epoch: 88 [12000/35212 (34%)]\tLoss: 0.012534\n",
            "Train Epoch: 88 [14000/35212 (40%)]\tLoss: 0.022273\n",
            "Train Epoch: 88 [16000/35212 (45%)]\tLoss: 0.020221\n",
            "Train Epoch: 88 [18000/35212 (51%)]\tLoss: 0.029604\n",
            "Train Epoch: 88 [20000/35212 (57%)]\tLoss: 0.012559\n",
            "Train Epoch: 88 [22000/35212 (62%)]\tLoss: 0.017964\n",
            "Train Epoch: 88 [24000/35212 (68%)]\tLoss: 0.009860\n",
            "Train Epoch: 88 [26000/35212 (74%)]\tLoss: 0.019727\n",
            "Train Epoch: 88 [28000/35212 (80%)]\tLoss: 0.037986\n",
            "Train Epoch: 88 [30000/35212 (85%)]\tLoss: 0.015553\n",
            "Train Epoch: 88 [32000/35212 (91%)]\tLoss: 0.028347\n",
            "Train Epoch: 88 [34000/35212 (97%)]\tLoss: 0.009765\n",
            "Train Epoch: 89 [0/35212 (0%)]\tLoss: 0.014287\n",
            "Train Epoch: 89 [2000/35212 (6%)]\tLoss: 0.013866\n",
            "Train Epoch: 89 [4000/35212 (11%)]\tLoss: 0.011424\n",
            "Train Epoch: 89 [6000/35212 (17%)]\tLoss: 0.015515\n",
            "Train Epoch: 89 [8000/35212 (23%)]\tLoss: 0.011714\n",
            "Train Epoch: 89 [10000/35212 (28%)]\tLoss: 0.052666\n",
            "Train Epoch: 89 [12000/35212 (34%)]\tLoss: 0.019867\n",
            "Train Epoch: 89 [14000/35212 (40%)]\tLoss: 0.022883\n",
            "Train Epoch: 89 [16000/35212 (45%)]\tLoss: 0.011600\n",
            "Train Epoch: 89 [18000/35212 (51%)]\tLoss: 0.020779\n",
            "Train Epoch: 89 [20000/35212 (57%)]\tLoss: 0.024644\n",
            "Train Epoch: 89 [22000/35212 (62%)]\tLoss: 0.037499\n",
            "Train Epoch: 89 [24000/35212 (68%)]\tLoss: 0.028440\n",
            "Train Epoch: 89 [26000/35212 (74%)]\tLoss: 0.048517\n",
            "Train Epoch: 89 [28000/35212 (80%)]\tLoss: 0.034727\n",
            "Train Epoch: 89 [30000/35212 (85%)]\tLoss: 0.012496\n",
            "Train Epoch: 89 [32000/35212 (91%)]\tLoss: 0.053110\n",
            "Train Epoch: 89 [34000/35212 (97%)]\tLoss: 0.006229\n",
            "Train Epoch: 90 [0/35212 (0%)]\tLoss: 0.014245\n",
            "Train Epoch: 90 [2000/35212 (6%)]\tLoss: 0.022046\n",
            "Train Epoch: 90 [4000/35212 (11%)]\tLoss: 0.011083\n",
            "Train Epoch: 90 [6000/35212 (17%)]\tLoss: 0.019135\n",
            "Train Epoch: 90 [8000/35212 (23%)]\tLoss: 0.006173\n",
            "Train Epoch: 90 [10000/35212 (28%)]\tLoss: 0.038637\n",
            "Train Epoch: 90 [12000/35212 (34%)]\tLoss: 0.015833\n",
            "Train Epoch: 90 [14000/35212 (40%)]\tLoss: 0.016610\n",
            "Train Epoch: 90 [16000/35212 (45%)]\tLoss: 0.005684\n",
            "Train Epoch: 90 [18000/35212 (51%)]\tLoss: 0.018258\n",
            "Train Epoch: 90 [20000/35212 (57%)]\tLoss: 0.011228\n",
            "Train Epoch: 90 [22000/35212 (62%)]\tLoss: 0.015376\n",
            "Train Epoch: 90 [24000/35212 (68%)]\tLoss: 0.020722\n",
            "Train Epoch: 90 [26000/35212 (74%)]\tLoss: 0.029292\n",
            "Train Epoch: 90 [28000/35212 (80%)]\tLoss: 0.047506\n",
            "Train Epoch: 90 [30000/35212 (85%)]\tLoss: 0.016275\n",
            "Train Epoch: 90 [32000/35212 (91%)]\tLoss: 0.043655\n",
            "Train Epoch: 90 [34000/35212 (97%)]\tLoss: 0.012323\n",
            "Train Epoch: 91 [0/35212 (0%)]\tLoss: 0.021700\n",
            "Train Epoch: 91 [2000/35212 (6%)]\tLoss: 0.016099\n",
            "Train Epoch: 91 [4000/35212 (11%)]\tLoss: 0.008563\n",
            "Train Epoch: 91 [6000/35212 (17%)]\tLoss: 0.026345\n",
            "Train Epoch: 91 [8000/35212 (23%)]\tLoss: 0.009335\n",
            "Train Epoch: 91 [10000/35212 (28%)]\tLoss: 0.036584\n",
            "Train Epoch: 91 [12000/35212 (34%)]\tLoss: 0.012893\n",
            "Train Epoch: 91 [14000/35212 (40%)]\tLoss: 0.011104\n",
            "Train Epoch: 91 [16000/35212 (45%)]\tLoss: 0.014223\n",
            "Train Epoch: 91 [18000/35212 (51%)]\tLoss: 0.035737\n",
            "Train Epoch: 91 [20000/35212 (57%)]\tLoss: 0.011298\n",
            "Train Epoch: 91 [22000/35212 (62%)]\tLoss: 0.013032\n",
            "Train Epoch: 91 [24000/35212 (68%)]\tLoss: 0.013519\n",
            "Train Epoch: 91 [26000/35212 (74%)]\tLoss: 0.021486\n",
            "Train Epoch: 91 [28000/35212 (80%)]\tLoss: 0.041891\n",
            "Train Epoch: 91 [30000/35212 (85%)]\tLoss: 0.033285\n",
            "Train Epoch: 91 [32000/35212 (91%)]\tLoss: 0.023603\n",
            "Train Epoch: 91 [34000/35212 (97%)]\tLoss: 0.008512\n",
            "Train Epoch: 92 [0/35212 (0%)]\tLoss: 0.016786\n",
            "Train Epoch: 92 [2000/35212 (6%)]\tLoss: 0.013724\n",
            "Train Epoch: 92 [4000/35212 (11%)]\tLoss: 0.010110\n",
            "Train Epoch: 92 [6000/35212 (17%)]\tLoss: 0.007577\n",
            "Train Epoch: 92 [8000/35212 (23%)]\tLoss: 0.009807\n",
            "Train Epoch: 92 [10000/35212 (28%)]\tLoss: 0.063124\n",
            "Train Epoch: 92 [12000/35212 (34%)]\tLoss: 0.012521\n",
            "Train Epoch: 92 [14000/35212 (40%)]\tLoss: 0.010914\n",
            "Train Epoch: 92 [16000/35212 (45%)]\tLoss: 0.011140\n",
            "Train Epoch: 92 [18000/35212 (51%)]\tLoss: 0.018098\n",
            "Train Epoch: 92 [20000/35212 (57%)]\tLoss: 0.015400\n",
            "Train Epoch: 92 [22000/35212 (62%)]\tLoss: 0.016054\n",
            "Train Epoch: 92 [24000/35212 (68%)]\tLoss: 0.048298\n",
            "Train Epoch: 92 [26000/35212 (74%)]\tLoss: 0.011610\n",
            "Train Epoch: 92 [28000/35212 (80%)]\tLoss: 0.035006\n",
            "Train Epoch: 92 [30000/35212 (85%)]\tLoss: 0.038008\n",
            "Train Epoch: 92 [32000/35212 (91%)]\tLoss: 0.040583\n",
            "Train Epoch: 92 [34000/35212 (97%)]\tLoss: 0.011616\n",
            "Train Epoch: 93 [0/35212 (0%)]\tLoss: 0.008225\n",
            "Train Epoch: 93 [2000/35212 (6%)]\tLoss: 0.013131\n",
            "Train Epoch: 93 [4000/35212 (11%)]\tLoss: 0.010661\n",
            "Train Epoch: 93 [6000/35212 (17%)]\tLoss: 0.022866\n",
            "Train Epoch: 93 [8000/35212 (23%)]\tLoss: 0.008664\n",
            "Train Epoch: 93 [10000/35212 (28%)]\tLoss: 0.031054\n",
            "Train Epoch: 93 [12000/35212 (34%)]\tLoss: 0.014708\n",
            "Train Epoch: 93 [14000/35212 (40%)]\tLoss: 0.007489\n",
            "Train Epoch: 93 [16000/35212 (45%)]\tLoss: 0.008677\n",
            "Train Epoch: 93 [18000/35212 (51%)]\tLoss: 0.014230\n",
            "Train Epoch: 93 [20000/35212 (57%)]\tLoss: 0.010653\n",
            "Train Epoch: 93 [22000/35212 (62%)]\tLoss: 0.015352\n",
            "Train Epoch: 93 [24000/35212 (68%)]\tLoss: 0.028425\n",
            "Train Epoch: 93 [26000/35212 (74%)]\tLoss: 0.019347\n",
            "Train Epoch: 93 [28000/35212 (80%)]\tLoss: 0.038014\n",
            "Train Epoch: 93 [30000/35212 (85%)]\tLoss: 0.018253\n",
            "Train Epoch: 93 [32000/35212 (91%)]\tLoss: 0.027468\n",
            "Train Epoch: 93 [34000/35212 (97%)]\tLoss: 0.009477\n",
            "Train Epoch: 94 [0/35212 (0%)]\tLoss: 0.010979\n",
            "Train Epoch: 94 [2000/35212 (6%)]\tLoss: 0.010846\n",
            "Train Epoch: 94 [4000/35212 (11%)]\tLoss: 0.007160\n",
            "Train Epoch: 94 [6000/35212 (17%)]\tLoss: 0.059558\n",
            "Train Epoch: 94 [8000/35212 (23%)]\tLoss: 0.006121\n",
            "Train Epoch: 94 [10000/35212 (28%)]\tLoss: 0.045248\n",
            "Train Epoch: 94 [12000/35212 (34%)]\tLoss: 0.008832\n",
            "Train Epoch: 94 [14000/35212 (40%)]\tLoss: 0.008001\n",
            "Train Epoch: 94 [16000/35212 (45%)]\tLoss: 0.009516\n",
            "Train Epoch: 94 [18000/35212 (51%)]\tLoss: 0.029088\n",
            "Train Epoch: 94 [20000/35212 (57%)]\tLoss: 0.010166\n",
            "Train Epoch: 94 [22000/35212 (62%)]\tLoss: 0.012665\n",
            "Train Epoch: 94 [24000/35212 (68%)]\tLoss: 0.023260\n",
            "Train Epoch: 94 [26000/35212 (74%)]\tLoss: 0.013265\n",
            "Train Epoch: 94 [28000/35212 (80%)]\tLoss: 0.032520\n",
            "Train Epoch: 94 [30000/35212 (85%)]\tLoss: 0.010914\n",
            "Train Epoch: 94 [32000/35212 (91%)]\tLoss: 0.024513\n",
            "Train Epoch: 94 [34000/35212 (97%)]\tLoss: 0.008871\n",
            "Train Epoch: 95 [0/35212 (0%)]\tLoss: 0.006243\n",
            "Train Epoch: 95 [2000/35212 (6%)]\tLoss: 0.011345\n",
            "Train Epoch: 95 [4000/35212 (11%)]\tLoss: 0.008228\n",
            "Train Epoch: 95 [6000/35212 (17%)]\tLoss: 0.026537\n",
            "Train Epoch: 95 [8000/35212 (23%)]\tLoss: 0.005721\n",
            "Train Epoch: 95 [10000/35212 (28%)]\tLoss: 0.023688\n",
            "Train Epoch: 95 [12000/35212 (34%)]\tLoss: 0.014835\n",
            "Train Epoch: 95 [14000/35212 (40%)]\tLoss: 0.011980\n",
            "Train Epoch: 95 [16000/35212 (45%)]\tLoss: 0.009238\n",
            "Train Epoch: 95 [18000/35212 (51%)]\tLoss: 0.015018\n",
            "Train Epoch: 95 [20000/35212 (57%)]\tLoss: 0.011977\n",
            "Train Epoch: 95 [22000/35212 (62%)]\tLoss: 0.010923\n",
            "Train Epoch: 95 [24000/35212 (68%)]\tLoss: 0.017309\n",
            "Train Epoch: 95 [26000/35212 (74%)]\tLoss: 0.017225\n",
            "Train Epoch: 95 [28000/35212 (80%)]\tLoss: 0.030097\n",
            "Train Epoch: 95 [30000/35212 (85%)]\tLoss: 0.011202\n",
            "Train Epoch: 95 [32000/35212 (91%)]\tLoss: 0.033970\n",
            "Train Epoch: 95 [34000/35212 (97%)]\tLoss: 0.010302\n",
            "Train Epoch: 96 [0/35212 (0%)]\tLoss: 0.012366\n",
            "Train Epoch: 96 [2000/35212 (6%)]\tLoss: 0.009971\n",
            "Train Epoch: 96 [4000/35212 (11%)]\tLoss: 0.009610\n",
            "Train Epoch: 96 [6000/35212 (17%)]\tLoss: 0.008003\n",
            "Train Epoch: 96 [8000/35212 (23%)]\tLoss: 0.010093\n",
            "Train Epoch: 96 [10000/35212 (28%)]\tLoss: 0.036314\n",
            "Train Epoch: 96 [12000/35212 (34%)]\tLoss: 0.013458\n",
            "Train Epoch: 96 [14000/35212 (40%)]\tLoss: 0.018456\n",
            "Train Epoch: 96 [16000/35212 (45%)]\tLoss: 0.011136\n",
            "Train Epoch: 96 [18000/35212 (51%)]\tLoss: 0.020599\n",
            "Train Epoch: 96 [20000/35212 (57%)]\tLoss: 0.009516\n",
            "Train Epoch: 96 [22000/35212 (62%)]\tLoss: 0.012826\n",
            "Train Epoch: 96 [24000/35212 (68%)]\tLoss: 0.007011\n",
            "Train Epoch: 96 [26000/35212 (74%)]\tLoss: 0.024127\n",
            "Train Epoch: 96 [28000/35212 (80%)]\tLoss: 0.038764\n",
            "Train Epoch: 96 [30000/35212 (85%)]\tLoss: 0.010689\n",
            "Train Epoch: 96 [32000/35212 (91%)]\tLoss: 0.024343\n",
            "Train Epoch: 96 [34000/35212 (97%)]\tLoss: 0.010795\n",
            "Train Epoch: 97 [0/35212 (0%)]\tLoss: 0.007856\n",
            "Train Epoch: 97 [2000/35212 (6%)]\tLoss: 0.011196\n",
            "Train Epoch: 97 [4000/35212 (11%)]\tLoss: 0.005372\n",
            "Train Epoch: 97 [6000/35212 (17%)]\tLoss: 0.008116\n",
            "Train Epoch: 97 [8000/35212 (23%)]\tLoss: 0.006078\n",
            "Train Epoch: 97 [10000/35212 (28%)]\tLoss: 0.037747\n",
            "Train Epoch: 97 [12000/35212 (34%)]\tLoss: 0.010225\n",
            "Train Epoch: 97 [14000/35212 (40%)]\tLoss: 0.007745\n",
            "Train Epoch: 97 [16000/35212 (45%)]\tLoss: 0.005525\n",
            "Train Epoch: 97 [18000/35212 (51%)]\tLoss: 0.018876\n",
            "Train Epoch: 97 [20000/35212 (57%)]\tLoss: 0.009798\n",
            "Train Epoch: 97 [22000/35212 (62%)]\tLoss: 0.010421\n",
            "Train Epoch: 97 [24000/35212 (68%)]\tLoss: 0.007694\n",
            "Train Epoch: 97 [26000/35212 (74%)]\tLoss: 0.019892\n",
            "Train Epoch: 97 [28000/35212 (80%)]\tLoss: 0.033156\n",
            "Train Epoch: 97 [30000/35212 (85%)]\tLoss: 0.019483\n",
            "Train Epoch: 97 [32000/35212 (91%)]\tLoss: 0.026924\n",
            "Train Epoch: 97 [34000/35212 (97%)]\tLoss: 0.008191\n",
            "Train Epoch: 98 [0/35212 (0%)]\tLoss: 0.011120\n",
            "Train Epoch: 98 [2000/35212 (6%)]\tLoss: 0.012379\n",
            "Train Epoch: 98 [4000/35212 (11%)]\tLoss: 0.009486\n",
            "Train Epoch: 98 [6000/35212 (17%)]\tLoss: 0.006286\n",
            "Train Epoch: 98 [8000/35212 (23%)]\tLoss: 0.007205\n",
            "Train Epoch: 98 [10000/35212 (28%)]\tLoss: 0.033707\n",
            "Train Epoch: 98 [12000/35212 (34%)]\tLoss: 0.010965\n",
            "Train Epoch: 98 [14000/35212 (40%)]\tLoss: 0.006099\n",
            "Train Epoch: 98 [16000/35212 (45%)]\tLoss: 0.010077\n",
            "Train Epoch: 98 [18000/35212 (51%)]\tLoss: 0.014018\n",
            "Train Epoch: 98 [20000/35212 (57%)]\tLoss: 0.010535\n",
            "Train Epoch: 98 [22000/35212 (62%)]\tLoss: 0.014433\n",
            "Train Epoch: 98 [24000/35212 (68%)]\tLoss: 0.007714\n",
            "Train Epoch: 98 [26000/35212 (74%)]\tLoss: 0.011512\n",
            "Train Epoch: 98 [28000/35212 (80%)]\tLoss: 0.032197\n",
            "Train Epoch: 98 [30000/35212 (85%)]\tLoss: 0.012376\n",
            "Train Epoch: 98 [32000/35212 (91%)]\tLoss: 0.024864\n",
            "Train Epoch: 98 [34000/35212 (97%)]\tLoss: 0.010743\n",
            "Train Epoch: 99 [0/35212 (0%)]\tLoss: 0.008444\n",
            "Train Epoch: 99 [2000/35212 (6%)]\tLoss: 0.010275\n",
            "Train Epoch: 99 [4000/35212 (11%)]\tLoss: 0.004613\n",
            "Train Epoch: 99 [6000/35212 (17%)]\tLoss: 0.011846\n",
            "Train Epoch: 99 [8000/35212 (23%)]\tLoss: 0.006019\n",
            "Train Epoch: 99 [10000/35212 (28%)]\tLoss: 0.025315\n",
            "Train Epoch: 99 [12000/35212 (34%)]\tLoss: 0.011566\n",
            "Train Epoch: 99 [14000/35212 (40%)]\tLoss: 0.005620\n",
            "Train Epoch: 99 [16000/35212 (45%)]\tLoss: 0.008371\n",
            "Train Epoch: 99 [18000/35212 (51%)]\tLoss: 0.013682\n",
            "Train Epoch: 99 [20000/35212 (57%)]\tLoss: 0.006361\n",
            "Train Epoch: 99 [22000/35212 (62%)]\tLoss: 0.018498\n",
            "Train Epoch: 99 [24000/35212 (68%)]\tLoss: 0.012404\n",
            "Train Epoch: 99 [26000/35212 (74%)]\tLoss: 0.013229\n",
            "Train Epoch: 99 [28000/35212 (80%)]\tLoss: 0.032567\n",
            "Train Epoch: 99 [30000/35212 (85%)]\tLoss: 0.018254\n",
            "Train Epoch: 99 [32000/35212 (91%)]\tLoss: 0.022440\n",
            "Train Epoch: 99 [34000/35212 (97%)]\tLoss: 0.008460\n",
            "\n",
            "Test set: Average loss: 0.4326\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "count    15091.000000\n",
            "mean        -0.020108\n",
            "std          0.657449\n",
            "min        -14.217546\n",
            "25%         -0.096015\n",
            "50%         -0.016326\n",
            "75%          0.060894\n",
            "max         21.545506\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS1KDjLskNci4S1KDjLskNej/AE6ELczU34B3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}